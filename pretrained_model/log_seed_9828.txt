save path : ./110epoch500D8_2
{'gammas': [0.1, 0.1], 'schedule': [250, 375], 'decay': 0.0001, 'evaluate': False, 'learning_rate': 0.1, 'data_path': '/home/liheng/cifar10', 'print_freq': 200, 'start_epoch': 0, 'batch_size': 128, 'dataset': 'cifar10', 'epochs': 500, 'manualSeed': 9828, 'ngpu': 1, 'use_cuda': True, 'save_path': './110epoch500D8_2', 'resume': '', 'Ddim': 8, 'arch': 'resnet110', 'workers': 2, 'momentum': 0.9}
Random Seed: 9828
python version : 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)  [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
torch  version : 0.3.1.post3
cudnn  version : 7102
=> creating model 'resnet110'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0, ceil_mode=False, count_include_pad=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0, ceil_mode=False, count_include_pad=True)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (5): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (6): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (7): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (8): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (9): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (10): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (11): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (12): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (13): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (14): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (15): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (16): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (17): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0, ceil_mode=False, count_include_pad=True)
  (classifier): LinearCapsPro(
  )
)
=> do not use any checkpoint for resnet110 model

==>>[2018-05-03 13:45:09] [Epoch=000/500] [Need: 00:00:00] [learning_rate=0.1000] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 2.063 (2.063)   Data 0.126 (0.126)   Loss 3.9089 (3.9089)   Prec@1 8.594 (8.594)   Prec@5 47.656 (47.656)   [2018-05-03 13:45:11]
  Epoch: [000][200/391]   Time 0.140 (0.165)   Data 0.001 (0.002)   Loss 1.4477 (1.7504)   Prec@1 37.500 (35.005)   Prec@5 92.188 (85.805)   [2018-05-03 13:45:42]
  **Train** Prec@1 43.198 Prec@5 89.790 Error@1 56.802
  **Test** Prec@1 52.800 Prec@5 94.280 Error@1 47.200

==>>[2018-05-03 13:46:20] [Epoch=001/500] [Need: 09:40:14] [learning_rate=0.1000] [Best : Accuracy=52.80, Error=47.20]
  Epoch: [001][000/391]   Time 0.412 (0.412)   Data 0.155 (0.155)   Loss 1.1878 (1.1878)   Prec@1 56.250 (56.250)   Prec@5 95.312 (95.312)   [2018-05-03 13:46:21]
  Epoch: [001][200/391]   Time 0.144 (0.158)   Data 0.001 (0.001)   Loss 1.0544 (1.0738)   Prec@1 67.188 (61.392)   Prec@5 96.094 (96.296)   [2018-05-03 13:46:52]
  **Train** Prec@1 63.998 Prec@5 96.776 Error@1 36.002
  **Test** Prec@1 65.290 Prec@5 96.820 Error@1 34.710

==>>[2018-05-03 13:47:29] [Epoch=002/500] [Need: 09:35:05] [learning_rate=0.1000] [Best : Accuracy=65.29, Error=34.71]
  Epoch: [002][000/391]   Time 0.335 (0.335)   Data 0.168 (0.168)   Loss 0.8207 (0.8207)   Prec@1 67.969 (67.969)   Prec@5 99.219 (99.219)   [2018-05-03 13:47:29]
  Epoch: [002][200/391]   Time 0.159 (0.159)   Data 0.001 (0.002)   Loss 0.7031 (0.8082)   Prec@1 76.562 (71.405)   Prec@5 99.219 (97.994)   [2018-05-03 13:48:01]
  **Train** Prec@1 72.804 Prec@5 98.150 Error@1 27.196
  **Test** Prec@1 64.930 Prec@5 96.850 Error@1 35.070

==>>[2018-05-03 13:48:39] [Epoch=003/500] [Need: 09:36:20] [learning_rate=0.1000] [Best : Accuracy=65.29, Error=34.71]
  Epoch: [003][000/391]   Time 0.355 (0.355)   Data 0.157 (0.157)   Loss 0.6146 (0.6146)   Prec@1 78.125 (78.125)   Prec@5 98.438 (98.438)   [2018-05-03 13:48:40]
  Epoch: [003][200/391]   Time 0.189 (0.158)   Data 0.001 (0.001)   Loss 0.7327 (0.6599)   Prec@1 78.125 (76.850)   Prec@5 98.438 (98.663)   [2018-05-03 13:49:11]
  **Train** Prec@1 77.532 Prec@5 98.734 Error@1 22.468
  **Test** Prec@1 70.290 Prec@5 97.540 Error@1 29.710

==>>[2018-05-03 13:49:48] [Epoch=004/500] [Need: 09:33:55] [learning_rate=0.1000] [Best : Accuracy=70.29, Error=29.71]
  Epoch: [004][000/391]   Time 0.355 (0.355)   Data 0.166 (0.166)   Loss 0.6967 (0.6967)   Prec@1 77.344 (77.344)   Prec@5 99.219 (99.219)   [2018-05-03 13:49:49]
  Epoch: [004][200/391]   Time 0.178 (0.160)   Data 0.001 (0.002)   Loss 0.5772 (0.5735)   Prec@1 80.469 (80.150)   Prec@5 99.219 (98.931)   [2018-05-03 13:50:20]
  **Train** Prec@1 80.418 Prec@5 99.010 Error@1 19.582
  **Test** Prec@1 78.660 Prec@5 98.810 Error@1 21.340

==>>[2018-05-03 13:50:58] [Epoch=005/500] [Need: 09:33:44] [learning_rate=0.1000] [Best : Accuracy=78.66, Error=21.34]
  Epoch: [005][000/391]   Time 0.362 (0.362)   Data 0.174 (0.174)   Loss 0.3921 (0.3921)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2018-05-03 13:50:59]
  Epoch: [005][200/391]   Time 0.156 (0.156)   Data 0.001 (0.002)   Loss 0.4069 (0.5282)   Prec@1 86.719 (81.814)   Prec@5 99.219 (99.129)   [2018-05-03 13:51:30]
  **Train** Prec@1 81.868 Prec@5 99.122 Error@1 18.132
  **Test** Prec@1 73.910 Prec@5 98.640 Error@1 26.090

==>>[2018-05-03 13:52:10] [Epoch=006/500] [Need: 09:35:00] [learning_rate=0.1000] [Best : Accuracy=78.66, Error=21.34]
  Epoch: [006][000/391]   Time 0.353 (0.353)   Data 0.153 (0.153)   Loss 0.4800 (0.4800)   Prec@1 84.375 (84.375)   Prec@5 98.438 (98.438)   [2018-05-03 13:52:10]
  Epoch: [006][200/391]   Time 0.168 (0.159)   Data 0.001 (0.002)   Loss 0.4824 (0.4747)   Prec@1 82.812 (83.446)   Prec@5 100.000 (99.277)   [2018-05-03 13:52:42]
  **Train** Prec@1 83.604 Prec@5 99.272 Error@1 16.396
  **Test** Prec@1 79.490 Prec@5 99.090 Error@1 20.510

==>>[2018-05-03 13:53:20] [Epoch=007/500] [Need: 09:34:03] [learning_rate=0.1000] [Best : Accuracy=79.49, Error=20.51]
  Epoch: [007][000/391]   Time 0.368 (0.368)   Data 0.187 (0.187)   Loss 0.4431 (0.4431)   Prec@1 82.812 (82.812)   Prec@5 100.000 (100.000)   [2018-05-03 13:53:20]
  Epoch: [007][200/391]   Time 0.144 (0.160)   Data 0.001 (0.002)   Loss 0.5036 (0.4507)   Prec@1 82.812 (84.530)   Prec@5 98.438 (99.335)   [2018-05-03 13:53:52]
  **Train** Prec@1 84.684 Prec@5 99.366 Error@1 15.316
  **Test** Prec@1 75.740 Prec@5 98.630 Error@1 24.260

==>>[2018-05-03 13:54:30] [Epoch=008/500] [Need: 09:33:21] [learning_rate=0.1000] [Best : Accuracy=79.49, Error=20.51]
  Epoch: [008][000/391]   Time 0.379 (0.379)   Data 0.190 (0.190)   Loss 0.4773 (0.4773)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2018-05-03 13:54:30]
  Epoch: [008][200/391]   Time 0.154 (0.159)   Data 0.001 (0.002)   Loss 0.5075 (0.4221)   Prec@1 79.688 (85.471)   Prec@5 100.000 (99.429)   [2018-05-03 13:55:02]
  **Train** Prec@1 85.580 Prec@5 99.442 Error@1 14.420
  **Test** Prec@1 80.220 Prec@5 98.940 Error@1 19.780

==>>[2018-05-03 13:55:40] [Epoch=009/500] [Need: 09:32:32] [learning_rate=0.1000] [Best : Accuracy=80.22, Error=19.78]
  Epoch: [009][000/391]   Time 0.360 (0.360)   Data 0.160 (0.160)   Loss 0.3443 (0.3443)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2018-05-03 13:55:41]
  Epoch: [009][200/391]   Time 0.158 (0.157)   Data 0.001 (0.002)   Loss 0.3386 (0.3906)   Prec@1 89.062 (86.221)   Prec@5 100.000 (99.576)   [2018-05-03 13:56:12]
  **Train** Prec@1 86.148 Prec@5 99.520 Error@1 13.852
  **Test** Prec@1 82.080 Prec@5 99.190 Error@1 17.920

==>>[2018-05-03 13:56:50] [Epoch=010/500] [Need: 09:31:22] [learning_rate=0.1000] [Best : Accuracy=82.08, Error=17.92]
  Epoch: [010][000/391]   Time 0.337 (0.337)   Data 0.155 (0.155)   Loss 0.3268 (0.3268)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2018-05-03 13:56:51]
  Epoch: [010][200/391]   Time 0.178 (0.166)   Data 0.001 (0.002)   Loss 0.3777 (0.3715)   Prec@1 86.719 (86.894)   Prec@5 100.000 (99.619)   [2018-05-03 13:57:24]
  **Train** Prec@1 86.868 Prec@5 99.596 Error@1 13.132
  **Test** Prec@1 72.490 Prec@5 96.290 Error@1 27.510

==>>[2018-05-03 13:58:03] [Epoch=011/500] [Need: 09:31:59] [learning_rate=0.1000] [Best : Accuracy=82.08, Error=17.92]
  Epoch: [011][000/391]   Time 0.354 (0.354)   Data 0.165 (0.165)   Loss 0.5001 (0.5001)   Prec@1 83.594 (83.594)   Prec@5 98.438 (98.438)   [2018-05-03 13:58:03]
  Epoch: [011][200/391]   Time 0.164 (0.160)   Data 0.001 (0.002)   Loss 0.3060 (0.3570)   Prec@1 89.062 (87.609)   Prec@5 100.000 (99.576)   [2018-05-03 13:58:35]
  **Train** Prec@1 87.766 Prec@5 99.594 Error@1 12.234
  **Test** Prec@1 81.790 Prec@5 99.210 Error@1 18.210

==>>[2018-05-03 13:59:14] [Epoch=012/500] [Need: 09:31:36] [learning_rate=0.1000] [Best : Accuracy=82.08, Error=17.92]
  Epoch: [012][000/391]   Time 0.367 (0.367)   Data 0.156 (0.156)   Loss 0.4125 (0.4125)   Prec@1 85.938 (85.938)   Prec@5 99.219 (99.219)   [2018-05-03 13:59:14]
  Epoch: [012][200/391]   Time 0.179 (0.163)   Data 0.001 (0.002)   Loss 0.2707 (0.3397)   Prec@1 90.625 (88.219)   Prec@5 100.000 (99.662)   [2018-05-03 13:59:47]
  **Train** Prec@1 88.256 Prec@5 99.638 Error@1 11.744
  **Test** Prec@1 82.180 Prec@5 99.260 Error@1 17.820

==>>[2018-05-03 14:00:25] [Epoch=013/500] [Need: 09:31:09] [learning_rate=0.1000] [Best : Accuracy=82.18, Error=17.82]
  Epoch: [013][000/391]   Time 0.327 (0.327)   Data 0.135 (0.135)   Loss 0.4736 (0.4736)   Prec@1 83.594 (83.594)   Prec@5 99.219 (99.219)   [2018-05-03 14:00:26]
  Epoch: [013][200/391]   Time 0.145 (0.165)   Data 0.001 (0.001)   Loss 0.4709 (0.3214)   Prec@1 85.938 (88.973)   Prec@5 100.000 (99.685)   [2018-05-03 14:00:58]
  **Train** Prec@1 88.710 Prec@5 99.684 Error@1 11.290
  **Test** Prec@1 84.860 Prec@5 99.420 Error@1 15.140

==>>[2018-05-03 14:01:38] [Epoch=014/500] [Need: 09:31:05] [learning_rate=0.1000] [Best : Accuracy=84.86, Error=15.14]
  Epoch: [014][000/391]   Time 0.398 (0.398)   Data 0.197 (0.197)   Loss 0.4087 (0.4087)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-05-03 14:01:38]
  Epoch: [014][200/391]   Time 0.156 (0.163)   Data 0.001 (0.002)   Loss 0.3110 (0.3149)   Prec@1 91.406 (88.845)   Prec@5 100.000 (99.728)   [2018-05-03 14:02:10]
  **Train** Prec@1 88.916 Prec@5 99.690 Error@1 11.084
  **Test** Prec@1 85.580 Prec@5 99.400 Error@1 14.420

==>>[2018-05-03 14:02:49] [Epoch=015/500] [Need: 09:30:40] [learning_rate=0.1000] [Best : Accuracy=85.58, Error=14.42]
  Epoch: [015][000/391]   Time 0.402 (0.402)   Data 0.205 (0.205)   Loss 0.2239 (0.2239)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 14:02:50]
  Epoch: [015][200/391]   Time 0.172 (0.165)   Data 0.001 (0.002)   Loss 0.2505 (0.3008)   Prec@1 90.625 (89.533)   Prec@5 100.000 (99.771)   [2018-05-03 14:03:23]
  **Train** Prec@1 89.476 Prec@5 99.744 Error@1 10.524
  **Test** Prec@1 86.040 Prec@5 99.540 Error@1 13.960

==>>[2018-05-03 14:04:02] [Epoch=016/500] [Need: 09:30:16] [learning_rate=0.1000] [Best : Accuracy=86.04, Error=13.96]
  Epoch: [016][000/391]   Time 0.300 (0.300)   Data 0.119 (0.119)   Loss 0.4030 (0.4030)   Prec@1 85.156 (85.156)   Prec@5 100.000 (100.000)   [2018-05-03 14:04:02]
  Epoch: [016][200/391]   Time 0.174 (0.163)   Data 0.001 (0.001)   Loss 0.2824 (0.2872)   Prec@1 88.281 (90.038)   Prec@5 100.000 (99.724)   [2018-05-03 14:04:34]
  **Train** Prec@1 89.758 Prec@5 99.724 Error@1 10.242
  **Test** Prec@1 84.040 Prec@5 99.480 Error@1 15.960

==>>[2018-05-03 14:05:15] [Epoch=017/500] [Need: 09:30:23] [learning_rate=0.1000] [Best : Accuracy=86.04, Error=13.96]
  Epoch: [017][000/391]   Time 0.356 (0.356)   Data 0.153 (0.153)   Loss 0.2544 (0.2544)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 14:05:15]
  Epoch: [017][200/391]   Time 0.158 (0.160)   Data 0.001 (0.002)   Loss 0.2147 (0.2771)   Prec@1 93.750 (90.190)   Prec@5 100.000 (99.743)   [2018-05-03 14:05:47]
  **Train** Prec@1 90.066 Prec@5 99.750 Error@1 9.934
  **Test** Prec@1 85.490 Prec@5 99.400 Error@1 14.510

==>>[2018-05-03 14:06:26] [Epoch=018/500] [Need: 09:29:28] [learning_rate=0.1000] [Best : Accuracy=86.04, Error=13.96]
  Epoch: [018][000/391]   Time 0.338 (0.338)   Data 0.146 (0.146)   Loss 0.3324 (0.3324)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2018-05-03 14:06:27]
  Epoch: [018][200/391]   Time 0.161 (0.166)   Data 0.001 (0.001)   Loss 0.2245 (0.2698)   Prec@1 92.969 (90.606)   Prec@5 100.000 (99.790)   [2018-05-03 14:07:00]
  **Train** Prec@1 90.420 Prec@5 99.784 Error@1 9.580
  **Test** Prec@1 85.290 Prec@5 99.510 Error@1 14.710

==>>[2018-05-03 14:07:39] [Epoch=019/500] [Need: 09:28:59] [learning_rate=0.1000] [Best : Accuracy=86.04, Error=13.96]
  Epoch: [019][000/391]   Time 0.371 (0.371)   Data 0.167 (0.167)   Loss 0.2051 (0.2051)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:07:39]
  Epoch: [019][200/391]   Time 0.172 (0.160)   Data 0.001 (0.002)   Loss 0.3651 (0.2581)   Prec@1 88.281 (91.014)   Prec@5 100.000 (99.794)   [2018-05-03 14:08:11]
  **Train** Prec@1 90.708 Prec@5 99.778 Error@1 9.292
  **Test** Prec@1 86.510 Prec@5 99.500 Error@1 13.490

==>>[2018-05-03 14:08:49] [Epoch=020/500] [Need: 09:27:22] [learning_rate=0.1000] [Best : Accuracy=86.51, Error=13.49]
  Epoch: [020][000/391]   Time 0.354 (0.354)   Data 0.162 (0.162)   Loss 0.2329 (0.2329)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-05-03 14:08:49]
  Epoch: [020][200/391]   Time 0.141 (0.161)   Data 0.001 (0.002)   Loss 0.2127 (0.2573)   Prec@1 92.969 (91.134)   Prec@5 100.000 (99.810)   [2018-05-03 14:09:21]
  **Train** Prec@1 91.012 Prec@5 99.804 Error@1 8.988
  **Test** Prec@1 84.310 Prec@5 98.980 Error@1 15.690

==>>[2018-05-03 14:10:01] [Epoch=021/500] [Need: 09:26:33] [learning_rate=0.1000] [Best : Accuracy=86.51, Error=13.49]
  Epoch: [021][000/391]   Time 0.357 (0.357)   Data 0.160 (0.160)   Loss 0.2292 (0.2292)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-05-03 14:10:01]
  Epoch: [021][200/391]   Time 0.175 (0.162)   Data 0.001 (0.002)   Loss 0.1564 (0.2466)   Prec@1 96.094 (91.383)   Prec@5 99.219 (99.848)   [2018-05-03 14:10:33]
  **Train** Prec@1 91.060 Prec@5 99.810 Error@1 8.940
  **Test** Prec@1 83.060 Prec@5 99.080 Error@1 16.940

==>>[2018-05-03 14:11:13] [Epoch=022/500] [Need: 09:25:47] [learning_rate=0.1000] [Best : Accuracy=86.51, Error=13.49]
  Epoch: [022][000/391]   Time 0.383 (0.383)   Data 0.161 (0.161)   Loss 0.1347 (0.1347)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 14:11:13]
  Epoch: [022][200/391]   Time 0.188 (0.161)   Data 0.002 (0.002)   Loss 0.2642 (0.2368)   Prec@1 89.844 (91.799)   Prec@5 100.000 (99.821)   [2018-05-03 14:11:45]
  **Train** Prec@1 91.456 Prec@5 99.812 Error@1 8.544
  **Test** Prec@1 84.930 Prec@5 99.220 Error@1 15.070

==>>[2018-05-03 14:12:24] [Epoch=023/500] [Need: 09:24:32] [learning_rate=0.1000] [Best : Accuracy=86.51, Error=13.49]
  Epoch: [023][000/391]   Time 0.329 (0.329)   Data 0.165 (0.165)   Loss 0.2315 (0.2315)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 14:12:24]
  Epoch: [023][200/391]   Time 0.131 (0.159)   Data 0.001 (0.001)   Loss 0.1872 (0.2298)   Prec@1 94.531 (92.106)   Prec@5 100.000 (99.841)   [2018-05-03 14:12:56]
  **Train** Prec@1 91.576 Prec@5 99.824 Error@1 8.424
  **Test** Prec@1 84.730 Prec@5 99.200 Error@1 15.270

==>>[2018-05-03 14:13:35] [Epoch=024/500] [Need: 09:23:30] [learning_rate=0.1000] [Best : Accuracy=86.51, Error=13.49]
  Epoch: [024][000/391]   Time 0.338 (0.338)   Data 0.146 (0.146)   Loss 0.2700 (0.2700)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2018-05-03 14:13:36]
  Epoch: [024][200/391]   Time 0.158 (0.163)   Data 0.001 (0.002)   Loss 0.2486 (0.2401)   Prec@1 92.969 (91.535)   Prec@5 100.000 (99.813)   [2018-05-03 14:14:08]
  **Train** Prec@1 91.642 Prec@5 99.836 Error@1 8.358
  **Test** Prec@1 87.310 Prec@5 99.620 Error@1 12.690

==>>[2018-05-03 14:14:48] [Epoch=025/500] [Need: 09:22:43] [learning_rate=0.1000] [Best : Accuracy=87.31, Error=12.69]
  Epoch: [025][000/391]   Time 0.338 (0.338)   Data 0.171 (0.171)   Loss 0.1831 (0.1831)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 14:14:48]
  Epoch: [025][200/391]   Time 0.191 (0.163)   Data 0.001 (0.002)   Loss 0.2460 (0.2224)   Prec@1 92.188 (92.067)   Prec@5 99.219 (99.899)   [2018-05-03 14:15:20]
  **Train** Prec@1 91.996 Prec@5 99.858 Error@1 8.004
  **Test** Prec@1 84.820 Prec@5 99.140 Error@1 15.180

==>>[2018-05-03 14:16:00] [Epoch=026/500] [Need: 09:21:48] [learning_rate=0.1000] [Best : Accuracy=87.31, Error=12.69]
  Epoch: [026][000/391]   Time 0.395 (0.395)   Data 0.171 (0.171)   Loss 0.2094 (0.2094)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:16:00]
  Epoch: [026][200/391]   Time 0.175 (0.164)   Data 0.001 (0.002)   Loss 0.1215 (0.2253)   Prec@1 96.094 (92.059)   Prec@5 100.000 (99.876)   [2018-05-03 14:16:32]
  **Train** Prec@1 92.046 Prec@5 99.864 Error@1 7.954
  **Test** Prec@1 86.910 Prec@5 99.600 Error@1 13.090

==>>[2018-05-03 14:17:12] [Epoch=027/500] [Need: 09:21:08] [learning_rate=0.1000] [Best : Accuracy=87.31, Error=12.69]
  Epoch: [027][000/391]   Time 0.374 (0.374)   Data 0.156 (0.156)   Loss 0.2351 (0.2351)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-05-03 14:17:13]
  Epoch: [027][200/391]   Time 0.167 (0.163)   Data 0.001 (0.002)   Loss 0.2479 (0.2141)   Prec@1 91.406 (92.600)   Prec@5 100.000 (99.848)   [2018-05-03 14:17:45]
  **Train** Prec@1 92.304 Prec@5 99.860 Error@1 7.696
  **Test** Prec@1 87.100 Prec@5 99.420 Error@1 12.900

==>>[2018-05-03 14:18:24] [Epoch=028/500] [Need: 09:20:12] [learning_rate=0.1000] [Best : Accuracy=87.31, Error=12.69]
  Epoch: [028][000/391]   Time 0.308 (0.308)   Data 0.129 (0.129)   Loss 0.3009 (0.3009)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2018-05-03 14:18:25]
  Epoch: [028][200/391]   Time 0.183 (0.158)   Data 0.001 (0.001)   Loss 0.2047 (0.2145)   Prec@1 92.969 (92.487)   Prec@5 100.000 (99.864)   [2018-05-03 14:18:56]
  **Train** Prec@1 92.380 Prec@5 99.864 Error@1 7.620
  **Test** Prec@1 87.730 Prec@5 99.610 Error@1 12.270

==>>[2018-05-03 14:19:36] [Epoch=029/500] [Need: 09:19:02] [learning_rate=0.1000] [Best : Accuracy=87.73, Error=12.27]
  Epoch: [029][000/391]   Time 0.307 (0.307)   Data 0.139 (0.139)   Loss 0.3076 (0.3076)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2018-05-03 14:19:36]
  Epoch: [029][200/391]   Time 0.150 (0.158)   Data 0.001 (0.001)   Loss 0.2198 (0.2171)   Prec@1 92.969 (92.428)   Prec@5 100.000 (99.837)   [2018-05-03 14:20:08]
  **Train** Prec@1 92.364 Prec@5 99.838 Error@1 7.636
  **Test** Prec@1 87.030 Prec@5 99.370 Error@1 12.970

==>>[2018-05-03 14:20:46] [Epoch=030/500] [Need: 09:17:36] [learning_rate=0.1000] [Best : Accuracy=87.73, Error=12.27]
  Epoch: [030][000/391]   Time 0.325 (0.325)   Data 0.135 (0.135)   Loss 0.1568 (0.1568)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 14:20:46]
  Epoch: [030][200/391]   Time 0.144 (0.161)   Data 0.001 (0.001)   Loss 0.2183 (0.2059)   Prec@1 94.531 (92.942)   Prec@5 100.000 (99.880)   [2018-05-03 14:21:18]
  **Train** Prec@1 92.762 Prec@5 99.890 Error@1 7.238
  **Test** Prec@1 85.830 Prec@5 99.470 Error@1 14.170

==>>[2018-05-03 14:21:58] [Epoch=031/500] [Need: 09:16:39] [learning_rate=0.1000] [Best : Accuracy=87.73, Error=12.27]
  Epoch: [031][000/391]   Time 0.347 (0.347)   Data 0.150 (0.150)   Loss 0.1687 (0.1687)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 14:21:58]
  Epoch: [031][200/391]   Time 0.150 (0.164)   Data 0.001 (0.002)   Loss 0.2389 (0.2052)   Prec@1 93.750 (92.701)   Prec@5 99.219 (99.880)   [2018-05-03 14:22:31]
  **Train** Prec@1 92.550 Prec@5 99.860 Error@1 7.450
  **Test** Prec@1 87.960 Prec@5 99.610 Error@1 12.040

==>>[2018-05-03 14:23:11] [Epoch=032/500] [Need: 09:15:50] [learning_rate=0.1000] [Best : Accuracy=87.96, Error=12.04]
  Epoch: [032][000/391]   Time 0.368 (0.368)   Data 0.152 (0.152)   Loss 0.2415 (0.2415)   Prec@1 91.406 (91.406)   Prec@5 99.219 (99.219)   [2018-05-03 14:23:11]
  Epoch: [032][200/391]   Time 0.147 (0.173)   Data 0.001 (0.002)   Loss 0.1336 (0.1987)   Prec@1 95.312 (92.996)   Prec@5 100.000 (99.872)   [2018-05-03 14:23:45]
  **Train** Prec@1 92.760 Prec@5 99.886 Error@1 7.240
  **Test** Prec@1 83.990 Prec@5 98.650 Error@1 16.010

==>>[2018-05-03 14:24:24] [Epoch=033/500] [Need: 09:15:03] [learning_rate=0.1000] [Best : Accuracy=87.96, Error=12.04]
  Epoch: [033][000/391]   Time 0.346 (0.346)   Data 0.166 (0.166)   Loss 0.1595 (0.1595)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 14:24:24]
  Epoch: [033][200/391]   Time 0.156 (0.160)   Data 0.001 (0.001)   Loss 0.2047 (0.2001)   Prec@1 92.188 (92.891)   Prec@5 100.000 (99.899)   [2018-05-03 14:24:56]
  **Train** Prec@1 92.810 Prec@5 99.890 Error@1 7.190
  **Test** Prec@1 88.240 Prec@5 99.640 Error@1 11.760

==>>[2018-05-03 14:25:37] [Epoch=034/500] [Need: 09:14:15] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [034][000/391]   Time 0.367 (0.367)   Data 0.182 (0.182)   Loss 0.2222 (0.2222)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2018-05-03 14:25:37]
  Epoch: [034][200/391]   Time 0.162 (0.166)   Data 0.001 (0.002)   Loss 0.1543 (0.1936)   Prec@1 93.750 (93.354)   Prec@5 100.000 (99.883)   [2018-05-03 14:26:10]
  **Train** Prec@1 93.080 Prec@5 99.884 Error@1 6.920
  **Test** Prec@1 87.010 Prec@5 99.380 Error@1 12.990

==>>[2018-05-03 14:26:49] [Epoch=035/500] [Need: 09:13:12] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [035][000/391]   Time 0.330 (0.330)   Data 0.154 (0.154)   Loss 0.1382 (0.1382)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 14:26:49]
  Epoch: [035][200/391]   Time 0.153 (0.163)   Data 0.001 (0.001)   Loss 0.1338 (0.1879)   Prec@1 94.531 (93.458)   Prec@5 100.000 (99.860)   [2018-05-03 14:27:22]
  **Train** Prec@1 93.140 Prec@5 99.888 Error@1 6.860
  **Test** Prec@1 85.290 Prec@5 99.210 Error@1 14.710

==>>[2018-05-03 14:28:01] [Epoch=036/500] [Need: 09:12:07] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [036][000/391]   Time 0.360 (0.360)   Data 0.177 (0.177)   Loss 0.1586 (0.1586)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:28:01]
  Epoch: [036][200/391]   Time 0.144 (0.160)   Data 0.001 (0.002)   Loss 0.1676 (0.1844)   Prec@1 95.312 (93.525)   Prec@5 100.000 (99.918)   [2018-05-03 14:28:33]
  **Train** Prec@1 93.124 Prec@5 99.900 Error@1 6.876
  **Test** Prec@1 86.360 Prec@5 99.420 Error@1 13.640

==>>[2018-05-03 14:29:12] [Epoch=037/500] [Need: 09:10:53] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [037][000/391]   Time 0.347 (0.347)   Data 0.133 (0.133)   Loss 0.1950 (0.1950)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-05-03 14:29:12]
  Epoch: [037][200/391]   Time 0.168 (0.163)   Data 0.001 (0.001)   Loss 0.3453 (0.1865)   Prec@1 88.281 (93.486)   Prec@5 99.219 (99.934)   [2018-05-03 14:29:45]
  **Train** Prec@1 93.290 Prec@5 99.892 Error@1 6.710
  **Test** Prec@1 86.080 Prec@5 99.520 Error@1 13.920

==>>[2018-05-03 14:30:23] [Epoch=038/500] [Need: 09:09:41] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [038][000/391]   Time 0.367 (0.367)   Data 0.154 (0.154)   Loss 0.2069 (0.2069)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-05-03 14:30:24]
  Epoch: [038][200/391]   Time 0.142 (0.164)   Data 0.001 (0.001)   Loss 0.2701 (0.1872)   Prec@1 93.750 (93.560)   Prec@5 100.000 (99.942)   [2018-05-03 14:30:56]
  **Train** Prec@1 93.454 Prec@5 99.940 Error@1 6.546
  **Test** Prec@1 87.550 Prec@5 99.600 Error@1 12.450

==>>[2018-05-03 14:31:35] [Epoch=039/500] [Need: 09:08:30] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [039][000/391]   Time 0.369 (0.369)   Data 0.171 (0.171)   Loss 0.2128 (0.2128)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 14:31:35]
  Epoch: [039][200/391]   Time 0.171 (0.165)   Data 0.001 (0.002)   Loss 0.2468 (0.1797)   Prec@1 91.406 (93.801)   Prec@5 100.000 (99.907)   [2018-05-03 14:32:08]
  **Train** Prec@1 93.490 Prec@5 99.904 Error@1 6.510
  **Test** Prec@1 87.110 Prec@5 99.540 Error@1 12.890

==>>[2018-05-03 14:32:47] [Epoch=040/500] [Need: 09:07:32] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [040][000/391]   Time 0.307 (0.307)   Data 0.129 (0.129)   Loss 0.1496 (0.1496)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 14:32:48]
  Epoch: [040][200/391]   Time 0.157 (0.160)   Data 0.000 (0.001)   Loss 0.1444 (0.1859)   Prec@1 94.531 (93.571)   Prec@5 100.000 (99.891)   [2018-05-03 14:33:19]
  **Train** Prec@1 93.468 Prec@5 99.902 Error@1 6.532
  **Test** Prec@1 86.680 Prec@5 99.510 Error@1 13.320

==>>[2018-05-03 14:33:59] [Epoch=041/500] [Need: 09:06:25] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [041][000/391]   Time 0.363 (0.363)   Data 0.157 (0.157)   Loss 0.1496 (0.1496)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 14:33:59]
  Epoch: [041][200/391]   Time 0.160 (0.162)   Data 0.001 (0.002)   Loss 0.1232 (0.1761)   Prec@1 96.094 (93.824)   Prec@5 100.000 (99.938)   [2018-05-03 14:34:32]
  **Train** Prec@1 93.472 Prec@5 99.924 Error@1 6.528
  **Test** Prec@1 87.510 Prec@5 99.590 Error@1 12.490

==>>[2018-05-03 14:35:10] [Epoch=042/500] [Need: 09:05:11] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [042][000/391]   Time 0.368 (0.368)   Data 0.156 (0.156)   Loss 0.1148 (0.1148)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 14:35:11]
  Epoch: [042][200/391]   Time 0.145 (0.159)   Data 0.001 (0.002)   Loss 0.2378 (0.1704)   Prec@1 90.625 (94.018)   Prec@5 100.000 (99.942)   [2018-05-03 14:35:42]
  **Train** Prec@1 93.526 Prec@5 99.910 Error@1 6.474
  **Test** Prec@1 86.580 Prec@5 99.300 Error@1 13.420

==>>[2018-05-03 14:36:21] [Epoch=043/500] [Need: 09:03:50] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [043][000/391]   Time 0.349 (0.349)   Data 0.166 (0.166)   Loss 0.0828 (0.0828)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 14:36:21]
  Epoch: [043][200/391]   Time 0.168 (0.162)   Data 0.001 (0.001)   Loss 0.1613 (0.1672)   Prec@1 96.094 (94.139)   Prec@5 100.000 (99.918)   [2018-05-03 14:36:53]
  **Train** Prec@1 93.744 Prec@5 99.916 Error@1 6.256
  **Test** Prec@1 86.850 Prec@5 99.520 Error@1 13.150

==>>[2018-05-03 14:37:32] [Epoch=044/500] [Need: 09:02:41] [learning_rate=0.1000] [Best : Accuracy=88.24, Error=11.76]
  Epoch: [044][000/391]   Time 0.364 (0.364)   Data 0.152 (0.152)   Loss 0.1720 (0.1720)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2018-05-03 14:37:33]
  Epoch: [044][200/391]   Time 0.185 (0.165)   Data 0.001 (0.002)   Loss 0.2365 (0.1725)   Prec@1 89.062 (94.154)   Prec@5 100.000 (99.946)   [2018-05-03 14:38:06]
  **Train** Prec@1 93.776 Prec@5 99.916 Error@1 6.224
  **Test** Prec@1 88.510 Prec@5 99.570 Error@1 11.490

==>>[2018-05-03 14:38:45] [Epoch=045/500] [Need: 09:01:39] [learning_rate=0.1000] [Best : Accuracy=88.51, Error=11.49]
  Epoch: [045][000/391]   Time 0.353 (0.353)   Data 0.154 (0.154)   Loss 0.1375 (0.1375)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 14:38:45]
  Epoch: [045][200/391]   Time 0.185 (0.163)   Data 0.001 (0.002)   Loss 0.2167 (0.1714)   Prec@1 90.625 (94.205)   Prec@5 100.000 (99.914)   [2018-05-03 14:39:17]
  **Train** Prec@1 93.910 Prec@5 99.922 Error@1 6.090
  **Test** Prec@1 88.030 Prec@5 99.610 Error@1 11.970

==>>[2018-05-03 14:39:56] [Epoch=046/500] [Need: 09:00:30] [learning_rate=0.1000] [Best : Accuracy=88.51, Error=11.49]
  Epoch: [046][000/391]   Time 0.355 (0.355)   Data 0.148 (0.148)   Loss 0.2016 (0.2016)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:39:57]
  Epoch: [046][200/391]   Time 0.168 (0.164)   Data 0.001 (0.002)   Loss 0.1934 (0.1697)   Prec@1 92.188 (94.135)   Prec@5 100.000 (99.930)   [2018-05-03 14:40:29]
  **Train** Prec@1 93.852 Prec@5 99.920 Error@1 6.148
  **Test** Prec@1 89.280 Prec@5 99.500 Error@1 10.720

==>>[2018-05-03 14:41:08] [Epoch=047/500] [Need: 08:59:17] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [047][000/391]   Time 0.382 (0.382)   Data 0.183 (0.183)   Loss 0.1787 (0.1787)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 14:41:08]
  Epoch: [047][200/391]   Time 0.151 (0.162)   Data 0.001 (0.002)   Loss 0.2822 (0.1676)   Prec@1 89.844 (94.166)   Prec@5 100.000 (99.946)   [2018-05-03 14:41:40]
  **Train** Prec@1 94.068 Prec@5 99.932 Error@1 5.932
  **Test** Prec@1 88.210 Prec@5 99.460 Error@1 11.790

==>>[2018-05-03 14:42:20] [Epoch=048/500] [Need: 08:58:13] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [048][000/391]   Time 0.327 (0.327)   Data 0.148 (0.148)   Loss 0.1885 (0.1885)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 14:42:20]
  Epoch: [048][200/391]   Time 0.173 (0.165)   Data 0.001 (0.001)   Loss 0.1563 (0.1628)   Prec@1 96.875 (94.473)   Prec@5 99.219 (99.899)   [2018-05-03 14:42:53]
  **Train** Prec@1 94.048 Prec@5 99.902 Error@1 5.952
  **Test** Prec@1 86.900 Prec@5 99.430 Error@1 13.100

==>>[2018-05-03 14:43:32] [Epoch=049/500] [Need: 08:57:07] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [049][000/391]   Time 0.362 (0.362)   Data 0.149 (0.149)   Loss 0.2276 (0.2276)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2018-05-03 14:43:32]
  Epoch: [049][200/391]   Time 0.164 (0.162)   Data 0.001 (0.002)   Loss 0.1538 (0.1698)   Prec@1 95.312 (94.069)   Prec@5 100.000 (99.880)   [2018-05-03 14:44:05]
  **Train** Prec@1 93.928 Prec@5 99.886 Error@1 6.072
  **Test** Prec@1 85.280 Prec@5 99.140 Error@1 14.720

==>>[2018-05-03 14:44:43] [Epoch=050/500] [Need: 08:55:57] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [050][000/391]   Time 0.349 (0.349)   Data 0.149 (0.149)   Loss 0.0968 (0.0968)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 14:44:44]
  Epoch: [050][200/391]   Time 0.177 (0.161)   Data 0.000 (0.001)   Loss 0.1444 (0.1663)   Prec@1 96.094 (94.178)   Prec@5 100.000 (99.922)   [2018-05-03 14:45:16]
  **Train** Prec@1 93.840 Prec@5 99.932 Error@1 6.160
  **Test** Prec@1 88.000 Prec@5 99.640 Error@1 12.000

==>>[2018-05-03 14:45:55] [Epoch=051/500] [Need: 08:54:47] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [051][000/391]   Time 0.331 (0.331)   Data 0.141 (0.141)   Loss 0.2213 (0.2213)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:45:55]
  Epoch: [051][200/391]   Time 0.173 (0.164)   Data 0.001 (0.002)   Loss 0.1088 (0.1537)   Prec@1 97.656 (94.679)   Prec@5 100.000 (99.973)   [2018-05-03 14:46:28]
  **Train** Prec@1 94.216 Prec@5 99.956 Error@1 5.784
  **Test** Prec@1 87.020 Prec@5 99.450 Error@1 12.980

==>>[2018-05-03 14:47:08] [Epoch=052/500] [Need: 08:53:46] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [052][000/391]   Time 0.355 (0.355)   Data 0.175 (0.175)   Loss 0.0792 (0.0792)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 14:47:08]
  Epoch: [052][200/391]   Time 0.160 (0.164)   Data 0.001 (0.002)   Loss 0.1260 (0.1622)   Prec@1 93.750 (94.317)   Prec@5 100.000 (99.926)   [2018-05-03 14:47:41]
  **Train** Prec@1 94.254 Prec@5 99.930 Error@1 5.746
  **Test** Prec@1 87.640 Prec@5 99.550 Error@1 12.360

==>>[2018-05-03 14:48:20] [Epoch=053/500] [Need: 08:52:43] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [053][000/391]   Time 0.303 (0.303)   Data 0.127 (0.127)   Loss 0.1669 (0.1669)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 14:48:21]
  Epoch: [053][200/391]   Time 0.160 (0.165)   Data 0.000 (0.001)   Loss 0.1569 (0.1562)   Prec@1 93.750 (94.706)   Prec@5 100.000 (99.949)   [2018-05-03 14:48:54]
  **Train** Prec@1 94.320 Prec@5 99.932 Error@1 5.680
  **Test** Prec@1 86.680 Prec@5 99.490 Error@1 13.320

==>>[2018-05-03 14:49:34] [Epoch=054/500] [Need: 08:51:46] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [054][000/391]   Time 0.293 (0.293)   Data 0.128 (0.128)   Loss 0.1377 (0.1377)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 14:49:34]
  Epoch: [054][200/391]   Time 0.162 (0.163)   Data 0.001 (0.001)   Loss 0.1275 (0.1611)   Prec@1 95.312 (94.419)   Prec@5 100.000 (99.934)   [2018-05-03 14:50:06]
  **Train** Prec@1 94.250 Prec@5 99.932 Error@1 5.750
  **Test** Prec@1 86.960 Prec@5 99.300 Error@1 13.040

==>>[2018-05-03 14:50:47] [Epoch=055/500] [Need: 08:50:47] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [055][000/391]   Time 0.402 (0.402)   Data 0.200 (0.200)   Loss 0.1816 (0.1816)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 14:50:47]
  Epoch: [055][200/391]   Time 0.175 (0.164)   Data 0.001 (0.002)   Loss 0.3751 (0.1615)   Prec@1 86.719 (94.325)   Prec@5 100.000 (99.899)   [2018-05-03 14:51:20]
  **Train** Prec@1 94.126 Prec@5 99.918 Error@1 5.874
  **Test** Prec@1 85.910 Prec@5 99.310 Error@1 14.090

==>>[2018-05-03 14:51:59] [Epoch=056/500] [Need: 08:49:40] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [056][000/391]   Time 0.336 (0.336)   Data 0.139 (0.139)   Loss 0.1152 (0.1152)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 14:51:59]
  Epoch: [056][200/391]   Time 0.179 (0.160)   Data 0.001 (0.001)   Loss 0.1386 (0.1503)   Prec@1 94.531 (94.780)   Prec@5 100.000 (99.942)   [2018-05-03 14:52:31]
  **Train** Prec@1 94.364 Prec@5 99.932 Error@1 5.636
  **Test** Prec@1 89.080 Prec@5 99.580 Error@1 10.920

==>>[2018-05-03 14:53:10] [Epoch=057/500] [Need: 08:48:26] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [057][000/391]   Time 0.401 (0.401)   Data 0.185 (0.185)   Loss 0.1035 (0.1035)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 14:53:11]
  Epoch: [057][200/391]   Time 0.167 (0.162)   Data 0.001 (0.002)   Loss 0.1611 (0.1573)   Prec@1 93.750 (94.648)   Prec@5 100.000 (99.946)   [2018-05-03 14:53:43]
  **Train** Prec@1 94.444 Prec@5 99.944 Error@1 5.556
  **Test** Prec@1 88.950 Prec@5 99.620 Error@1 11.050

==>>[2018-05-03 14:54:23] [Epoch=058/500] [Need: 08:47:25] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [058][000/391]   Time 0.384 (0.384)   Data 0.179 (0.179)   Loss 0.0856 (0.0856)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 14:54:23]
  Epoch: [058][200/391]   Time 0.154 (0.161)   Data 0.001 (0.002)   Loss 0.1949 (0.1503)   Prec@1 92.969 (94.605)   Prec@5 99.219 (99.949)   [2018-05-03 14:54:55]
  **Train** Prec@1 94.352 Prec@5 99.946 Error@1 5.648
  **Test** Prec@1 87.440 Prec@5 99.440 Error@1 12.560

==>>[2018-05-03 14:55:34] [Epoch=059/500] [Need: 08:46:08] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [059][000/391]   Time 0.358 (0.358)   Data 0.184 (0.184)   Loss 0.1146 (0.1146)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 14:55:34]
  Epoch: [059][200/391]   Time 0.159 (0.169)   Data 0.001 (0.002)   Loss 0.1251 (0.1601)   Prec@1 94.531 (94.461)   Prec@5 100.000 (99.957)   [2018-05-03 14:56:08]
  **Train** Prec@1 94.446 Prec@5 99.926 Error@1 5.554
  **Test** Prec@1 88.680 Prec@5 99.580 Error@1 11.320

==>>[2018-05-03 14:56:47] [Epoch=060/500] [Need: 08:45:07] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [060][000/391]   Time 0.306 (0.306)   Data 0.131 (0.131)   Loss 0.1664 (0.1664)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 14:56:47]
  Epoch: [060][200/391]   Time 0.153 (0.156)   Data 0.001 (0.001)   Loss 0.1342 (0.1483)   Prec@1 96.875 (94.831)   Prec@5 100.000 (99.930)   [2018-05-03 14:57:18]
  **Train** Prec@1 94.512 Prec@5 99.936 Error@1 5.488
  **Test** Prec@1 88.850 Prec@5 99.560 Error@1 11.150

==>>[2018-05-03 14:57:56] [Epoch=061/500] [Need: 08:43:36] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [061][000/391]   Time 0.319 (0.319)   Data 0.127 (0.127)   Loss 0.2281 (0.2281)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 14:57:56]
  Epoch: [061][200/391]   Time 0.151 (0.161)   Data 0.001 (0.001)   Loss 0.0790 (0.1505)   Prec@1 97.656 (94.834)   Prec@5 100.000 (99.942)   [2018-05-03 14:58:28]
  **Train** Prec@1 94.518 Prec@5 99.930 Error@1 5.482
  **Test** Prec@1 88.290 Prec@5 99.570 Error@1 11.710

==>>[2018-05-03 14:59:06] [Epoch=062/500] [Need: 08:42:16] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [062][000/391]   Time 0.318 (0.318)   Data 0.130 (0.130)   Loss 0.1306 (0.1306)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 14:59:07]
  Epoch: [062][200/391]   Time 0.192 (0.159)   Data 0.001 (0.001)   Loss 0.1834 (0.1445)   Prec@1 92.969 (95.130)   Prec@5 100.000 (99.957)   [2018-05-03 14:59:38]
  **Train** Prec@1 94.678 Prec@5 99.930 Error@1 5.322
  **Test** Prec@1 88.830 Prec@5 99.640 Error@1 11.170

==>>[2018-05-03 15:00:17] [Epoch=063/500] [Need: 08:40:58] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [063][000/391]   Time 0.341 (0.341)   Data 0.162 (0.162)   Loss 0.1431 (0.1431)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:00:17]
  Epoch: [063][200/391]   Time 0.159 (0.159)   Data 0.001 (0.001)   Loss 0.1466 (0.1476)   Prec@1 93.750 (94.823)   Prec@5 100.000 (99.946)   [2018-05-03 15:00:49]
  **Train** Prec@1 94.616 Prec@5 99.946 Error@1 5.384
  **Test** Prec@1 87.560 Prec@5 99.110 Error@1 12.440

==>>[2018-05-03 15:01:26] [Epoch=064/500] [Need: 08:39:31] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [064][000/391]   Time 0.323 (0.323)   Data 0.164 (0.164)   Loss 0.1597 (0.1597)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 15:01:26]
  Epoch: [064][200/391]   Time 0.158 (0.161)   Data 0.001 (0.002)   Loss 0.1325 (0.1431)   Prec@1 95.312 (95.095)   Prec@5 100.000 (99.981)   [2018-05-03 15:01:58]
  **Train** Prec@1 94.684 Prec@5 99.964 Error@1 5.316
  **Test** Prec@1 89.080 Prec@5 99.680 Error@1 10.920

==>>[2018-05-03 15:02:37] [Epoch=065/500] [Need: 08:38:17] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [065][000/391]   Time 0.332 (0.332)   Data 0.135 (0.135)   Loss 0.0918 (0.0918)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 15:02:38]
  Epoch: [065][200/391]   Time 0.150 (0.157)   Data 0.001 (0.001)   Loss 0.1805 (0.1453)   Prec@1 95.312 (95.009)   Prec@5 100.000 (99.926)   [2018-05-03 15:03:09]
  **Train** Prec@1 94.656 Prec@5 99.936 Error@1 5.344
  **Test** Prec@1 85.740 Prec@5 99.400 Error@1 14.260

==>>[2018-05-03 15:03:47] [Epoch=066/500] [Need: 08:36:52] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [066][000/391]   Time 0.298 (0.298)   Data 0.148 (0.148)   Loss 0.0953 (0.0953)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:03:47]
  Epoch: [066][200/391]   Time 0.170 (0.161)   Data 0.001 (0.002)   Loss 0.0806 (0.1409)   Prec@1 97.656 (95.060)   Prec@5 100.000 (99.961)   [2018-05-03 15:04:19]
  **Train** Prec@1 94.826 Prec@5 99.964 Error@1 5.174
  **Test** Prec@1 87.100 Prec@5 99.550 Error@1 12.900

==>>[2018-05-03 15:04:57] [Epoch=067/500] [Need: 08:35:32] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [067][000/391]   Time 0.342 (0.342)   Data 0.167 (0.167)   Loss 0.1500 (0.1500)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 15:04:57]
  Epoch: [067][200/391]   Time 0.163 (0.159)   Data 0.001 (0.002)   Loss 0.0931 (0.1472)   Prec@1 94.531 (94.873)   Prec@5 100.000 (99.953)   [2018-05-03 15:05:29]
  **Train** Prec@1 94.632 Prec@5 99.952 Error@1 5.368
  **Test** Prec@1 85.620 Prec@5 99.330 Error@1 14.380

==>>[2018-05-03 15:06:08] [Epoch=068/500] [Need: 08:34:16] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [068][000/391]   Time 0.285 (0.285)   Data 0.131 (0.131)   Loss 0.1048 (0.1048)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:06:08]
  Epoch: [068][200/391]   Time 0.185 (0.162)   Data 0.001 (0.001)   Loss 0.1672 (0.1505)   Prec@1 91.406 (94.671)   Prec@5 100.000 (99.938)   [2018-05-03 15:06:40]
  **Train** Prec@1 94.680 Prec@5 99.948 Error@1 5.320
  **Test** Prec@1 87.340 Prec@5 99.640 Error@1 12.660

==>>[2018-05-03 15:07:20] [Epoch=069/500] [Need: 08:33:08] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [069][000/391]   Time 0.309 (0.309)   Data 0.125 (0.125)   Loss 0.0815 (0.0815)   Prec@1 97.656 (97.656)   Prec@5 99.219 (99.219)   [2018-05-03 15:07:20]
  Epoch: [069][200/391]   Time 0.159 (0.159)   Data 0.002 (0.001)   Loss 0.2243 (0.1421)   Prec@1 93.750 (95.052)   Prec@5 100.000 (99.949)   [2018-05-03 15:07:52]
  **Train** Prec@1 94.604 Prec@5 99.940 Error@1 5.396
  **Test** Prec@1 85.880 Prec@5 99.350 Error@1 14.120

==>>[2018-05-03 15:08:30] [Epoch=070/500] [Need: 08:31:53] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [070][000/391]   Time 0.307 (0.307)   Data 0.140 (0.140)   Loss 0.2533 (0.2533)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2018-05-03 15:08:31]
  Epoch: [070][200/391]   Time 0.160 (0.160)   Data 0.001 (0.002)   Loss 0.1717 (0.1410)   Prec@1 92.969 (95.079)   Prec@5 100.000 (99.961)   [2018-05-03 15:09:02]
  **Train** Prec@1 94.776 Prec@5 99.950 Error@1 5.224
  **Test** Prec@1 88.970 Prec@5 99.440 Error@1 11.030

==>>[2018-05-03 15:09:42] [Epoch=071/500] [Need: 08:30:41] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [071][000/391]   Time 0.305 (0.305)   Data 0.124 (0.124)   Loss 0.1641 (0.1641)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:09:42]
  Epoch: [071][200/391]   Time 0.158 (0.162)   Data 0.001 (0.001)   Loss 0.0786 (0.1411)   Prec@1 97.656 (95.122)   Prec@5 100.000 (99.965)   [2018-05-03 15:10:14]
  **Train** Prec@1 94.918 Prec@5 99.946 Error@1 5.082
  **Test** Prec@1 87.350 Prec@5 99.350 Error@1 12.650

==>>[2018-05-03 15:10:53] [Epoch=072/500] [Need: 08:29:28] [learning_rate=0.1000] [Best : Accuracy=89.28, Error=10.72]
  Epoch: [072][000/391]   Time 0.370 (0.370)   Data 0.151 (0.151)   Loss 0.1249 (0.1249)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2018-05-03 15:10:53]
  Epoch: [072][200/391]   Time 0.151 (0.158)   Data 0.001 (0.002)   Loss 0.1385 (0.1406)   Prec@1 95.312 (95.017)   Prec@5 100.000 (99.953)   [2018-05-03 15:11:25]
  **Train** Prec@1 94.798 Prec@5 99.952 Error@1 5.202
  **Test** Prec@1 89.730 Prec@5 99.590 Error@1 10.270

==>>[2018-05-03 15:12:02] [Epoch=073/500] [Need: 08:28:05] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [073][000/391]   Time 0.341 (0.341)   Data 0.144 (0.144)   Loss 0.1794 (0.1794)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 15:12:03]
  Epoch: [073][200/391]   Time 0.142 (0.179)   Data 0.000 (0.001)   Loss 0.0727 (0.1355)   Prec@1 98.438 (95.394)   Prec@5 100.000 (99.942)   [2018-05-03 15:12:38]
  **Train** Prec@1 94.920 Prec@5 99.950 Error@1 5.080
  **Test** Prec@1 88.980 Prec@5 99.590 Error@1 11.020

==>>[2018-05-03 15:13:17] [Epoch=074/500] [Need: 08:27:14] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [074][000/391]   Time 0.336 (0.336)   Data 0.130 (0.130)   Loss 0.1234 (0.1234)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 15:13:18]
  Epoch: [074][200/391]   Time 0.167 (0.161)   Data 0.001 (0.002)   Loss 0.1264 (0.1361)   Prec@1 95.312 (95.192)   Prec@5 100.000 (99.946)   [2018-05-03 15:13:50]
  **Train** Prec@1 94.914 Prec@5 99.944 Error@1 5.086
  **Test** Prec@1 88.370 Prec@5 99.500 Error@1 11.630

==>>[2018-05-03 15:14:28] [Epoch=075/500] [Need: 08:25:59] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [075][000/391]   Time 0.342 (0.342)   Data 0.164 (0.164)   Loss 0.1841 (0.1841)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:14:28]
  Epoch: [075][200/391]   Time 0.163 (0.163)   Data 0.001 (0.001)   Loss 0.0983 (0.1481)   Prec@1 95.312 (94.819)   Prec@5 100.000 (99.914)   [2018-05-03 15:15:01]
  **Train** Prec@1 94.898 Prec@5 99.932 Error@1 5.102
  **Test** Prec@1 88.980 Prec@5 99.660 Error@1 11.020

==>>[2018-05-03 15:15:39] [Epoch=076/500] [Need: 08:24:44] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [076][000/391]   Time 0.325 (0.325)   Data 0.154 (0.154)   Loss 0.1361 (0.1361)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 15:15:39]
  Epoch: [076][200/391]   Time 0.163 (0.159)   Data 0.001 (0.002)   Loss 0.2584 (0.1349)   Prec@1 90.625 (95.192)   Prec@5 100.000 (99.984)   [2018-05-03 15:16:11]
  **Train** Prec@1 94.880 Prec@5 99.956 Error@1 5.120
  **Test** Prec@1 86.900 Prec@5 98.990 Error@1 13.100

==>>[2018-05-03 15:16:49] [Epoch=077/500] [Need: 08:23:28] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [077][000/391]   Time 0.310 (0.310)   Data 0.127 (0.127)   Loss 0.2093 (0.2093)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2018-05-03 15:16:50]
  Epoch: [077][200/391]   Time 0.156 (0.160)   Data 0.001 (0.001)   Loss 0.1266 (0.1422)   Prec@1 94.531 (95.052)   Prec@5 100.000 (99.953)   [2018-05-03 15:17:22]
  **Train** Prec@1 95.050 Prec@5 99.940 Error@1 4.950
  **Test** Prec@1 89.140 Prec@5 99.580 Error@1 10.860

==>>[2018-05-03 15:18:00] [Epoch=078/500] [Need: 08:22:14] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [078][000/391]   Time 0.345 (0.345)   Data 0.122 (0.122)   Loss 0.1477 (0.1477)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:18:01]
  Epoch: [078][200/391]   Time 0.148 (0.162)   Data 0.001 (0.001)   Loss 0.1469 (0.1377)   Prec@1 92.969 (95.180)   Prec@5 100.000 (99.946)   [2018-05-03 15:18:33]
  **Train** Prec@1 94.838 Prec@5 99.934 Error@1 5.162
  **Test** Prec@1 89.590 Prec@5 99.640 Error@1 10.410

==>>[2018-05-03 15:19:11] [Epoch=079/500] [Need: 08:21:01] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [079][000/391]   Time 0.342 (0.342)   Data 0.134 (0.134)   Loss 0.0819 (0.0819)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:19:12]
  Epoch: [079][200/391]   Time 0.145 (0.157)   Data 0.001 (0.001)   Loss 0.1700 (0.1351)   Prec@1 93.750 (95.145)   Prec@5 99.219 (99.942)   [2018-05-03 15:19:43]
  **Train** Prec@1 94.824 Prec@5 99.948 Error@1 5.176
  **Test** Prec@1 86.810 Prec@5 99.380 Error@1 13.190

==>>[2018-05-03 15:20:21] [Epoch=080/500] [Need: 08:19:37] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [080][000/391]   Time 0.326 (0.326)   Data 0.146 (0.146)   Loss 0.1533 (0.1533)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:20:21]
  Epoch: [080][200/391]   Time 0.165 (0.160)   Data 0.001 (0.001)   Loss 0.1876 (0.1400)   Prec@1 95.312 (95.064)   Prec@5 100.000 (99.969)   [2018-05-03 15:20:53]
  **Train** Prec@1 95.154 Prec@5 99.970 Error@1 4.846
  **Test** Prec@1 87.870 Prec@5 99.370 Error@1 12.130

==>>[2018-05-03 15:21:31] [Epoch=081/500] [Need: 08:18:20] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [081][000/391]   Time 0.296 (0.296)   Data 0.122 (0.122)   Loss 0.2028 (0.2028)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:21:31]
  Epoch: [081][200/391]   Time 0.170 (0.159)   Data 0.001 (0.001)   Loss 0.0718 (0.1354)   Prec@1 96.875 (95.231)   Prec@5 100.000 (99.946)   [2018-05-03 15:22:03]
  **Train** Prec@1 95.084 Prec@5 99.946 Error@1 4.916
  **Test** Prec@1 88.840 Prec@5 99.530 Error@1 11.160

==>>[2018-05-03 15:22:41] [Epoch=082/500] [Need: 08:17:03] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [082][000/391]   Time 0.334 (0.334)   Data 0.134 (0.134)   Loss 0.1282 (0.1282)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:22:41]
  Epoch: [082][200/391]   Time 0.150 (0.159)   Data 0.001 (0.001)   Loss 0.1531 (0.1378)   Prec@1 94.531 (95.141)   Prec@5 100.000 (99.938)   [2018-05-03 15:23:13]
  **Train** Prec@1 94.922 Prec@5 99.924 Error@1 5.078
  **Test** Prec@1 87.320 Prec@5 99.470 Error@1 12.680

==>>[2018-05-03 15:23:51] [Epoch=083/500] [Need: 08:15:43] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [083][000/391]   Time 0.278 (0.278)   Data 0.125 (0.125)   Loss 0.1328 (0.1328)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:23:51]
  Epoch: [083][200/391]   Time 0.179 (0.159)   Data 0.001 (0.001)   Loss 0.1324 (0.1328)   Prec@1 94.531 (95.460)   Prec@5 99.219 (99.973)   [2018-05-03 15:24:23]
  **Train** Prec@1 95.064 Prec@5 99.960 Error@1 4.936
  **Test** Prec@1 88.510 Prec@5 99.590 Error@1 11.490

==>>[2018-05-03 15:25:00] [Epoch=084/500] [Need: 08:14:24] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [084][000/391]   Time 0.317 (0.317)   Data 0.130 (0.130)   Loss 0.1167 (0.1167)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:25:01]
  Epoch: [084][200/391]   Time 0.140 (0.158)   Data 0.001 (0.001)   Loss 0.1868 (0.1303)   Prec@1 92.969 (95.499)   Prec@5 100.000 (99.973)   [2018-05-03 15:25:32]
  **Train** Prec@1 95.232 Prec@5 99.956 Error@1 4.768
  **Test** Prec@1 88.500 Prec@5 99.450 Error@1 11.500

==>>[2018-05-03 15:26:11] [Epoch=085/500] [Need: 08:13:06] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [085][000/391]   Time 0.308 (0.308)   Data 0.133 (0.133)   Loss 0.1478 (0.1478)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 15:26:11]
  Epoch: [085][200/391]   Time 0.202 (0.163)   Data 0.001 (0.001)   Loss 0.1245 (0.1330)   Prec@1 94.531 (95.394)   Prec@5 100.000 (99.984)   [2018-05-03 15:26:43]
  **Train** Prec@1 95.128 Prec@5 99.966 Error@1 4.872
  **Test** Prec@1 86.190 Prec@5 99.250 Error@1 13.810

==>>[2018-05-03 15:27:23] [Epoch=086/500] [Need: 08:12:00] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [086][000/391]   Time 0.341 (0.341)   Data 0.131 (0.131)   Loss 0.2051 (0.2051)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:27:23]
  Epoch: [086][200/391]   Time 0.164 (0.160)   Data 0.001 (0.002)   Loss 0.2013 (0.1323)   Prec@1 92.969 (95.573)   Prec@5 100.000 (99.965)   [2018-05-03 15:27:55]
  **Train** Prec@1 95.188 Prec@5 99.966 Error@1 4.812
  **Test** Prec@1 88.510 Prec@5 99.650 Error@1 11.490

==>>[2018-05-03 15:28:33] [Epoch=087/500] [Need: 08:10:46] [learning_rate=0.1000] [Best : Accuracy=89.73, Error=10.27]
  Epoch: [087][000/391]   Time 0.367 (0.367)   Data 0.171 (0.171)   Loss 0.2377 (0.2377)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-05-03 15:28:34]
  Epoch: [087][200/391]   Time 0.173 (0.158)   Data 0.001 (0.002)   Loss 0.1157 (0.1287)   Prec@1 96.875 (95.600)   Prec@5 100.000 (99.957)   [2018-05-03 15:29:05]
  **Train** Prec@1 95.226 Prec@5 99.938 Error@1 4.774
  **Test** Prec@1 89.810 Prec@5 99.560 Error@1 10.190

==>>[2018-05-03 15:29:43] [Epoch=088/500] [Need: 08:09:27] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [088][000/391]   Time 0.343 (0.343)   Data 0.140 (0.140)   Loss 0.0810 (0.0810)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:29:44]
  Epoch: [088][200/391]   Time 0.136 (0.162)   Data 0.001 (0.001)   Loss 0.0888 (0.1353)   Prec@1 96.094 (95.386)   Prec@5 100.000 (99.973)   [2018-05-03 15:30:16]
  **Train** Prec@1 95.302 Prec@5 99.954 Error@1 4.698
  **Test** Prec@1 88.080 Prec@5 99.560 Error@1 11.920

==>>[2018-05-03 15:30:55] [Epoch=089/500] [Need: 08:08:17] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [089][000/391]   Time 0.294 (0.294)   Data 0.119 (0.119)   Loss 0.1887 (0.1887)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:30:55]
  Epoch: [089][200/391]   Time 0.161 (0.164)   Data 0.001 (0.001)   Loss 0.0434 (0.1347)   Prec@1 98.438 (95.243)   Prec@5 100.000 (99.946)   [2018-05-03 15:31:28]
  **Train** Prec@1 95.194 Prec@5 99.944 Error@1 4.806
  **Test** Prec@1 89.210 Prec@5 99.510 Error@1 10.790

==>>[2018-05-03 15:32:06] [Epoch=090/500] [Need: 08:07:06] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [090][000/391]   Time 0.352 (0.352)   Data 0.154 (0.154)   Loss 0.1129 (0.1129)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 15:32:06]
  Epoch: [090][200/391]   Time 0.175 (0.161)   Data 0.001 (0.001)   Loss 0.1299 (0.1417)   Prec@1 95.312 (95.091)   Prec@5 100.000 (99.957)   [2018-05-03 15:32:38]
  **Train** Prec@1 95.138 Prec@5 99.968 Error@1 4.862
  **Test** Prec@1 88.940 Prec@5 99.500 Error@1 11.060

==>>[2018-05-03 15:33:16] [Epoch=091/500] [Need: 08:05:47] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [091][000/391]   Time 0.313 (0.313)   Data 0.137 (0.137)   Loss 0.1204 (0.1204)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:33:16]
  Epoch: [091][200/391]   Time 0.144 (0.160)   Data 0.001 (0.002)   Loss 0.2110 (0.1314)   Prec@1 94.531 (95.398)   Prec@5 100.000 (99.969)   [2018-05-03 15:33:48]
  **Train** Prec@1 95.314 Prec@5 99.964 Error@1 4.686
  **Test** Prec@1 88.530 Prec@5 99.410 Error@1 11.470

==>>[2018-05-03 15:34:26] [Epoch=092/500] [Need: 08:04:34] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [092][000/391]   Time 0.313 (0.313)   Data 0.136 (0.136)   Loss 0.1533 (0.1533)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:34:27]
  Epoch: [092][200/391]   Time 0.166 (0.157)   Data 0.001 (0.001)   Loss 0.1690 (0.1282)   Prec@1 95.312 (95.546)   Prec@5 100.000 (99.988)   [2018-05-03 15:34:58]
  **Train** Prec@1 95.230 Prec@5 99.972 Error@1 4.770
  **Test** Prec@1 87.550 Prec@5 99.540 Error@1 12.450

==>>[2018-05-03 15:35:36] [Epoch=093/500] [Need: 08:03:14] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [093][000/391]   Time 0.375 (0.375)   Data 0.178 (0.178)   Loss 0.1629 (0.1629)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:35:36]
  Epoch: [093][200/391]   Time 0.166 (0.159)   Data 0.000 (0.002)   Loss 0.1157 (0.1346)   Prec@1 96.094 (95.386)   Prec@5 99.219 (99.922)   [2018-05-03 15:36:08]
  **Train** Prec@1 95.254 Prec@5 99.942 Error@1 4.746
  **Test** Prec@1 88.520 Prec@5 99.530 Error@1 11.480

==>>[2018-05-03 15:36:47] [Epoch=094/500] [Need: 08:02:01] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [094][000/391]   Time 0.332 (0.332)   Data 0.126 (0.126)   Loss 0.0755 (0.0755)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:36:47]
  Epoch: [094][200/391]   Time 0.165 (0.157)   Data 0.001 (0.001)   Loss 0.1157 (0.1197)   Prec@1 95.312 (95.923)   Prec@5 100.000 (99.965)   [2018-05-03 15:37:18]
  **Train** Prec@1 95.472 Prec@5 99.956 Error@1 4.528
  **Test** Prec@1 86.750 Prec@5 99.530 Error@1 13.250

==>>[2018-05-03 15:37:56] [Epoch=095/500] [Need: 08:00:42] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [095][000/391]   Time 0.328 (0.328)   Data 0.135 (0.135)   Loss 0.1014 (0.1014)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:37:56]
  Epoch: [095][200/391]   Time 0.146 (0.161)   Data 0.001 (0.001)   Loss 0.2154 (0.1262)   Prec@1 95.312 (95.592)   Prec@5 100.000 (99.934)   [2018-05-03 15:38:28]
  **Train** Prec@1 95.394 Prec@5 99.928 Error@1 4.606
  **Test** Prec@1 87.410 Prec@5 99.390 Error@1 12.590

==>>[2018-05-03 15:39:07] [Epoch=096/500] [Need: 07:59:32] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [096][000/391]   Time 0.337 (0.337)   Data 0.146 (0.146)   Loss 0.0648 (0.0648)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:39:08]
  Epoch: [096][200/391]   Time 0.169 (0.160)   Data 0.001 (0.002)   Loss 0.1495 (0.1309)   Prec@1 95.312 (95.394)   Prec@5 100.000 (99.977)   [2018-05-03 15:39:40]
  **Train** Prec@1 95.250 Prec@5 99.962 Error@1 4.750
  **Test** Prec@1 89.590 Prec@5 99.590 Error@1 10.410

==>>[2018-05-03 15:40:18] [Epoch=097/500] [Need: 07:58:17] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [097][000/391]   Time 0.323 (0.323)   Data 0.130 (0.130)   Loss 0.1096 (0.1096)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:40:18]
  Epoch: [097][200/391]   Time 0.191 (0.164)   Data 0.001 (0.001)   Loss 0.1370 (0.1295)   Prec@1 95.312 (95.491)   Prec@5 100.000 (99.953)   [2018-05-03 15:40:51]
  **Train** Prec@1 95.410 Prec@5 99.956 Error@1 4.590
  **Test** Prec@1 86.070 Prec@5 99.570 Error@1 13.930

==>>[2018-05-03 15:41:29] [Epoch=098/500] [Need: 07:57:06] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [098][000/391]   Time 0.333 (0.333)   Data 0.131 (0.131)   Loss 0.0949 (0.0949)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:41:29]
  Epoch: [098][200/391]   Time 0.171 (0.157)   Data 0.001 (0.001)   Loss 0.1395 (0.1240)   Prec@1 95.312 (95.631)   Prec@5 100.000 (99.984)   [2018-05-03 15:42:01]
  **Train** Prec@1 95.226 Prec@5 99.964 Error@1 4.774
  **Test** Prec@1 87.740 Prec@5 99.470 Error@1 12.260

==>>[2018-05-03 15:42:39] [Epoch=099/500] [Need: 07:55:50] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [099][000/391]   Time 0.318 (0.318)   Data 0.151 (0.151)   Loss 0.1115 (0.1115)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:42:40]
  Epoch: [099][200/391]   Time 0.140 (0.161)   Data 0.001 (0.002)   Loss 0.1667 (0.1337)   Prec@1 92.969 (95.484)   Prec@5 100.000 (99.965)   [2018-05-03 15:43:12]
  **Train** Prec@1 95.388 Prec@5 99.956 Error@1 4.612
  **Test** Prec@1 88.770 Prec@5 99.660 Error@1 11.230

==>>[2018-05-03 15:43:51] [Epoch=100/500] [Need: 07:54:40] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [100][000/391]   Time 0.340 (0.340)   Data 0.154 (0.154)   Loss 0.0763 (0.0763)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:43:51]
  Epoch: [100][200/391]   Time 0.152 (0.165)   Data 0.001 (0.001)   Loss 0.1870 (0.1238)   Prec@1 92.188 (95.701)   Prec@5 100.000 (99.957)   [2018-05-03 15:44:24]
  **Train** Prec@1 95.536 Prec@5 99.950 Error@1 4.464
  **Test** Prec@1 87.660 Prec@5 99.320 Error@1 12.340

==>>[2018-05-03 15:45:02] [Epoch=101/500] [Need: 07:53:29] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [101][000/391]   Time 0.318 (0.318)   Data 0.145 (0.145)   Loss 0.1545 (0.1545)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 15:45:02]
  Epoch: [101][200/391]   Time 0.168 (0.160)   Data 0.001 (0.002)   Loss 0.1620 (0.1324)   Prec@1 93.750 (95.386)   Prec@5 100.000 (99.957)   [2018-05-03 15:45:34]
  **Train** Prec@1 95.330 Prec@5 99.950 Error@1 4.670
  **Test** Prec@1 89.090 Prec@5 99.600 Error@1 10.910

==>>[2018-05-03 15:46:12] [Epoch=102/500] [Need: 07:52:14] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [102][000/391]   Time 0.361 (0.361)   Data 0.156 (0.156)   Loss 0.1151 (0.1151)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2018-05-03 15:46:12]
  Epoch: [102][200/391]   Time 0.166 (0.164)   Data 0.001 (0.002)   Loss 0.1632 (0.1177)   Prec@1 94.531 (95.899)   Prec@5 100.000 (99.977)   [2018-05-03 15:46:45]
  **Train** Prec@1 95.586 Prec@5 99.960 Error@1 4.414
  **Test** Prec@1 88.150 Prec@5 99.560 Error@1 11.850

==>>[2018-05-03 15:47:23] [Epoch=103/500] [Need: 07:51:03] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [103][000/391]   Time 0.301 (0.301)   Data 0.128 (0.128)   Loss 0.0737 (0.0737)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 15:47:24]
  Epoch: [103][200/391]   Time 0.163 (0.162)   Data 0.001 (0.001)   Loss 0.1817 (0.1285)   Prec@1 92.188 (95.534)   Prec@5 100.000 (99.965)   [2018-05-03 15:47:56]
  **Train** Prec@1 95.290 Prec@5 99.952 Error@1 4.710
  **Test** Prec@1 88.190 Prec@5 99.500 Error@1 11.810

==>>[2018-05-03 15:48:35] [Epoch=104/500] [Need: 07:49:53] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [104][000/391]   Time 0.328 (0.328)   Data 0.140 (0.140)   Loss 0.0821 (0.0821)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:48:35]
  Epoch: [104][200/391]   Time 0.174 (0.161)   Data 0.001 (0.002)   Loss 0.1027 (0.1148)   Prec@1 97.656 (96.105)   Prec@5 100.000 (99.984)   [2018-05-03 15:49:07]
  **Train** Prec@1 95.640 Prec@5 99.966 Error@1 4.360
  **Test** Prec@1 89.670 Prec@5 99.510 Error@1 10.330

==>>[2018-05-03 15:49:46] [Epoch=105/500] [Need: 07:48:41] [learning_rate=0.1000] [Best : Accuracy=89.81, Error=10.19]
  Epoch: [105][000/391]   Time 0.358 (0.358)   Data 0.151 (0.151)   Loss 0.1710 (0.1710)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:49:46]
  Epoch: [105][200/391]   Time 0.163 (0.156)   Data 0.001 (0.001)   Loss 0.0789 (0.1257)   Prec@1 96.875 (95.511)   Prec@5 100.000 (99.953)   [2018-05-03 15:50:17]
  **Train** Prec@1 95.288 Prec@5 99.946 Error@1 4.712
  **Test** Prec@1 89.990 Prec@5 99.670 Error@1 10.010

==>>[2018-05-03 15:50:55] [Epoch=106/500] [Need: 07:47:22] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [106][000/391]   Time 0.339 (0.339)   Data 0.167 (0.167)   Loss 0.0849 (0.0849)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:50:55]
  Epoch: [106][200/391]   Time 0.168 (0.160)   Data 0.001 (0.002)   Loss 0.1590 (0.1141)   Prec@1 93.750 (96.140)   Prec@5 100.000 (99.969)   [2018-05-03 15:51:27]
  **Train** Prec@1 95.746 Prec@5 99.962 Error@1 4.254
  **Test** Prec@1 88.410 Prec@5 99.540 Error@1 11.590

==>>[2018-05-03 15:52:06] [Epoch=107/500] [Need: 07:46:10] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [107][000/391]   Time 0.340 (0.340)   Data 0.129 (0.129)   Loss 0.1254 (0.1254)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 15:52:06]
  Epoch: [107][200/391]   Time 0.139 (0.162)   Data 0.001 (0.001)   Loss 0.1732 (0.1323)   Prec@1 92.969 (95.227)   Prec@5 100.000 (99.953)   [2018-05-03 15:52:38]
  **Train** Prec@1 95.152 Prec@5 99.950 Error@1 4.848
  **Test** Prec@1 86.810 Prec@5 99.410 Error@1 13.190

==>>[2018-05-03 15:53:17] [Epoch=108/500] [Need: 07:44:57] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [108][000/391]   Time 0.325 (0.325)   Data 0.135 (0.135)   Loss 0.0899 (0.0899)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 15:53:17]
  Epoch: [108][200/391]   Time 0.156 (0.161)   Data 0.001 (0.001)   Loss 0.1383 (0.1194)   Prec@1 97.656 (95.880)   Prec@5 100.000 (99.957)   [2018-05-03 15:53:49]
  **Train** Prec@1 95.662 Prec@5 99.952 Error@1 4.338
  **Test** Prec@1 88.820 Prec@5 99.460 Error@1 11.180

==>>[2018-05-03 15:54:28] [Epoch=109/500] [Need: 07:43:47] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [109][000/391]   Time 0.395 (0.395)   Data 0.189 (0.189)   Loss 0.1694 (0.1694)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 15:54:28]
  Epoch: [109][200/391]   Time 0.170 (0.165)   Data 0.001 (0.002)   Loss 0.1226 (0.1281)   Prec@1 97.656 (95.596)   Prec@5 100.000 (99.965)   [2018-05-03 15:55:01]
  **Train** Prec@1 95.474 Prec@5 99.958 Error@1 4.526
  **Test** Prec@1 88.500 Prec@5 99.550 Error@1 11.500

==>>[2018-05-03 15:55:41] [Epoch=110/500] [Need: 07:42:41] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [110][000/391]   Time 0.336 (0.336)   Data 0.130 (0.130)   Loss 0.1383 (0.1383)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 15:55:41]
  Epoch: [110][200/391]   Time 0.161 (0.161)   Data 0.001 (0.002)   Loss 0.1988 (0.1238)   Prec@1 94.531 (95.635)   Prec@5 100.000 (99.977)   [2018-05-03 15:56:13]
  **Train** Prec@1 95.390 Prec@5 99.964 Error@1 4.610
  **Test** Prec@1 88.140 Prec@5 99.620 Error@1 11.860

==>>[2018-05-03 15:56:52] [Epoch=111/500] [Need: 07:41:29] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [111][000/391]   Time 0.363 (0.363)   Data 0.156 (0.156)   Loss 0.1683 (0.1683)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 15:56:52]
  Epoch: [111][200/391]   Time 0.146 (0.160)   Data 0.001 (0.002)   Loss 0.2282 (0.1310)   Prec@1 91.406 (95.429)   Prec@5 100.000 (99.977)   [2018-05-03 15:57:24]
  **Train** Prec@1 95.450 Prec@5 99.962 Error@1 4.550
  **Test** Prec@1 88.460 Prec@5 99.530 Error@1 11.540

==>>[2018-05-03 15:58:02] [Epoch=112/500] [Need: 07:40:15] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [112][000/391]   Time 0.317 (0.317)   Data 0.142 (0.142)   Loss 0.1615 (0.1615)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 15:58:02]
  Epoch: [112][200/391]   Time 0.169 (0.161)   Data 0.001 (0.002)   Loss 0.1078 (0.1237)   Prec@1 96.094 (95.690)   Prec@5 100.000 (99.953)   [2018-05-03 15:58:34]
  **Train** Prec@1 95.584 Prec@5 99.948 Error@1 4.416
  **Test** Prec@1 89.590 Prec@5 99.580 Error@1 10.410

==>>[2018-05-03 15:59:12] [Epoch=113/500] [Need: 07:39:01] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [113][000/391]   Time 0.360 (0.360)   Data 0.165 (0.165)   Loss 0.0800 (0.0800)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 15:59:13]
  Epoch: [113][200/391]   Time 0.161 (0.158)   Data 0.001 (0.001)   Loss 0.1012 (0.1289)   Prec@1 96.094 (95.359)   Prec@5 100.000 (99.973)   [2018-05-03 15:59:44]
  **Train** Prec@1 95.372 Prec@5 99.956 Error@1 4.628
  **Test** Prec@1 88.440 Prec@5 99.370 Error@1 11.560

==>>[2018-05-03 16:00:22] [Epoch=114/500] [Need: 07:37:45] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [114][000/391]   Time 0.306 (0.306)   Data 0.126 (0.126)   Loss 0.1497 (0.1497)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 16:00:22]
  Epoch: [114][200/391]   Time 0.159 (0.163)   Data 0.001 (0.002)   Loss 0.1398 (0.1210)   Prec@1 95.312 (95.876)   Prec@5 100.000 (99.953)   [2018-05-03 16:00:55]
  **Train** Prec@1 95.538 Prec@5 99.956 Error@1 4.462
  **Test** Prec@1 89.360 Prec@5 99.560 Error@1 10.640

==>>[2018-05-03 16:01:33] [Epoch=115/500] [Need: 07:36:32] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [115][000/391]   Time 0.305 (0.305)   Data 0.128 (0.128)   Loss 0.1792 (0.1792)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 16:01:33]
  Epoch: [115][200/391]   Time 0.178 (0.160)   Data 0.001 (0.001)   Loss 0.0781 (0.1251)   Prec@1 97.656 (95.717)   Prec@5 100.000 (99.961)   [2018-05-03 16:02:05]
  **Train** Prec@1 95.590 Prec@5 99.966 Error@1 4.410
  **Test** Prec@1 88.780 Prec@5 99.500 Error@1 11.220

==>>[2018-05-03 16:02:44] [Epoch=116/500] [Need: 07:35:21] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [116][000/391]   Time 0.346 (0.346)   Data 0.143 (0.143)   Loss 0.0798 (0.0798)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:02:44]
  Epoch: [116][200/391]   Time 0.133 (0.157)   Data 0.001 (0.002)   Loss 0.0978 (0.1230)   Prec@1 99.219 (95.868)   Prec@5 100.000 (99.946)   [2018-05-03 16:03:15]
  **Train** Prec@1 95.594 Prec@5 99.962 Error@1 4.406
  **Test** Prec@1 87.580 Prec@5 99.570 Error@1 12.420

==>>[2018-05-03 16:03:53] [Epoch=117/500] [Need: 07:34:03] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [117][000/391]   Time 0.284 (0.284)   Data 0.126 (0.126)   Loss 0.1033 (0.1033)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 16:03:53]
  Epoch: [117][200/391]   Time 0.155 (0.162)   Data 0.001 (0.002)   Loss 0.0911 (0.1211)   Prec@1 96.875 (95.744)   Prec@5 100.000 (99.965)   [2018-05-03 16:04:25]
  **Train** Prec@1 95.524 Prec@5 99.966 Error@1 4.476
  **Test** Prec@1 89.050 Prec@5 99.630 Error@1 10.950

==>>[2018-05-03 16:05:04] [Epoch=118/500] [Need: 07:32:51] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [118][000/391]   Time 0.354 (0.354)   Data 0.147 (0.147)   Loss 0.0856 (0.0856)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:05:04]
  Epoch: [118][200/391]   Time 0.180 (0.159)   Data 0.001 (0.001)   Loss 0.2201 (0.1253)   Prec@1 93.750 (95.783)   Prec@5 100.000 (99.981)   [2018-05-03 16:05:36]
  **Train** Prec@1 95.588 Prec@5 99.970 Error@1 4.412
  **Test** Prec@1 88.750 Prec@5 99.510 Error@1 11.250

==>>[2018-05-03 16:06:14] [Epoch=119/500] [Need: 07:31:38] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [119][000/391]   Time 0.339 (0.339)   Data 0.146 (0.146)   Loss 0.0995 (0.0995)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:06:15]
  Epoch: [119][200/391]   Time 0.152 (0.159)   Data 0.001 (0.002)   Loss 0.1293 (0.1195)   Prec@1 95.312 (95.868)   Prec@5 100.000 (99.965)   [2018-05-03 16:06:46]
  **Train** Prec@1 95.560 Prec@5 99.954 Error@1 4.440
  **Test** Prec@1 88.830 Prec@5 99.640 Error@1 11.170

==>>[2018-05-03 16:07:26] [Epoch=120/500] [Need: 07:30:28] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [120][000/391]   Time 0.362 (0.362)   Data 0.171 (0.171)   Loss 0.1017 (0.1017)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:07:26]
  Epoch: [120][200/391]   Time 0.159 (0.159)   Data 0.001 (0.002)   Loss 0.1980 (0.1238)   Prec@1 92.969 (95.565)   Prec@5 100.000 (99.969)   [2018-05-03 16:07:58]
  **Train** Prec@1 95.390 Prec@5 99.966 Error@1 4.610
  **Test** Prec@1 88.730 Prec@5 99.440 Error@1 11.270

==>>[2018-05-03 16:08:36] [Epoch=121/500] [Need: 07:29:14] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [121][000/391]   Time 0.306 (0.306)   Data 0.128 (0.128)   Loss 0.1306 (0.1306)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 16:08:36]
  Epoch: [121][200/391]   Time 0.161 (0.156)   Data 0.001 (0.001)   Loss 0.1502 (0.1202)   Prec@1 95.312 (95.942)   Prec@5 100.000 (99.988)   [2018-05-03 16:09:08]
  **Train** Prec@1 95.750 Prec@5 99.968 Error@1 4.250
  **Test** Prec@1 88.480 Prec@5 99.650 Error@1 11.520

==>>[2018-05-03 16:09:46] [Epoch=122/500] [Need: 07:27:58] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [122][000/391]   Time 0.337 (0.337)   Data 0.155 (0.155)   Loss 0.0449 (0.0449)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 16:09:46]
  Epoch: [122][200/391]   Time 0.141 (0.161)   Data 0.001 (0.002)   Loss 0.0330 (0.1251)   Prec@1 99.219 (95.740)   Prec@5 100.000 (99.926)   [2018-05-03 16:10:18]
  **Train** Prec@1 95.542 Prec@5 99.946 Error@1 4.458
  **Test** Prec@1 88.850 Prec@5 99.530 Error@1 11.150

==>>[2018-05-03 16:10:56] [Epoch=123/500] [Need: 07:26:45] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [123][000/391]   Time 0.313 (0.313)   Data 0.151 (0.151)   Loss 0.1582 (0.1582)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 16:10:56]
  Epoch: [123][200/391]   Time 0.184 (0.161)   Data 0.001 (0.001)   Loss 0.1113 (0.1185)   Prec@1 96.094 (95.923)   Prec@5 100.000 (99.957)   [2018-05-03 16:11:28]
  **Train** Prec@1 95.750 Prec@5 99.958 Error@1 4.250
  **Test** Prec@1 89.020 Prec@5 99.480 Error@1 10.980

==>>[2018-05-03 16:12:06] [Epoch=124/500] [Need: 07:25:31] [learning_rate=0.1000] [Best : Accuracy=89.99, Error=10.01]
  Epoch: [124][000/391]   Time 0.340 (0.340)   Data 0.162 (0.162)   Loss 0.1407 (0.1407)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2018-05-03 16:12:06]
  Epoch: [124][200/391]   Time 0.148 (0.162)   Data 0.001 (0.002)   Loss 0.2023 (0.1255)   Prec@1 92.188 (95.686)   Prec@5 100.000 (99.965)   [2018-05-03 16:12:39]
  **Train** Prec@1 95.570 Prec@5 99.964 Error@1 4.430
  **Test** Prec@1 90.130 Prec@5 99.680 Error@1 9.870

==>>[2018-05-03 16:13:17] [Epoch=125/500] [Need: 07:24:18] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [125][000/391]   Time 0.354 (0.354)   Data 0.140 (0.140)   Loss 0.1414 (0.1414)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 16:13:17]
  Epoch: [125][200/391]   Time 0.144 (0.160)   Data 0.001 (0.001)   Loss 0.1692 (0.1182)   Prec@1 94.531 (96.012)   Prec@5 100.000 (99.949)   [2018-05-03 16:13:49]
  **Train** Prec@1 95.714 Prec@5 99.956 Error@1 4.286
  **Test** Prec@1 88.560 Prec@5 99.620 Error@1 11.440

==>>[2018-05-03 16:14:27] [Epoch=126/500] [Need: 07:23:05] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [126][000/391]   Time 0.371 (0.371)   Data 0.189 (0.189)   Loss 0.1804 (0.1804)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2018-05-03 16:14:27]
  Epoch: [126][200/391]   Time 0.172 (0.161)   Data 0.001 (0.002)   Loss 0.1125 (0.1189)   Prec@1 96.875 (95.861)   Prec@5 100.000 (99.965)   [2018-05-03 16:15:00]
  **Train** Prec@1 95.716 Prec@5 99.960 Error@1 4.284
  **Test** Prec@1 88.800 Prec@5 99.450 Error@1 11.200

==>>[2018-05-03 16:15:39] [Epoch=127/500] [Need: 07:21:56] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [127][000/391]   Time 0.349 (0.349)   Data 0.148 (0.148)   Loss 0.1196 (0.1196)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:15:39]
  Epoch: [127][200/391]   Time 0.171 (0.161)   Data 0.001 (0.002)   Loss 0.1126 (0.1209)   Prec@1 94.531 (95.853)   Prec@5 100.000 (99.953)   [2018-05-03 16:16:11]
  **Train** Prec@1 95.592 Prec@5 99.950 Error@1 4.408
  **Test** Prec@1 90.090 Prec@5 99.690 Error@1 9.910

==>>[2018-05-03 16:16:50] [Epoch=128/500] [Need: 07:20:45] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [128][000/391]   Time 0.323 (0.323)   Data 0.129 (0.129)   Loss 0.0640 (0.0640)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 16:16:50]
  Epoch: [128][200/391]   Time 0.163 (0.161)   Data 0.001 (0.001)   Loss 0.0739 (0.1191)   Prec@1 97.656 (95.997)   Prec@5 100.000 (99.969)   [2018-05-03 16:17:22]
  **Train** Prec@1 95.762 Prec@5 99.966 Error@1 4.238
  **Test** Prec@1 89.730 Prec@5 99.570 Error@1 10.270

==>>[2018-05-03 16:18:01] [Epoch=129/500] [Need: 07:19:35] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [129][000/391]   Time 0.333 (0.333)   Data 0.150 (0.150)   Loss 0.1011 (0.1011)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:18:02]
  Epoch: [129][200/391]   Time 0.170 (0.161)   Data 0.001 (0.002)   Loss 0.1164 (0.1192)   Prec@1 96.875 (95.907)   Prec@5 100.000 (99.984)   [2018-05-03 16:18:34]
  **Train** Prec@1 95.720 Prec@5 99.980 Error@1 4.280
  **Test** Prec@1 88.580 Prec@5 99.580 Error@1 11.420

==>>[2018-05-03 16:19:13] [Epoch=130/500] [Need: 07:18:25] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [130][000/391]   Time 0.291 (0.291)   Data 0.130 (0.130)   Loss 0.1206 (0.1206)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 16:19:13]
  Epoch: [130][200/391]   Time 0.200 (0.163)   Data 0.001 (0.001)   Loss 0.1789 (0.1127)   Prec@1 92.969 (96.168)   Prec@5 100.000 (99.981)   [2018-05-03 16:19:46]
  **Train** Prec@1 95.638 Prec@5 99.976 Error@1 4.362
  **Test** Prec@1 88.100 Prec@5 99.450 Error@1 11.900

==>>[2018-05-03 16:20:25] [Epoch=131/500] [Need: 07:17:15] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [131][000/391]   Time 0.361 (0.361)   Data 0.159 (0.159)   Loss 0.0950 (0.0950)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:20:25]
  Epoch: [131][200/391]   Time 0.162 (0.162)   Data 0.001 (0.002)   Loss 0.1524 (0.1183)   Prec@1 95.312 (96.051)   Prec@5 100.000 (99.977)   [2018-05-03 16:20:57]
  **Train** Prec@1 95.670 Prec@5 99.962 Error@1 4.330
  **Test** Prec@1 88.930 Prec@5 99.370 Error@1 11.070

==>>[2018-05-03 16:21:36] [Epoch=132/500] [Need: 07:16:05] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [132][000/391]   Time 0.382 (0.382)   Data 0.181 (0.181)   Loss 0.1108 (0.1108)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 16:21:36]
  Epoch: [132][200/391]   Time 0.171 (0.157)   Data 0.001 (0.002)   Loss 0.1051 (0.1213)   Prec@1 96.875 (95.822)   Prec@5 100.000 (99.973)   [2018-05-03 16:22:07]
  **Train** Prec@1 95.652 Prec@5 99.964 Error@1 4.348
  **Test** Prec@1 87.560 Prec@5 99.330 Error@1 12.440

==>>[2018-05-03 16:22:46] [Epoch=133/500] [Need: 07:14:51] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [133][000/391]   Time 0.335 (0.335)   Data 0.149 (0.149)   Loss 0.0978 (0.0978)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:22:46]
  Epoch: [133][200/391]   Time 0.177 (0.160)   Data 0.001 (0.001)   Loss 0.1288 (0.1176)   Prec@1 93.750 (95.861)   Prec@5 100.000 (99.984)   [2018-05-03 16:23:18]
  **Train** Prec@1 95.756 Prec@5 99.970 Error@1 4.244
  **Test** Prec@1 85.400 Prec@5 99.410 Error@1 14.600

==>>[2018-05-03 16:23:57] [Epoch=134/500] [Need: 07:13:39] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [134][000/391]   Time 0.312 (0.312)   Data 0.128 (0.128)   Loss 0.0707 (0.0707)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:23:57]
  Epoch: [134][200/391]   Time 0.164 (0.159)   Data 0.001 (0.001)   Loss 0.2058 (0.1155)   Prec@1 93.750 (96.012)   Prec@5 99.219 (99.973)   [2018-05-03 16:24:29]
  **Train** Prec@1 95.646 Prec@5 99.966 Error@1 4.354
  **Test** Prec@1 88.280 Prec@5 99.280 Error@1 11.720

==>>[2018-05-03 16:25:08] [Epoch=135/500] [Need: 07:12:27] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [135][000/391]   Time 0.326 (0.326)   Data 0.136 (0.136)   Loss 0.1338 (0.1338)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 16:25:08]
  Epoch: [135][200/391]   Time 0.142 (0.163)   Data 0.001 (0.001)   Loss 0.1011 (0.1208)   Prec@1 96.094 (95.837)   Prec@5 100.000 (99.957)   [2018-05-03 16:25:40]
  **Train** Prec@1 95.710 Prec@5 99.960 Error@1 4.290
  **Test** Prec@1 88.840 Prec@5 99.640 Error@1 11.160

==>>[2018-05-03 16:26:18] [Epoch=136/500] [Need: 07:11:15] [learning_rate=0.1000] [Best : Accuracy=90.13, Error=9.87]
  Epoch: [136][000/391]   Time 0.331 (0.331)   Data 0.153 (0.153)   Loss 0.1275 (0.1275)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 16:26:18]
  Epoch: [136][200/391]   Time 0.177 (0.159)   Data 0.001 (0.002)   Loss 0.0840 (0.1180)   Prec@1 98.438 (96.121)   Prec@5 100.000 (99.957)   [2018-05-03 16:26:50]
  **Train** Prec@1 96.000 Prec@5 99.968 Error@1 4.000
  **Test** Prec@1 90.230 Prec@5 99.520 Error@1 9.770

==>>[2018-05-03 16:27:28] [Epoch=137/500] [Need: 07:10:01] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [137][000/391]   Time 0.370 (0.370)   Data 0.146 (0.146)   Loss 0.1035 (0.1035)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:27:29]
  Epoch: [137][200/391]   Time 0.157 (0.159)   Data 0.001 (0.002)   Loss 0.0436 (0.1156)   Prec@1 98.438 (95.950)   Prec@5 100.000 (99.973)   [2018-05-03 16:28:00]
  **Train** Prec@1 95.686 Prec@5 99.960 Error@1 4.314
  **Test** Prec@1 89.110 Prec@5 99.570 Error@1 10.890

==>>[2018-05-03 16:28:39] [Epoch=138/500] [Need: 07:08:50] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [138][000/391]   Time 0.345 (0.345)   Data 0.156 (0.156)   Loss 0.1312 (0.1312)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:28:40]
  Epoch: [138][200/391]   Time 0.174 (0.163)   Data 0.001 (0.001)   Loss 0.1273 (0.1243)   Prec@1 96.094 (95.896)   Prec@5 100.000 (99.965)   [2018-05-03 16:29:12]
  **Train** Prec@1 95.862 Prec@5 99.970 Error@1 4.138
  **Test** Prec@1 88.200 Prec@5 99.660 Error@1 11.800

==>>[2018-05-03 16:29:50] [Epoch=139/500] [Need: 07:07:38] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [139][000/391]   Time 0.330 (0.330)   Data 0.129 (0.129)   Loss 0.1148 (0.1148)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2018-05-03 16:29:50]
  Epoch: [139][200/391]   Time 0.139 (0.160)   Data 0.001 (0.001)   Loss 0.2417 (0.1231)   Prec@1 92.969 (95.822)   Prec@5 100.000 (99.981)   [2018-05-03 16:30:22]
  **Train** Prec@1 95.554 Prec@5 99.968 Error@1 4.446
  **Test** Prec@1 88.690 Prec@5 99.580 Error@1 11.310

==>>[2018-05-03 16:31:01] [Epoch=140/500] [Need: 07:06:26] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [140][000/391]   Time 0.323 (0.323)   Data 0.126 (0.126)   Loss 0.1020 (0.1020)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:31:01]
  Epoch: [140][200/391]   Time 0.181 (0.161)   Data 0.001 (0.001)   Loss 0.1769 (0.1215)   Prec@1 97.656 (95.822)   Prec@5 99.219 (99.973)   [2018-05-03 16:31:33]
  **Train** Prec@1 95.708 Prec@5 99.970 Error@1 4.292
  **Test** Prec@1 89.350 Prec@5 99.710 Error@1 10.650

==>>[2018-05-03 16:32:11] [Epoch=141/500] [Need: 07:05:14] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [141][000/391]   Time 0.327 (0.327)   Data 0.140 (0.140)   Loss 0.0375 (0.0375)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 16:32:12]
  Epoch: [141][200/391]   Time 0.152 (0.160)   Data 0.001 (0.002)   Loss 0.1115 (0.1117)   Prec@1 96.875 (96.156)   Prec@5 100.000 (99.973)   [2018-05-03 16:32:44]
  **Train** Prec@1 95.830 Prec@5 99.972 Error@1 4.170
  **Test** Prec@1 85.340 Prec@5 98.840 Error@1 14.660

==>>[2018-05-03 16:33:22] [Epoch=142/500] [Need: 07:04:03] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [142][000/391]   Time 0.294 (0.294)   Data 0.128 (0.128)   Loss 0.1009 (0.1009)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:33:23]
  Epoch: [142][200/391]   Time 0.153 (0.159)   Data 0.001 (0.001)   Loss 0.1224 (0.1211)   Prec@1 95.312 (95.911)   Prec@5 100.000 (99.973)   [2018-05-03 16:33:54]
  **Train** Prec@1 95.766 Prec@5 99.968 Error@1 4.234
  **Test** Prec@1 87.620 Prec@5 99.450 Error@1 12.380

==>>[2018-05-03 16:34:33] [Epoch=143/500] [Need: 07:02:50] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [143][000/391]   Time 0.317 (0.317)   Data 0.127 (0.127)   Loss 0.0879 (0.0879)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 16:34:33]
  Epoch: [143][200/391]   Time 0.169 (0.159)   Data 0.001 (0.001)   Loss 0.0577 (0.1156)   Prec@1 98.438 (95.923)   Prec@5 100.000 (99.965)   [2018-05-03 16:35:05]
  **Train** Prec@1 95.622 Prec@5 99.954 Error@1 4.378
  **Test** Prec@1 89.240 Prec@5 99.640 Error@1 10.760

==>>[2018-05-03 16:35:44] [Epoch=144/500] [Need: 07:01:38] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [144][000/391]   Time 0.369 (0.369)   Data 0.139 (0.139)   Loss 0.1699 (0.1699)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 16:35:44]
  Epoch: [144][200/391]   Time 0.172 (0.163)   Data 0.001 (0.002)   Loss 0.1880 (0.1136)   Prec@1 93.750 (96.160)   Prec@5 100.000 (99.957)   [2018-05-03 16:36:16]
  **Train** Prec@1 95.956 Prec@5 99.966 Error@1 4.044
  **Test** Prec@1 89.850 Prec@5 99.670 Error@1 10.150

==>>[2018-05-03 16:36:54] [Epoch=145/500] [Need: 07:00:27] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [145][000/391]   Time 0.310 (0.310)   Data 0.134 (0.134)   Loss 0.1267 (0.1267)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:36:55]
  Epoch: [145][200/391]   Time 0.163 (0.159)   Data 0.001 (0.002)   Loss 0.1539 (0.1244)   Prec@1 94.531 (95.655)   Prec@5 99.219 (99.961)   [2018-05-03 16:37:26]
  **Train** Prec@1 95.530 Prec@5 99.950 Error@1 4.470
  **Test** Prec@1 88.950 Prec@5 99.340 Error@1 11.050

==>>[2018-05-03 16:38:06] [Epoch=146/500] [Need: 06:59:17] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [146][000/391]   Time 0.316 (0.316)   Data 0.129 (0.129)   Loss 0.0577 (0.0577)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 16:38:06]
  Epoch: [146][200/391]   Time 0.141 (0.159)   Data 0.001 (0.002)   Loss 0.1528 (0.1158)   Prec@1 96.094 (96.109)   Prec@5 100.000 (99.946)   [2018-05-03 16:38:38]
  **Train** Prec@1 95.868 Prec@5 99.958 Error@1 4.132
  **Test** Prec@1 88.450 Prec@5 99.460 Error@1 11.550

==>>[2018-05-03 16:39:16] [Epoch=147/500] [Need: 06:58:03] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [147][000/391]   Time 0.375 (0.375)   Data 0.181 (0.181)   Loss 0.1173 (0.1173)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 16:39:16]
  Epoch: [147][200/391]   Time 0.168 (0.161)   Data 0.001 (0.002)   Loss 0.0657 (0.1172)   Prec@1 98.438 (95.997)   Prec@5 100.000 (99.961)   [2018-05-03 16:39:48]
  **Train** Prec@1 95.728 Prec@5 99.958 Error@1 4.272
  **Test** Prec@1 88.350 Prec@5 99.550 Error@1 11.650

==>>[2018-05-03 16:40:27] [Epoch=148/500] [Need: 06:56:52] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [148][000/391]   Time 0.344 (0.344)   Data 0.147 (0.147)   Loss 0.0806 (0.0806)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:40:27]
  Epoch: [148][200/391]   Time 0.178 (0.164)   Data 0.001 (0.001)   Loss 0.1102 (0.1137)   Prec@1 95.312 (96.086)   Prec@5 100.000 (99.961)   [2018-05-03 16:41:00]
  **Train** Prec@1 95.870 Prec@5 99.964 Error@1 4.130
  **Test** Prec@1 89.670 Prec@5 99.560 Error@1 10.330

==>>[2018-05-03 16:41:39] [Epoch=149/500] [Need: 06:55:44] [learning_rate=0.1000] [Best : Accuracy=90.23, Error=9.77]
  Epoch: [149][000/391]   Time 0.359 (0.359)   Data 0.172 (0.172)   Loss 0.0634 (0.0634)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:41:40]
  Epoch: [149][200/391]   Time 0.167 (0.159)   Data 0.001 (0.002)   Loss 0.0661 (0.1163)   Prec@1 98.438 (96.109)   Prec@5 100.000 (99.946)   [2018-05-03 16:42:11]
  **Train** Prec@1 95.972 Prec@5 99.960 Error@1 4.028
  **Test** Prec@1 90.700 Prec@5 99.750 Error@1 9.300

==>>[2018-05-03 16:42:50] [Epoch=150/500] [Need: 06:54:32] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [150][000/391]   Time 0.267 (0.267)   Data 0.125 (0.125)   Loss 0.0821 (0.0821)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:42:50]
  Epoch: [150][200/391]   Time 0.152 (0.165)   Data 0.001 (0.001)   Loss 0.1628 (0.1189)   Prec@1 94.531 (95.849)   Prec@5 100.000 (99.957)   [2018-05-03 16:43:23]
  **Train** Prec@1 95.702 Prec@5 99.956 Error@1 4.298
  **Test** Prec@1 89.480 Prec@5 99.530 Error@1 10.520

==>>[2018-05-03 16:44:03] [Epoch=151/500] [Need: 06:53:24] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [151][000/391]   Time 0.352 (0.352)   Data 0.162 (0.162)   Loss 0.0443 (0.0443)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 16:44:03]
  Epoch: [151][200/391]   Time 0.162 (0.159)   Data 0.001 (0.002)   Loss 0.0997 (0.1133)   Prec@1 96.094 (96.109)   Prec@5 100.000 (99.965)   [2018-05-03 16:44:35]
  **Train** Prec@1 95.976 Prec@5 99.962 Error@1 4.024
  **Test** Prec@1 88.330 Prec@5 99.450 Error@1 11.670

==>>[2018-05-03 16:45:12] [Epoch=152/500] [Need: 06:52:10] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [152][000/391]   Time 0.341 (0.341)   Data 0.165 (0.165)   Loss 0.1047 (0.1047)   Prec@1 96.875 (96.875)   Prec@5 99.219 (99.219)   [2018-05-03 16:45:13]
  Epoch: [152][200/391]   Time 0.158 (0.158)   Data 0.001 (0.002)   Loss 0.1946 (0.1155)   Prec@1 93.750 (96.148)   Prec@5 100.000 (99.957)   [2018-05-03 16:45:44]
  **Train** Prec@1 96.012 Prec@5 99.950 Error@1 3.988
  **Test** Prec@1 89.340 Prec@5 99.590 Error@1 10.660

==>>[2018-05-03 16:46:23] [Epoch=153/500] [Need: 06:50:58] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [153][000/391]   Time 0.350 (0.350)   Data 0.127 (0.127)   Loss 0.0856 (0.0856)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:46:23]
  Epoch: [153][200/391]   Time 0.159 (0.163)   Data 0.001 (0.001)   Loss 0.0752 (0.1194)   Prec@1 97.656 (95.997)   Prec@5 100.000 (99.973)   [2018-05-03 16:46:56]
  **Train** Prec@1 95.760 Prec@5 99.964 Error@1 4.240
  **Test** Prec@1 86.350 Prec@5 99.280 Error@1 13.650

==>>[2018-05-03 16:47:35] [Epoch=154/500] [Need: 06:49:48] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [154][000/391]   Time 0.343 (0.343)   Data 0.160 (0.160)   Loss 0.1426 (0.1426)   Prec@1 94.531 (94.531)   Prec@5 99.219 (99.219)   [2018-05-03 16:47:35]
  Epoch: [154][200/391]   Time 0.163 (0.161)   Data 0.001 (0.002)   Loss 0.1251 (0.1130)   Prec@1 96.094 (96.133)   Prec@5 100.000 (99.973)   [2018-05-03 16:48:07]
  **Train** Prec@1 95.886 Prec@5 99.972 Error@1 4.114
  **Test** Prec@1 88.690 Prec@5 99.570 Error@1 11.310

==>>[2018-05-03 16:48:45] [Epoch=155/500] [Need: 06:48:36] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [155][000/391]   Time 0.338 (0.338)   Data 0.152 (0.152)   Loss 0.2782 (0.2782)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2018-05-03 16:48:45]
  Epoch: [155][200/391]   Time 0.175 (0.162)   Data 0.001 (0.001)   Loss 0.0768 (0.1197)   Prec@1 96.875 (95.927)   Prec@5 100.000 (99.965)   [2018-05-03 16:49:17]
  **Train** Prec@1 95.734 Prec@5 99.962 Error@1 4.266
  **Test** Prec@1 86.870 Prec@5 99.510 Error@1 13.130

==>>[2018-05-03 16:49:56] [Epoch=156/500] [Need: 06:47:24] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [156][000/391]   Time 0.373 (0.373)   Data 0.172 (0.172)   Loss 0.1116 (0.1116)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:49:56]
  Epoch: [156][200/391]   Time 0.190 (0.163)   Data 0.001 (0.002)   Loss 0.1995 (0.1163)   Prec@1 95.312 (95.985)   Prec@5 100.000 (99.957)   [2018-05-03 16:50:29]
  **Train** Prec@1 95.886 Prec@5 99.962 Error@1 4.114
  **Test** Prec@1 89.840 Prec@5 99.720 Error@1 10.160

==>>[2018-05-03 16:51:07] [Epoch=157/500] [Need: 06:46:13] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [157][000/391]   Time 0.298 (0.298)   Data 0.128 (0.128)   Loss 0.0655 (0.0655)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:51:07]
  Epoch: [157][200/391]   Time 0.169 (0.160)   Data 0.001 (0.001)   Loss 0.1095 (0.1154)   Prec@1 94.531 (96.098)   Prec@5 100.000 (99.984)   [2018-05-03 16:51:39]
  **Train** Prec@1 95.784 Prec@5 99.960 Error@1 4.216
  **Test** Prec@1 88.940 Prec@5 99.490 Error@1 11.060

==>>[2018-05-03 16:52:18] [Epoch=158/500] [Need: 06:45:03] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [158][000/391]   Time 0.320 (0.320)   Data 0.136 (0.136)   Loss 0.1369 (0.1369)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 16:52:19]
  Epoch: [158][200/391]   Time 0.161 (0.158)   Data 0.001 (0.001)   Loss 0.0856 (0.1126)   Prec@1 96.875 (96.171)   Prec@5 100.000 (99.977)   [2018-05-03 16:52:50]
  **Train** Prec@1 95.824 Prec@5 99.974 Error@1 4.176
  **Test** Prec@1 87.150 Prec@5 99.270 Error@1 12.850

==>>[2018-05-03 16:53:29] [Epoch=159/500] [Need: 06:43:50] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [159][000/391]   Time 0.337 (0.337)   Data 0.136 (0.136)   Loss 0.1065 (0.1065)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 16:53:29]
  Epoch: [159][200/391]   Time 0.157 (0.159)   Data 0.001 (0.001)   Loss 0.0606 (0.1190)   Prec@1 97.656 (95.861)   Prec@5 100.000 (99.969)   [2018-05-03 16:54:01]
  **Train** Prec@1 95.736 Prec@5 99.966 Error@1 4.264
  **Test** Prec@1 89.030 Prec@5 99.650 Error@1 10.970

==>>[2018-05-03 16:54:40] [Epoch=160/500] [Need: 06:42:39] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [160][000/391]   Time 0.321 (0.321)   Data 0.124 (0.124)   Loss 0.0652 (0.0652)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 16:54:40]
  Epoch: [160][200/391]   Time 0.172 (0.164)   Data 0.001 (0.001)   Loss 0.0795 (0.1155)   Prec@1 96.875 (95.985)   Prec@5 100.000 (99.965)   [2018-05-03 16:55:13]
  **Train** Prec@1 95.818 Prec@5 99.962 Error@1 4.182
  **Test** Prec@1 89.120 Prec@5 99.510 Error@1 10.880

==>>[2018-05-03 16:55:52] [Epoch=161/500] [Need: 06:41:31] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [161][000/391]   Time 0.378 (0.378)   Data 0.168 (0.168)   Loss 0.0849 (0.0849)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 16:55:53]
  Epoch: [161][200/391]   Time 0.181 (0.162)   Data 0.001 (0.002)   Loss 0.1721 (0.1121)   Prec@1 93.750 (96.210)   Prec@5 100.000 (99.977)   [2018-05-03 16:56:25]
  **Train** Prec@1 95.902 Prec@5 99.960 Error@1 4.098
  **Test** Prec@1 89.090 Prec@5 99.680 Error@1 10.910

==>>[2018-05-03 16:57:03] [Epoch=162/500] [Need: 06:40:20] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [162][000/391]   Time 0.383 (0.383)   Data 0.196 (0.196)   Loss 0.0889 (0.0889)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 16:57:03]
  Epoch: [162][200/391]   Time 0.170 (0.159)   Data 0.001 (0.002)   Loss 0.1402 (0.1198)   Prec@1 92.969 (95.810)   Prec@5 100.000 (99.965)   [2018-05-03 16:57:35]
  **Train** Prec@1 95.702 Prec@5 99.970 Error@1 4.298
  **Test** Prec@1 87.600 Prec@5 99.380 Error@1 12.400

==>>[2018-05-03 16:58:13] [Epoch=163/500] [Need: 06:39:07] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [163][000/391]   Time 0.333 (0.333)   Data 0.124 (0.124)   Loss 0.1096 (0.1096)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 16:58:14]
  Epoch: [163][200/391]   Time 0.149 (0.160)   Data 0.001 (0.001)   Loss 0.1336 (0.1032)   Prec@1 94.531 (96.583)   Prec@5 100.000 (99.984)   [2018-05-03 16:58:45]
  **Train** Prec@1 96.078 Prec@5 99.978 Error@1 3.922
  **Test** Prec@1 89.200 Prec@5 99.430 Error@1 10.800

==>>[2018-05-03 16:59:25] [Epoch=164/500] [Need: 06:37:57] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [164][000/391]   Time 0.294 (0.294)   Data 0.126 (0.126)   Loss 0.0574 (0.0574)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 16:59:25]
  Epoch: [164][200/391]   Time 0.157 (0.160)   Data 0.001 (0.001)   Loss 0.2830 (0.1203)   Prec@1 91.406 (95.864)   Prec@5 100.000 (99.957)   [2018-05-03 16:59:57]
  **Train** Prec@1 95.782 Prec@5 99.958 Error@1 4.218
  **Test** Prec@1 89.030 Prec@5 99.550 Error@1 10.970

==>>[2018-05-03 17:00:35] [Epoch=165/500] [Need: 06:36:45] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [165][000/391]   Time 0.315 (0.315)   Data 0.123 (0.123)   Loss 0.1208 (0.1208)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:00:36]
  Epoch: [165][200/391]   Time 0.137 (0.160)   Data 0.001 (0.001)   Loss 0.0459 (0.1114)   Prec@1 98.438 (96.175)   Prec@5 100.000 (99.988)   [2018-05-03 17:01:08]
  **Train** Prec@1 95.872 Prec@5 99.968 Error@1 4.128
  **Test** Prec@1 86.070 Prec@5 99.460 Error@1 13.930

==>>[2018-05-03 17:01:46] [Epoch=166/500] [Need: 06:35:32] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [166][000/391]   Time 0.321 (0.321)   Data 0.132 (0.132)   Loss 0.1542 (0.1542)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:01:46]
  Epoch: [166][200/391]   Time 0.146 (0.158)   Data 0.001 (0.001)   Loss 0.1066 (0.1104)   Prec@1 96.875 (96.222)   Prec@5 100.000 (99.981)   [2018-05-03 17:02:18]
  **Train** Prec@1 95.922 Prec@5 99.972 Error@1 4.078
  **Test** Prec@1 89.370 Prec@5 99.500 Error@1 10.630

==>>[2018-05-03 17:02:56] [Epoch=167/500] [Need: 06:34:19] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [167][000/391]   Time 0.349 (0.349)   Data 0.168 (0.168)   Loss 0.1112 (0.1112)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:02:56]
  Epoch: [167][200/391]   Time 0.147 (0.161)   Data 0.001 (0.002)   Loss 0.0995 (0.1159)   Prec@1 96.875 (96.047)   Prec@5 100.000 (99.938)   [2018-05-03 17:03:28]
  **Train** Prec@1 95.918 Prec@5 99.952 Error@1 4.082
  **Test** Prec@1 90.020 Prec@5 99.730 Error@1 9.980

==>>[2018-05-03 17:04:07] [Epoch=168/500] [Need: 06:33:08] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [168][000/391]   Time 0.337 (0.337)   Data 0.151 (0.151)   Loss 0.1247 (0.1247)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:04:07]
  Epoch: [168][200/391]   Time 0.192 (0.159)   Data 0.001 (0.001)   Loss 0.1439 (0.1138)   Prec@1 93.750 (96.074)   Prec@5 100.000 (99.969)   [2018-05-03 17:04:39]
  **Train** Prec@1 96.082 Prec@5 99.966 Error@1 3.918
  **Test** Prec@1 89.500 Prec@5 99.620 Error@1 10.500

==>>[2018-05-03 17:05:18] [Epoch=169/500] [Need: 06:31:56] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [169][000/391]   Time 0.320 (0.320)   Data 0.136 (0.136)   Loss 0.1447 (0.1447)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:05:18]
  Epoch: [169][200/391]   Time 0.154 (0.161)   Data 0.001 (0.002)   Loss 0.1662 (0.1117)   Prec@1 93.750 (96.094)   Prec@5 100.000 (99.977)   [2018-05-03 17:05:50]
  **Train** Prec@1 95.848 Prec@5 99.980 Error@1 4.152
  **Test** Prec@1 89.040 Prec@5 99.550 Error@1 10.960

==>>[2018-05-03 17:06:27] [Epoch=170/500] [Need: 06:30:43] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [170][000/391]   Time 0.307 (0.307)   Data 0.141 (0.141)   Loss 0.1644 (0.1644)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 17:06:28]
  Epoch: [170][200/391]   Time 0.166 (0.161)   Data 0.001 (0.001)   Loss 0.1239 (0.1099)   Prec@1 95.312 (96.296)   Prec@5 100.000 (99.977)   [2018-05-03 17:07:00]
  **Train** Prec@1 96.110 Prec@5 99.960 Error@1 3.890
  **Test** Prec@1 89.900 Prec@5 99.580 Error@1 10.100

==>>[2018-05-03 17:07:39] [Epoch=171/500] [Need: 06:29:33] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [171][000/391]   Time 0.319 (0.319)   Data 0.135 (0.135)   Loss 0.0882 (0.0882)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 17:07:39]
  Epoch: [171][200/391]   Time 0.163 (0.162)   Data 0.001 (0.002)   Loss 0.1695 (0.1122)   Prec@1 94.531 (96.308)   Prec@5 100.000 (99.973)   [2018-05-03 17:08:12]
  **Train** Prec@1 96.000 Prec@5 99.968 Error@1 4.000
  **Test** Prec@1 89.570 Prec@5 99.520 Error@1 10.430

==>>[2018-05-03 17:08:50] [Epoch=172/500] [Need: 06:28:23] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [172][000/391]   Time 0.337 (0.337)   Data 0.132 (0.132)   Loss 0.0982 (0.0982)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:08:51]
  Epoch: [172][200/391]   Time 0.163 (0.162)   Data 0.001 (0.001)   Loss 0.0706 (0.1066)   Prec@1 98.438 (96.459)   Prec@5 100.000 (99.977)   [2018-05-03 17:09:23]
  **Train** Prec@1 96.066 Prec@5 99.974 Error@1 3.934
  **Test** Prec@1 88.700 Prec@5 99.610 Error@1 11.300

==>>[2018-05-03 17:10:02] [Epoch=173/500] [Need: 06:27:12] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [173][000/391]   Time 0.345 (0.345)   Data 0.148 (0.148)   Loss 0.0959 (0.0959)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:10:02]
  Epoch: [173][200/391]   Time 0.144 (0.158)   Data 0.000 (0.001)   Loss 0.1059 (0.1133)   Prec@1 97.656 (96.164)   Prec@5 100.000 (99.961)   [2018-05-03 17:10:33]
  **Train** Prec@1 95.982 Prec@5 99.952 Error@1 4.018
  **Test** Prec@1 89.220 Prec@5 99.490 Error@1 10.780

==>>[2018-05-03 17:11:11] [Epoch=174/500] [Need: 06:25:58] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [174][000/391]   Time 0.362 (0.362)   Data 0.185 (0.185)   Loss 0.1156 (0.1156)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:11:12]
  Epoch: [174][200/391]   Time 0.162 (0.158)   Data 0.001 (0.002)   Loss 0.1272 (0.1145)   Prec@1 92.969 (96.043)   Prec@5 100.000 (99.969)   [2018-05-03 17:11:43]
  **Train** Prec@1 95.886 Prec@5 99.970 Error@1 4.114
  **Test** Prec@1 89.600 Prec@5 99.610 Error@1 10.400

==>>[2018-05-03 17:12:21] [Epoch=175/500] [Need: 06:24:45] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [175][000/391]   Time 0.377 (0.377)   Data 0.176 (0.176)   Loss 0.1323 (0.1323)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:12:21]
  Epoch: [175][200/391]   Time 0.176 (0.160)   Data 0.001 (0.002)   Loss 0.1058 (0.1125)   Prec@1 96.094 (96.137)   Prec@5 100.000 (99.965)   [2018-05-03 17:12:53]
  **Train** Prec@1 95.798 Prec@5 99.964 Error@1 4.202
  **Test** Prec@1 90.190 Prec@5 99.700 Error@1 9.810

==>>[2018-05-03 17:13:31] [Epoch=176/500] [Need: 06:23:32] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [176][000/391]   Time 0.346 (0.346)   Data 0.156 (0.156)   Loss 0.0585 (0.0585)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 17:13:32]
  Epoch: [176][200/391]   Time 0.178 (0.162)   Data 0.001 (0.002)   Loss 0.0850 (0.1155)   Prec@1 95.312 (96.098)   Prec@5 100.000 (99.965)   [2018-05-03 17:14:04]
  **Train** Prec@1 96.014 Prec@5 99.966 Error@1 3.986
  **Test** Prec@1 88.080 Prec@5 99.530 Error@1 11.920

==>>[2018-05-03 17:14:42] [Epoch=177/500] [Need: 06:22:21] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [177][000/391]   Time 0.309 (0.309)   Data 0.134 (0.134)   Loss 0.1417 (0.1417)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:14:42]
  Epoch: [177][200/391]   Time 0.152 (0.162)   Data 0.001 (0.001)   Loss 0.0860 (0.1194)   Prec@1 97.656 (95.841)   Prec@5 100.000 (99.973)   [2018-05-03 17:15:15]
  **Train** Prec@1 95.772 Prec@5 99.966 Error@1 4.228
  **Test** Prec@1 89.050 Prec@5 99.560 Error@1 10.950

==>>[2018-05-03 17:15:54] [Epoch=178/500] [Need: 06:21:11] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [178][000/391]   Time 0.302 (0.302)   Data 0.125 (0.125)   Loss 0.0678 (0.0678)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:15:54]
  Epoch: [178][200/391]   Time 0.162 (0.158)   Data 0.001 (0.001)   Loss 0.1179 (0.1082)   Prec@1 96.094 (96.273)   Prec@5 100.000 (99.977)   [2018-05-03 17:16:26]
  **Train** Prec@1 96.024 Prec@5 99.978 Error@1 3.976
  **Test** Prec@1 87.960 Prec@5 99.450 Error@1 12.040

==>>[2018-05-03 17:17:04] [Epoch=179/500] [Need: 06:19:59] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [179][000/391]   Time 0.328 (0.328)   Data 0.128 (0.128)   Loss 0.1048 (0.1048)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:17:05]
  Epoch: [179][200/391]   Time 0.173 (0.161)   Data 0.001 (0.001)   Loss 0.1319 (0.1115)   Prec@1 96.094 (96.086)   Prec@5 100.000 (99.953)   [2018-05-03 17:17:37]
  **Train** Prec@1 95.942 Prec@5 99.960 Error@1 4.058
  **Test** Prec@1 89.670 Prec@5 99.470 Error@1 10.330

==>>[2018-05-03 17:18:16] [Epoch=180/500] [Need: 06:18:49] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [180][000/391]   Time 0.313 (0.313)   Data 0.135 (0.135)   Loss 0.1435 (0.1435)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:18:16]
  Epoch: [180][200/391]   Time 0.161 (0.158)   Data 0.001 (0.001)   Loss 0.0735 (0.1109)   Prec@1 97.656 (96.121)   Prec@5 100.000 (99.961)   [2018-05-03 17:18:48]
  **Train** Prec@1 95.972 Prec@5 99.964 Error@1 4.028
  **Test** Prec@1 88.280 Prec@5 99.420 Error@1 11.720

==>>[2018-05-03 17:19:26] [Epoch=181/500] [Need: 06:17:37] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [181][000/391]   Time 0.353 (0.353)   Data 0.162 (0.162)   Loss 0.1056 (0.1056)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:19:27]
  Epoch: [181][200/391]   Time 0.157 (0.163)   Data 0.001 (0.002)   Loss 0.1174 (0.1116)   Prec@1 96.094 (96.102)   Prec@5 100.000 (99.981)   [2018-05-03 17:19:59]
  **Train** Prec@1 96.026 Prec@5 99.962 Error@1 3.974
  **Test** Prec@1 89.500 Prec@5 99.520 Error@1 10.500

==>>[2018-05-03 17:20:37] [Epoch=182/500] [Need: 06:16:26] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [182][000/391]   Time 0.328 (0.328)   Data 0.126 (0.126)   Loss 0.1198 (0.1198)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:20:38]
  Epoch: [182][200/391]   Time 0.142 (0.161)   Data 0.001 (0.001)   Loss 0.0421 (0.1170)   Prec@1 99.219 (95.989)   Prec@5 100.000 (99.965)   [2018-05-03 17:21:10]
  **Train** Prec@1 95.818 Prec@5 99.962 Error@1 4.182
  **Test** Prec@1 89.770 Prec@5 99.600 Error@1 10.230

==>>[2018-05-03 17:21:48] [Epoch=183/500] [Need: 06:15:14] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [183][000/391]   Time 0.374 (0.374)   Data 0.155 (0.155)   Loss 0.0947 (0.0947)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:21:48]
  Epoch: [183][200/391]   Time 0.153 (0.158)   Data 0.001 (0.001)   Loss 0.1125 (0.1096)   Prec@1 95.312 (96.311)   Prec@5 100.000 (99.957)   [2018-05-03 17:22:19]
  **Train** Prec@1 96.162 Prec@5 99.966 Error@1 3.838
  **Test** Prec@1 88.580 Prec@5 99.570 Error@1 11.420

==>>[2018-05-03 17:22:58] [Epoch=184/500] [Need: 06:14:01] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [184][000/391]   Time 0.292 (0.292)   Data 0.128 (0.128)   Loss 0.0625 (0.0625)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:22:58]
  Epoch: [184][200/391]   Time 0.140 (0.162)   Data 0.001 (0.001)   Loss 0.0748 (0.1117)   Prec@1 97.656 (96.273)   Prec@5 100.000 (99.949)   [2018-05-03 17:23:30]
  **Train** Prec@1 95.962 Prec@5 99.960 Error@1 4.038
  **Test** Prec@1 89.300 Prec@5 99.630 Error@1 10.700

==>>[2018-05-03 17:24:10] [Epoch=185/500] [Need: 06:12:52] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [185][000/391]   Time 0.380 (0.380)   Data 0.179 (0.179)   Loss 0.1333 (0.1333)   Prec@1 95.312 (95.312)   Prec@5 99.219 (99.219)   [2018-05-03 17:24:10]
  Epoch: [185][200/391]   Time 0.156 (0.162)   Data 0.001 (0.002)   Loss 0.0711 (0.1133)   Prec@1 96.875 (96.171)   Prec@5 100.000 (99.977)   [2018-05-03 17:24:42]
  **Train** Prec@1 96.038 Prec@5 99.974 Error@1 3.962
  **Test** Prec@1 90.610 Prec@5 99.710 Error@1 9.390

==>>[2018-05-03 17:25:21] [Epoch=186/500] [Need: 06:11:40] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [186][000/391]   Time 0.364 (0.364)   Data 0.166 (0.166)   Loss 0.1294 (0.1294)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:25:21]
  Epoch: [186][200/391]   Time 0.150 (0.161)   Data 0.001 (0.002)   Loss 0.1446 (0.1094)   Prec@1 94.531 (96.284)   Prec@5 100.000 (99.973)   [2018-05-03 17:25:53]
  **Train** Prec@1 95.986 Prec@5 99.972 Error@1 4.014
  **Test** Prec@1 85.570 Prec@5 99.180 Error@1 14.430

==>>[2018-05-03 17:26:31] [Epoch=187/500] [Need: 06:10:28] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [187][000/391]   Time 0.303 (0.303)   Data 0.155 (0.155)   Loss 0.0749 (0.0749)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 17:26:31]
  Epoch: [187][200/391]   Time 0.181 (0.158)   Data 0.001 (0.002)   Loss 0.2007 (0.1122)   Prec@1 93.750 (96.230)   Prec@5 100.000 (99.973)   [2018-05-03 17:27:03]
  **Train** Prec@1 95.978 Prec@5 99.954 Error@1 4.022
  **Test** Prec@1 87.650 Prec@5 99.520 Error@1 12.350

==>>[2018-05-03 17:27:41] [Epoch=188/500] [Need: 06:09:15] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [188][000/391]   Time 0.359 (0.359)   Data 0.162 (0.162)   Loss 0.1777 (0.1777)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:27:41]
  Epoch: [188][200/391]   Time 0.153 (0.159)   Data 0.001 (0.001)   Loss 0.1633 (0.1126)   Prec@1 93.750 (96.148)   Prec@5 100.000 (99.973)   [2018-05-03 17:28:13]
  **Train** Prec@1 96.170 Prec@5 99.972 Error@1 3.830
  **Test** Prec@1 89.230 Prec@5 99.480 Error@1 10.770

==>>[2018-05-03 17:28:51] [Epoch=189/500] [Need: 06:08:04] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [189][000/391]   Time 0.407 (0.407)   Data 0.190 (0.190)   Loss 0.1392 (0.1392)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:28:52]
  Epoch: [189][200/391]   Time 0.147 (0.165)   Data 0.001 (0.002)   Loss 0.0610 (0.1103)   Prec@1 97.656 (96.257)   Prec@5 100.000 (99.969)   [2018-05-03 17:29:25]
  **Train** Prec@1 96.016 Prec@5 99.958 Error@1 3.984
  **Test** Prec@1 89.290 Prec@5 99.650 Error@1 10.710

==>>[2018-05-03 17:30:03] [Epoch=190/500] [Need: 06:06:53] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [190][000/391]   Time 0.326 (0.326)   Data 0.141 (0.141)   Loss 0.1964 (0.1964)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:30:03]
  Epoch: [190][200/391]   Time 0.156 (0.159)   Data 0.000 (0.001)   Loss 0.0824 (0.1119)   Prec@1 96.094 (96.230)   Prec@5 100.000 (99.984)   [2018-05-03 17:30:35]
  **Train** Prec@1 96.086 Prec@5 99.976 Error@1 3.914
  **Test** Prec@1 89.700 Prec@5 99.620 Error@1 10.300

==>>[2018-05-03 17:31:13] [Epoch=191/500] [Need: 06:05:41] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [191][000/391]   Time 0.342 (0.342)   Data 0.152 (0.152)   Loss 0.0651 (0.0651)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:31:13]
  Epoch: [191][200/391]   Time 0.150 (0.164)   Data 0.001 (0.002)   Loss 0.1266 (0.1118)   Prec@1 95.312 (96.292)   Prec@5 100.000 (99.984)   [2018-05-03 17:31:46]
  **Train** Prec@1 96.002 Prec@5 99.970 Error@1 3.998
  **Test** Prec@1 89.940 Prec@5 99.580 Error@1 10.060

==>>[2018-05-03 17:32:25] [Epoch=192/500] [Need: 06:04:32] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [192][000/391]   Time 0.337 (0.337)   Data 0.153 (0.153)   Loss 0.0980 (0.0980)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:32:26]
  Epoch: [192][200/391]   Time 0.162 (0.160)   Data 0.001 (0.002)   Loss 0.0575 (0.1165)   Prec@1 98.438 (96.028)   Prec@5 100.000 (99.977)   [2018-05-03 17:32:58]
  **Train** Prec@1 95.840 Prec@5 99.968 Error@1 4.160
  **Test** Prec@1 88.520 Prec@5 99.410 Error@1 11.480

==>>[2018-05-03 17:33:36] [Epoch=193/500] [Need: 06:03:21] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [193][000/391]   Time 0.337 (0.337)   Data 0.145 (0.145)   Loss 0.0861 (0.0861)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 17:33:36]
  Epoch: [193][200/391]   Time 0.172 (0.158)   Data 0.001 (0.001)   Loss 0.1636 (0.1100)   Prec@1 96.094 (96.397)   Prec@5 100.000 (99.957)   [2018-05-03 17:34:08]
  **Train** Prec@1 96.266 Prec@5 99.964 Error@1 3.734
  **Test** Prec@1 90.490 Prec@5 99.670 Error@1 9.510

==>>[2018-05-03 17:34:47] [Epoch=194/500] [Need: 06:02:10] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [194][000/391]   Time 0.323 (0.323)   Data 0.124 (0.124)   Loss 0.0643 (0.0643)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 17:34:47]
  Epoch: [194][200/391]   Time 0.144 (0.162)   Data 0.001 (0.002)   Loss 0.1523 (0.1171)   Prec@1 96.094 (95.876)   Prec@5 100.000 (99.961)   [2018-05-03 17:35:20]
  **Train** Prec@1 95.890 Prec@5 99.958 Error@1 4.110
  **Test** Prec@1 90.250 Prec@5 99.620 Error@1 9.750

==>>[2018-05-03 17:35:59] [Epoch=195/500] [Need: 06:01:00] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [195][000/391]   Time 0.371 (0.371)   Data 0.165 (0.165)   Loss 0.0508 (0.0508)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:36:00]
  Epoch: [195][200/391]   Time 0.163 (0.163)   Data 0.001 (0.002)   Loss 0.1652 (0.1082)   Prec@1 96.094 (96.222)   Prec@5 100.000 (99.984)   [2018-05-03 17:36:32]
  **Train** Prec@1 96.040 Prec@5 99.972 Error@1 3.960
  **Test** Prec@1 88.950 Prec@5 99.460 Error@1 11.050

==>>[2018-05-03 17:37:10] [Epoch=196/500] [Need: 05:59:49] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [196][000/391]   Time 0.339 (0.339)   Data 0.154 (0.154)   Loss 0.1614 (0.1614)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:37:11]
  Epoch: [196][200/391]   Time 0.132 (0.162)   Data 0.001 (0.002)   Loss 0.1146 (0.1129)   Prec@1 95.312 (96.035)   Prec@5 100.000 (99.957)   [2018-05-03 17:37:43]
  **Train** Prec@1 95.828 Prec@5 99.956 Error@1 4.172
  **Test** Prec@1 88.990 Prec@5 99.540 Error@1 11.010

==>>[2018-05-03 17:38:22] [Epoch=197/500] [Need: 05:58:39] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [197][000/391]   Time 0.293 (0.293)   Data 0.126 (0.126)   Loss 0.2051 (0.2051)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:38:22]
  Epoch: [197][200/391]   Time 0.153 (0.156)   Data 0.001 (0.001)   Loss 0.1166 (0.1122)   Prec@1 96.094 (96.214)   Prec@5 100.000 (99.981)   [2018-05-03 17:38:53]
  **Train** Prec@1 96.116 Prec@5 99.976 Error@1 3.884
  **Test** Prec@1 89.750 Prec@5 99.540 Error@1 10.250

==>>[2018-05-03 17:39:31] [Epoch=198/500] [Need: 05:57:26] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [198][000/391]   Time 0.289 (0.289)   Data 0.133 (0.133)   Loss 0.1051 (0.1051)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:39:32]
  Epoch: [198][200/391]   Time 0.175 (0.161)   Data 0.001 (0.001)   Loss 0.0967 (0.1037)   Prec@1 96.875 (96.471)   Prec@5 100.000 (99.981)   [2018-05-03 17:40:04]
  **Train** Prec@1 96.050 Prec@5 99.972 Error@1 3.950
  **Test** Prec@1 89.590 Prec@5 99.660 Error@1 10.410

==>>[2018-05-03 17:40:43] [Epoch=199/500] [Need: 05:56:16] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [199][000/391]   Time 0.312 (0.312)   Data 0.124 (0.124)   Loss 0.1462 (0.1462)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:40:43]
  Epoch: [199][200/391]   Time 0.171 (0.158)   Data 0.001 (0.001)   Loss 0.1361 (0.1045)   Prec@1 93.750 (96.459)   Prec@5 100.000 (99.981)   [2018-05-03 17:41:15]
  **Train** Prec@1 96.172 Prec@5 99.968 Error@1 3.828
  **Test** Prec@1 90.250 Prec@5 99.710 Error@1 9.750

==>>[2018-05-03 17:41:53] [Epoch=200/500] [Need: 05:55:03] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [200][000/391]   Time 0.326 (0.326)   Data 0.132 (0.132)   Loss 0.0452 (0.0452)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 17:41:53]
  Epoch: [200][200/391]   Time 0.153 (0.160)   Data 0.001 (0.001)   Loss 0.1538 (0.1037)   Prec@1 95.312 (96.397)   Prec@5 100.000 (99.969)   [2018-05-03 17:42:25]
  **Train** Prec@1 96.046 Prec@5 99.968 Error@1 3.954
  **Test** Prec@1 89.910 Prec@5 99.640 Error@1 10.090

==>>[2018-05-03 17:43:04] [Epoch=201/500] [Need: 05:53:53] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [201][000/391]   Time 0.361 (0.361)   Data 0.159 (0.159)   Loss 0.1065 (0.1065)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:43:05]
  Epoch: [201][200/391]   Time 0.171 (0.164)   Data 0.001 (0.002)   Loss 0.0854 (0.1119)   Prec@1 96.094 (96.203)   Prec@5 100.000 (99.969)   [2018-05-03 17:43:37]
  **Train** Prec@1 96.046 Prec@5 99.968 Error@1 3.954
  **Test** Prec@1 88.210 Prec@5 99.620 Error@1 11.790

==>>[2018-05-03 17:44:15] [Epoch=202/500] [Need: 05:52:42] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [202][000/391]   Time 0.345 (0.345)   Data 0.143 (0.143)   Loss 0.1563 (0.1563)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:44:16]
  Epoch: [202][200/391]   Time 0.155 (0.160)   Data 0.001 (0.002)   Loss 0.1593 (0.1132)   Prec@1 93.750 (96.253)   Prec@5 100.000 (99.981)   [2018-05-03 17:44:48]
  **Train** Prec@1 96.016 Prec@5 99.974 Error@1 3.984
  **Test** Prec@1 88.890 Prec@5 99.610 Error@1 11.110

==>>[2018-05-03 17:45:27] [Epoch=203/500] [Need: 05:51:31] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [203][000/391]   Time 0.305 (0.305)   Data 0.125 (0.125)   Loss 0.1035 (0.1035)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:45:27]
  Epoch: [203][200/391]   Time 0.164 (0.161)   Data 0.001 (0.001)   Loss 0.1173 (0.1093)   Prec@1 96.094 (96.273)   Prec@5 100.000 (99.965)   [2018-05-03 17:45:59]
  **Train** Prec@1 96.146 Prec@5 99.976 Error@1 3.854
  **Test** Prec@1 88.300 Prec@5 99.590 Error@1 11.700

==>>[2018-05-03 17:46:38] [Epoch=204/500] [Need: 05:50:20] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [204][000/391]   Time 0.321 (0.321)   Data 0.128 (0.128)   Loss 0.1525 (0.1525)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:46:38]
  Epoch: [204][200/391]   Time 0.176 (0.159)   Data 0.001 (0.001)   Loss 0.1415 (0.1116)   Prec@1 95.312 (96.063)   Prec@5 100.000 (99.977)   [2018-05-03 17:47:10]
  **Train** Prec@1 95.838 Prec@5 99.976 Error@1 4.162
  **Test** Prec@1 89.920 Prec@5 99.660 Error@1 10.080

==>>[2018-05-03 17:47:48] [Epoch=205/500] [Need: 05:49:08] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [205][000/391]   Time 0.326 (0.326)   Data 0.125 (0.125)   Loss 0.0977 (0.0977)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 17:47:48]
  Epoch: [205][200/391]   Time 0.131 (0.159)   Data 0.001 (0.001)   Loss 0.0458 (0.1095)   Prec@1 99.219 (96.362)   Prec@5 100.000 (99.965)   [2018-05-03 17:48:20]
  **Train** Prec@1 96.068 Prec@5 99.960 Error@1 3.932
  **Test** Prec@1 89.150 Prec@5 99.600 Error@1 10.850

==>>[2018-05-03 17:48:58] [Epoch=206/500] [Need: 05:47:56] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [206][000/391]   Time 0.359 (0.359)   Data 0.181 (0.181)   Loss 0.1229 (0.1229)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:48:58]
  Epoch: [206][200/391]   Time 0.157 (0.161)   Data 0.001 (0.002)   Loss 0.0751 (0.1133)   Prec@1 98.438 (96.203)   Prec@5 100.000 (99.961)   [2018-05-03 17:49:30]
  **Train** Prec@1 96.132 Prec@5 99.962 Error@1 3.868
  **Test** Prec@1 90.180 Prec@5 99.640 Error@1 9.820

==>>[2018-05-03 17:50:10] [Epoch=207/500] [Need: 05:46:46] [learning_rate=0.1000] [Best : Accuracy=90.70, Error=9.30]
  Epoch: [207][000/391]   Time 0.386 (0.386)   Data 0.189 (0.189)   Loss 0.0487 (0.0487)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:50:10]
  Epoch: [207][200/391]   Time 0.176 (0.159)   Data 0.001 (0.002)   Loss 0.1307 (0.1096)   Prec@1 94.531 (96.206)   Prec@5 100.000 (99.969)   [2018-05-03 17:50:42]
  **Train** Prec@1 95.998 Prec@5 99.962 Error@1 4.002
  **Test** Prec@1 91.110 Prec@5 99.710 Error@1 8.890

==>>[2018-05-03 17:51:20] [Epoch=208/500] [Need: 05:45:34] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [208][000/391]   Time 0.339 (0.339)   Data 0.160 (0.160)   Loss 0.0492 (0.0492)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 17:51:21]
  Epoch: [208][200/391]   Time 0.151 (0.161)   Data 0.000 (0.001)   Loss 0.0718 (0.1093)   Prec@1 98.438 (96.137)   Prec@5 100.000 (99.977)   [2018-05-03 17:51:53]
  **Train** Prec@1 95.938 Prec@5 99.974 Error@1 4.062
  **Test** Prec@1 89.950 Prec@5 99.630 Error@1 10.050

==>>[2018-05-03 17:52:31] [Epoch=209/500] [Need: 05:44:22] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [209][000/391]   Time 0.332 (0.332)   Data 0.141 (0.141)   Loss 0.1034 (0.1034)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 17:52:31]
  Epoch: [209][200/391]   Time 0.145 (0.157)   Data 0.001 (0.001)   Loss 0.1446 (0.1107)   Prec@1 93.750 (96.222)   Prec@5 100.000 (99.961)   [2018-05-03 17:53:02]
  **Train** Prec@1 96.018 Prec@5 99.966 Error@1 3.982
  **Test** Prec@1 89.580 Prec@5 99.560 Error@1 10.420

==>>[2018-05-03 17:53:41] [Epoch=210/500] [Need: 05:43:10] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [210][000/391]   Time 0.332 (0.332)   Data 0.153 (0.153)   Loss 0.1183 (0.1183)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:53:41]
  Epoch: [210][200/391]   Time 0.154 (0.162)   Data 0.001 (0.002)   Loss 0.1468 (0.1106)   Prec@1 92.969 (96.090)   Prec@5 100.000 (99.961)   [2018-05-03 17:54:13]
  **Train** Prec@1 96.116 Prec@5 99.960 Error@1 3.884
  **Test** Prec@1 90.090 Prec@5 99.530 Error@1 9.910

==>>[2018-05-03 17:54:51] [Epoch=211/500] [Need: 05:41:58] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [211][000/391]   Time 0.361 (0.361)   Data 0.172 (0.172)   Loss 0.1143 (0.1143)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 17:54:52]
  Epoch: [211][200/391]   Time 0.139 (0.160)   Data 0.001 (0.002)   Loss 0.1418 (0.1103)   Prec@1 94.531 (96.203)   Prec@5 100.000 (99.961)   [2018-05-03 17:55:23]
  **Train** Prec@1 96.132 Prec@5 99.968 Error@1 3.868
  **Test** Prec@1 88.980 Prec@5 99.690 Error@1 11.020

==>>[2018-05-03 17:56:03] [Epoch=212/500] [Need: 05:40:48] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [212][000/391]   Time 0.293 (0.293)   Data 0.129 (0.129)   Loss 0.0920 (0.0920)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 17:56:03]
  Epoch: [212][200/391]   Time 0.185 (0.164)   Data 0.001 (0.002)   Loss 0.0857 (0.1115)   Prec@1 96.875 (96.238)   Prec@5 100.000 (99.973)   [2018-05-03 17:56:36]
  **Train** Prec@1 96.016 Prec@5 99.962 Error@1 3.984
  **Test** Prec@1 90.160 Prec@5 99.410 Error@1 9.840

==>>[2018-05-03 17:57:15] [Epoch=213/500] [Need: 05:39:39] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [213][000/391]   Time 0.371 (0.371)   Data 0.152 (0.152)   Loss 0.0468 (0.0468)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 17:57:16]
  Epoch: [213][200/391]   Time 0.132 (0.157)   Data 0.000 (0.001)   Loss 0.1260 (0.1112)   Prec@1 95.312 (96.226)   Prec@5 100.000 (99.981)   [2018-05-03 17:57:47]
  **Train** Prec@1 96.034 Prec@5 99.974 Error@1 3.966
  **Test** Prec@1 88.250 Prec@5 99.430 Error@1 11.750

==>>[2018-05-03 17:58:25] [Epoch=214/500] [Need: 05:38:27] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [214][000/391]   Time 0.352 (0.352)   Data 0.159 (0.159)   Loss 0.2242 (0.2242)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 17:58:26]
  Epoch: [214][200/391]   Time 0.166 (0.161)   Data 0.001 (0.002)   Loss 0.0543 (0.1046)   Prec@1 98.438 (96.393)   Prec@5 100.000 (99.981)   [2018-05-03 17:58:58]
  **Train** Prec@1 96.014 Prec@5 99.964 Error@1 3.986
  **Test** Prec@1 86.580 Prec@5 99.180 Error@1 13.420

==>>[2018-05-03 17:59:37] [Epoch=215/500] [Need: 05:37:16] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [215][000/391]   Time 0.382 (0.382)   Data 0.172 (0.172)   Loss 0.0607 (0.0607)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 17:59:37]
  Epoch: [215][200/391]   Time 0.137 (0.159)   Data 0.000 (0.001)   Loss 0.1223 (0.1072)   Prec@1 96.094 (96.346)   Prec@5 100.000 (99.961)   [2018-05-03 18:00:09]
  **Train** Prec@1 96.154 Prec@5 99.966 Error@1 3.846
  **Test** Prec@1 88.880 Prec@5 99.530 Error@1 11.120

==>>[2018-05-03 18:00:47] [Epoch=216/500] [Need: 05:36:04] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [216][000/391]   Time 0.302 (0.302)   Data 0.130 (0.130)   Loss 0.1729 (0.1729)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-05-03 18:00:47]
  Epoch: [216][200/391]   Time 0.191 (0.155)   Data 0.001 (0.001)   Loss 0.1179 (0.1047)   Prec@1 96.875 (96.436)   Prec@5 100.000 (99.961)   [2018-05-03 18:01:18]
  **Train** Prec@1 96.162 Prec@5 99.956 Error@1 3.838
  **Test** Prec@1 90.030 Prec@5 99.600 Error@1 9.970

==>>[2018-05-03 18:01:57] [Epoch=217/500] [Need: 05:34:52] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [217][000/391]   Time 0.329 (0.329)   Data 0.137 (0.137)   Loss 0.1005 (0.1005)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 18:01:57]
  Epoch: [217][200/391]   Time 0.165 (0.161)   Data 0.001 (0.002)   Loss 0.1198 (0.1080)   Prec@1 96.094 (96.467)   Prec@5 100.000 (99.953)   [2018-05-03 18:02:29]
  **Train** Prec@1 96.194 Prec@5 99.958 Error@1 3.806
  **Test** Prec@1 88.990 Prec@5 99.530 Error@1 11.010

==>>[2018-05-03 18:03:08] [Epoch=218/500] [Need: 05:33:40] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [218][000/391]   Time 0.367 (0.367)   Data 0.175 (0.175)   Loss 0.0605 (0.0605)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 18:03:08]
  Epoch: [218][200/391]   Time 0.159 (0.161)   Data 0.001 (0.002)   Loss 0.0641 (0.1068)   Prec@1 97.656 (96.486)   Prec@5 100.000 (99.984)   [2018-05-03 18:03:40]
  **Train** Prec@1 96.154 Prec@5 99.972 Error@1 3.846
  **Test** Prec@1 90.210 Prec@5 99.650 Error@1 9.790

==>>[2018-05-03 18:04:18] [Epoch=219/500] [Need: 05:32:29] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [219][000/391]   Time 0.345 (0.345)   Data 0.148 (0.148)   Loss 0.0896 (0.0896)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:04:19]
  Epoch: [219][200/391]   Time 0.174 (0.159)   Data 0.001 (0.002)   Loss 0.2532 (0.1108)   Prec@1 92.188 (96.140)   Prec@5 100.000 (99.961)   [2018-05-03 18:04:50]
  **Train** Prec@1 95.922 Prec@5 99.964 Error@1 4.078
  **Test** Prec@1 89.690 Prec@5 99.520 Error@1 10.310

==>>[2018-05-03 18:05:29] [Epoch=220/500] [Need: 05:31:18] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [220][000/391]   Time 0.335 (0.335)   Data 0.147 (0.147)   Loss 0.0822 (0.0822)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:05:30]
  Epoch: [220][200/391]   Time 0.176 (0.159)   Data 0.001 (0.001)   Loss 0.0907 (0.1113)   Prec@1 94.531 (96.191)   Prec@5 100.000 (99.957)   [2018-05-03 18:06:01]
  **Train** Prec@1 95.972 Prec@5 99.966 Error@1 4.028
  **Test** Prec@1 89.250 Prec@5 99.630 Error@1 10.750

==>>[2018-05-03 18:06:39] [Epoch=221/500] [Need: 05:30:06] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [221][000/391]   Time 0.285 (0.285)   Data 0.123 (0.123)   Loss 0.1430 (0.1430)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 18:06:40]
  Epoch: [221][200/391]   Time 0.183 (0.158)   Data 0.001 (0.001)   Loss 0.2361 (0.1133)   Prec@1 93.750 (96.105)   Prec@5 100.000 (99.969)   [2018-05-03 18:07:11]
  **Train** Prec@1 96.136 Prec@5 99.972 Error@1 3.864
  **Test** Prec@1 89.970 Prec@5 99.580 Error@1 10.030

==>>[2018-05-03 18:07:49] [Epoch=222/500] [Need: 05:28:53] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [222][000/391]   Time 0.374 (0.374)   Data 0.187 (0.187)   Loss 0.0451 (0.0451)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:07:49]
  Epoch: [222][200/391]   Time 0.178 (0.161)   Data 0.001 (0.002)   Loss 0.1426 (0.1032)   Prec@1 93.750 (96.455)   Prec@5 100.000 (99.973)   [2018-05-03 18:08:21]
  **Train** Prec@1 96.244 Prec@5 99.966 Error@1 3.756
  **Test** Prec@1 89.330 Prec@5 99.610 Error@1 10.670

==>>[2018-05-03 18:09:00] [Epoch=223/500] [Need: 05:27:42] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [223][000/391]   Time 0.323 (0.323)   Data 0.128 (0.128)   Loss 0.1101 (0.1101)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:09:00]
  Epoch: [223][200/391]   Time 0.167 (0.161)   Data 0.001 (0.001)   Loss 0.0588 (0.1056)   Prec@1 96.875 (96.346)   Prec@5 100.000 (99.965)   [2018-05-03 18:09:32]
  **Train** Prec@1 96.134 Prec@5 99.970 Error@1 3.866
  **Test** Prec@1 88.600 Prec@5 99.470 Error@1 11.400

==>>[2018-05-03 18:10:12] [Epoch=224/500] [Need: 05:26:32] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [224][000/391]   Time 0.321 (0.321)   Data 0.137 (0.137)   Loss 0.0660 (0.0660)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:10:12]
  Epoch: [224][200/391]   Time 0.148 (0.160)   Data 0.001 (0.002)   Loss 0.0701 (0.1072)   Prec@1 97.656 (96.335)   Prec@5 100.000 (99.981)   [2018-05-03 18:10:44]
  **Train** Prec@1 96.022 Prec@5 99.958 Error@1 3.978
  **Test** Prec@1 87.360 Prec@5 99.680 Error@1 12.640

==>>[2018-05-03 18:11:22] [Epoch=225/500] [Need: 05:25:20] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [225][000/391]   Time 0.335 (0.335)   Data 0.148 (0.148)   Loss 0.1293 (0.1293)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 18:11:22]
  Epoch: [225][200/391]   Time 0.164 (0.161)   Data 0.001 (0.001)   Loss 0.2188 (0.1071)   Prec@1 91.406 (96.315)   Prec@5 100.000 (99.965)   [2018-05-03 18:11:54]
  **Train** Prec@1 96.082 Prec@5 99.966 Error@1 3.918
  **Test** Prec@1 90.450 Prec@5 99.640 Error@1 9.550

==>>[2018-05-03 18:12:34] [Epoch=226/500] [Need: 05:24:10] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [226][000/391]   Time 0.337 (0.337)   Data 0.143 (0.143)   Loss 0.0764 (0.0764)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:12:34]
  Epoch: [226][200/391]   Time 0.173 (0.162)   Data 0.001 (0.002)   Loss 0.1125 (0.0997)   Prec@1 96.094 (96.595)   Prec@5 100.000 (99.973)   [2018-05-03 18:13:06]
  **Train** Prec@1 96.226 Prec@5 99.976 Error@1 3.774
  **Test** Prec@1 90.080 Prec@5 99.610 Error@1 9.920

==>>[2018-05-03 18:13:44] [Epoch=227/500] [Need: 05:22:58] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [227][000/391]   Time 0.368 (0.368)   Data 0.154 (0.154)   Loss 0.1086 (0.1086)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 18:13:44]
  Epoch: [227][200/391]   Time 0.149 (0.158)   Data 0.000 (0.002)   Loss 0.0980 (0.1074)   Prec@1 96.875 (96.389)   Prec@5 100.000 (99.981)   [2018-05-03 18:14:16]
  **Train** Prec@1 96.090 Prec@5 99.968 Error@1 3.910
  **Test** Prec@1 88.400 Prec@5 99.540 Error@1 11.600

==>>[2018-05-03 18:14:54] [Epoch=228/500] [Need: 05:21:46] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [228][000/391]   Time 0.321 (0.321)   Data 0.125 (0.125)   Loss 0.1836 (0.1836)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-05-03 18:14:54]
  Epoch: [228][200/391]   Time 0.182 (0.160)   Data 0.001 (0.001)   Loss 0.1515 (0.1093)   Prec@1 92.969 (96.133)   Prec@5 100.000 (99.973)   [2018-05-03 18:15:26]
  **Train** Prec@1 95.866 Prec@5 99.954 Error@1 4.134
  **Test** Prec@1 86.650 Prec@5 99.630 Error@1 13.350

==>>[2018-05-03 18:16:04] [Epoch=229/500] [Need: 05:20:33] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [229][000/391]   Time 0.337 (0.337)   Data 0.161 (0.161)   Loss 0.1933 (0.1933)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-05-03 18:16:04]
  Epoch: [229][200/391]   Time 0.144 (0.158)   Data 0.001 (0.002)   Loss 0.1255 (0.1111)   Prec@1 96.094 (96.121)   Prec@5 100.000 (99.981)   [2018-05-03 18:16:35]
  **Train** Prec@1 96.002 Prec@5 99.972 Error@1 3.998
  **Test** Prec@1 90.830 Prec@5 99.700 Error@1 9.170

==>>[2018-05-03 18:17:13] [Epoch=230/500] [Need: 05:19:21] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [230][000/391]   Time 0.377 (0.377)   Data 0.186 (0.186)   Loss 0.1118 (0.1118)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:17:14]
  Epoch: [230][200/391]   Time 0.169 (0.160)   Data 0.001 (0.002)   Loss 0.0886 (0.1009)   Prec@1 96.875 (96.467)   Prec@5 100.000 (99.988)   [2018-05-03 18:17:46]
  **Train** Prec@1 96.126 Prec@5 99.980 Error@1 3.874
  **Test** Prec@1 86.780 Prec@5 99.550 Error@1 13.220

==>>[2018-05-03 18:18:25] [Epoch=231/500] [Need: 05:18:10] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [231][000/391]   Time 0.322 (0.322)   Data 0.126 (0.126)   Loss 0.0983 (0.0983)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:18:25]
  Epoch: [231][200/391]   Time 0.163 (0.162)   Data 0.001 (0.002)   Loss 0.1386 (0.1053)   Prec@1 93.750 (96.331)   Prec@5 100.000 (99.969)   [2018-05-03 18:18:57]
  **Train** Prec@1 96.028 Prec@5 99.966 Error@1 3.972
  **Test** Prec@1 88.510 Prec@5 99.590 Error@1 11.490

==>>[2018-05-03 18:19:35] [Epoch=232/500] [Need: 05:16:59] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [232][000/391]   Time 0.373 (0.373)   Data 0.187 (0.187)   Loss 0.1168 (0.1168)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:19:35]
  Epoch: [232][200/391]   Time 0.167 (0.162)   Data 0.001 (0.002)   Loss 0.0730 (0.1074)   Prec@1 98.438 (96.230)   Prec@5 100.000 (99.965)   [2018-05-03 18:20:07]
  **Train** Prec@1 96.184 Prec@5 99.966 Error@1 3.816
  **Test** Prec@1 88.180 Prec@5 99.350 Error@1 11.820

==>>[2018-05-03 18:20:47] [Epoch=233/500] [Need: 05:15:49] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [233][000/391]   Time 0.324 (0.324)   Data 0.131 (0.131)   Loss 0.1705 (0.1705)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 18:20:47]
  Epoch: [233][200/391]   Time 0.166 (0.160)   Data 0.000 (0.001)   Loss 0.1256 (0.1103)   Prec@1 93.750 (96.078)   Prec@5 100.000 (99.973)   [2018-05-03 18:21:19]
  **Train** Prec@1 96.092 Prec@5 99.970 Error@1 3.908
  **Test** Prec@1 88.300 Prec@5 99.580 Error@1 11.700

==>>[2018-05-03 18:21:58] [Epoch=234/500] [Need: 05:14:38] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [234][000/391]   Time 0.299 (0.299)   Data 0.124 (0.124)   Loss 0.1377 (0.1377)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 18:21:58]
  Epoch: [234][200/391]   Time 0.152 (0.159)   Data 0.001 (0.001)   Loss 0.1300 (0.1107)   Prec@1 93.750 (96.179)   Prec@5 100.000 (99.981)   [2018-05-03 18:22:30]
  **Train** Prec@1 96.160 Prec@5 99.974 Error@1 3.840
  **Test** Prec@1 89.990 Prec@5 99.660 Error@1 10.010

==>>[2018-05-03 18:23:07] [Epoch=235/500] [Need: 05:13:25] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [235][000/391]   Time 0.367 (0.367)   Data 0.178 (0.178)   Loss 0.0562 (0.0562)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 18:23:07]
  Epoch: [235][200/391]   Time 0.153 (0.160)   Data 0.001 (0.002)   Loss 0.1205 (0.1077)   Prec@1 98.438 (96.234)   Prec@5 100.000 (99.977)   [2018-05-03 18:23:39]
  **Train** Prec@1 96.044 Prec@5 99.960 Error@1 3.956
  **Test** Prec@1 89.440 Prec@5 99.660 Error@1 10.560

==>>[2018-05-03 18:24:18] [Epoch=236/500] [Need: 05:12:14] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [236][000/391]   Time 0.365 (0.365)   Data 0.150 (0.150)   Loss 0.1335 (0.1335)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:24:18]
  Epoch: [236][200/391]   Time 0.160 (0.158)   Data 0.001 (0.002)   Loss 0.1298 (0.1073)   Prec@1 96.875 (96.261)   Prec@5 99.219 (99.965)   [2018-05-03 18:24:50]
  **Train** Prec@1 96.106 Prec@5 99.968 Error@1 3.894
  **Test** Prec@1 90.040 Prec@5 99.690 Error@1 9.960

==>>[2018-05-03 18:25:28] [Epoch=237/500] [Need: 05:11:02] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [237][000/391]   Time 0.321 (0.321)   Data 0.137 (0.137)   Loss 0.1274 (0.1274)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 18:25:29]
  Epoch: [237][200/391]   Time 0.153 (0.161)   Data 0.001 (0.002)   Loss 0.0868 (0.1099)   Prec@1 97.656 (96.381)   Prec@5 100.000 (99.988)   [2018-05-03 18:26:01]
  **Train** Prec@1 96.096 Prec@5 99.976 Error@1 3.904
  **Test** Prec@1 89.620 Prec@5 99.580 Error@1 10.380

==>>[2018-05-03 18:26:39] [Epoch=238/500] [Need: 05:09:51] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [238][000/391]   Time 0.321 (0.321)   Data 0.128 (0.128)   Loss 0.0775 (0.0775)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-05-03 18:26:40]
  Epoch: [238][200/391]   Time 0.146 (0.158)   Data 0.000 (0.001)   Loss 0.0564 (0.1072)   Prec@1 97.656 (96.366)   Prec@5 100.000 (99.965)   [2018-05-03 18:27:11]
  **Train** Prec@1 96.190 Prec@5 99.960 Error@1 3.810
  **Test** Prec@1 88.060 Prec@5 99.500 Error@1 11.940

==>>[2018-05-03 18:27:49] [Epoch=239/500] [Need: 05:08:39] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [239][000/391]   Time 0.306 (0.306)   Data 0.142 (0.142)   Loss 0.0569 (0.0569)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 18:27:49]
  Epoch: [239][200/391]   Time 0.170 (0.162)   Data 0.001 (0.002)   Loss 0.1200 (0.1047)   Prec@1 96.094 (96.451)   Prec@5 100.000 (99.973)   [2018-05-03 18:28:22]
  **Train** Prec@1 96.150 Prec@5 99.962 Error@1 3.850
  **Test** Prec@1 89.400 Prec@5 99.670 Error@1 10.600

==>>[2018-05-03 18:29:01] [Epoch=240/500] [Need: 05:07:29] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [240][000/391]   Time 0.342 (0.342)   Data 0.154 (0.154)   Loss 0.1794 (0.1794)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 18:29:01]
  Epoch: [240][200/391]   Time 0.135 (0.163)   Data 0.001 (0.001)   Loss 0.1203 (0.1119)   Prec@1 96.094 (96.284)   Prec@5 100.000 (99.973)   [2018-05-03 18:29:33]
  **Train** Prec@1 96.130 Prec@5 99.966 Error@1 3.870
  **Test** Prec@1 90.110 Prec@5 99.610 Error@1 9.890

==>>[2018-05-03 18:30:13] [Epoch=241/500] [Need: 05:06:20] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [241][000/391]   Time 0.317 (0.317)   Data 0.128 (0.128)   Loss 0.0376 (0.0376)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:30:14]
  Epoch: [241][200/391]   Time 0.173 (0.160)   Data 0.001 (0.002)   Loss 0.0892 (0.1070)   Prec@1 97.656 (96.331)   Prec@5 100.000 (99.977)   [2018-05-03 18:30:45]
  **Train** Prec@1 96.106 Prec@5 99.972 Error@1 3.894
  **Test** Prec@1 90.280 Prec@5 99.610 Error@1 9.720

==>>[2018-05-03 18:31:24] [Epoch=242/500] [Need: 05:05:09] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [242][000/391]   Time 0.339 (0.339)   Data 0.158 (0.158)   Loss 0.0614 (0.0614)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-05-03 18:31:25]
  Epoch: [242][200/391]   Time 0.179 (0.160)   Data 0.001 (0.002)   Loss 0.0902 (0.1060)   Prec@1 98.438 (96.467)   Prec@5 100.000 (99.984)   [2018-05-03 18:31:57]
  **Train** Prec@1 96.258 Prec@5 99.976 Error@1 3.742
  **Test** Prec@1 88.870 Prec@5 99.490 Error@1 11.130

==>>[2018-05-03 18:32:36] [Epoch=243/500] [Need: 05:03:58] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [243][000/391]   Time 0.365 (0.365)   Data 0.172 (0.172)   Loss 0.1235 (0.1235)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:32:36]
  Epoch: [243][200/391]   Time 0.180 (0.161)   Data 0.001 (0.002)   Loss 0.1189 (0.1052)   Prec@1 96.094 (96.459)   Prec@5 100.000 (99.988)   [2018-05-03 18:33:08]
  **Train** Prec@1 96.128 Prec@5 99.982 Error@1 3.872
  **Test** Prec@1 89.470 Prec@5 99.670 Error@1 10.530

==>>[2018-05-03 18:33:47] [Epoch=244/500] [Need: 05:02:47] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [244][000/391]   Time 0.317 (0.317)   Data 0.144 (0.144)   Loss 0.0752 (0.0752)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 18:33:47]
  Epoch: [244][200/391]   Time 0.159 (0.162)   Data 0.001 (0.002)   Loss 0.1220 (0.1125)   Prec@1 94.531 (96.218)   Prec@5 100.000 (99.969)   [2018-05-03 18:34:19]
  **Train** Prec@1 96.038 Prec@5 99.968 Error@1 3.962
  **Test** Prec@1 89.890 Prec@5 99.710 Error@1 10.110

==>>[2018-05-03 18:35:00] [Epoch=245/500] [Need: 05:01:39] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [245][000/391]   Time 0.333 (0.333)   Data 0.137 (0.137)   Loss 0.1115 (0.1115)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-05-03 18:35:00]
  Epoch: [245][200/391]   Time 0.177 (0.157)   Data 0.001 (0.001)   Loss 0.0534 (0.1073)   Prec@1 98.438 (96.319)   Prec@5 100.000 (99.984)   [2018-05-03 18:35:31]
  **Train** Prec@1 96.230 Prec@5 99.988 Error@1 3.770
  **Test** Prec@1 89.540 Prec@5 99.640 Error@1 10.460

==>>[2018-05-03 18:36:09] [Epoch=246/500] [Need: 05:00:26] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [246][000/391]   Time 0.352 (0.352)   Data 0.171 (0.171)   Loss 0.0702 (0.0702)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 18:36:10]
  Epoch: [246][200/391]   Time 0.140 (0.159)   Data 0.001 (0.002)   Loss 0.1119 (0.1083)   Prec@1 95.312 (96.230)   Prec@5 100.000 (99.984)   [2018-05-03 18:36:41]
  **Train** Prec@1 96.010 Prec@5 99.970 Error@1 3.990
  **Test** Prec@1 88.740 Prec@5 99.510 Error@1 11.260

==>>[2018-05-03 18:37:20] [Epoch=247/500] [Need: 04:59:15] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [247][000/391]   Time 0.364 (0.364)   Data 0.156 (0.156)   Loss 0.0521 (0.0521)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:37:20]
  Epoch: [247][200/391]   Time 0.156 (0.158)   Data 0.001 (0.002)   Loss 0.1537 (0.0990)   Prec@1 93.750 (96.607)   Prec@5 100.000 (99.977)   [2018-05-03 18:37:51]
  **Train** Prec@1 96.214 Prec@5 99.972 Error@1 3.786
  **Test** Prec@1 88.070 Prec@5 99.550 Error@1 11.930

==>>[2018-05-03 18:38:30] [Epoch=248/500] [Need: 04:58:03] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [248][000/391]   Time 0.340 (0.340)   Data 0.149 (0.149)   Loss 0.1492 (0.1492)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-05-03 18:38:30]
  Epoch: [248][200/391]   Time 0.154 (0.161)   Data 0.001 (0.001)   Loss 0.0560 (0.1078)   Prec@1 97.656 (96.327)   Prec@5 100.000 (99.981)   [2018-05-03 18:39:02]
  **Train** Prec@1 96.066 Prec@5 99.974 Error@1 3.934
  **Test** Prec@1 89.090 Prec@5 99.560 Error@1 10.910

==>>[2018-05-03 18:39:41] [Epoch=249/500] [Need: 04:56:52] [learning_rate=0.1000] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [249][000/391]   Time 0.322 (0.322)   Data 0.125 (0.125)   Loss 0.1375 (0.1375)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-05-03 18:39:41]
  Epoch: [249][200/391]   Time 0.152 (0.160)   Data 0.001 (0.001)   Loss 0.0550 (0.1049)   Prec@1 97.656 (96.300)   Prec@5 100.000 (99.981)   [2018-05-03 18:40:13]
  **Train** Prec@1 96.136 Prec@5 99.964 Error@1 3.864
  **Test** Prec@1 90.170 Prec@5 99.680 Error@1 9.830

==>>[2018-05-03 18:40:54] [Epoch=250/500] [Need: 04:55:42] [learning_rate=0.0100] [Best : Accuracy=91.11, Error=8.89]
  Epoch: [250][000/391]   Time 0.314 (0.314)   Data 0.129 (0.129)   Loss 0.1230 (0.1230)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-05-03 18:40:54]
  Epoch: [250][200/391]   Time 0.158 (0.164)   Data 0.001 (0.001)   Loss 0.0154 (0.0569)   Prec@1 100.000 (98.189)   Prec@5 100.000 (99.996)   [2018-05-03 18:41:27]
  **Train** Prec@1 98.488 Prec@5 99.996 Error@1 1.512
  **Test** Prec@1 93.870 Prec@5 99.880 Error@1 6.130

==>>[2018-05-03 18:42:05] [Epoch=251/500] [Need: 04:54:32] [learning_rate=0.0100] [Best : Accuracy=93.87, Error=6.13]
  Epoch: [251][000/391]   Time 0.387 (0.387)   Data 0.193 (0.193)   Loss 0.0156 (0.0156)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:42:05]
  Epoch: [251][200/391]   Time 0.153 (0.173)   Data 0.001 (0.002)   Loss 0.0101 (0.0270)   Prec@1 100.000 (99.234)   Prec@5 100.000 (100.000)   [2018-05-03 18:42:40]
  **Train** Prec@1 99.272 Prec@5 100.000 Error@1 0.728
  **Test** Prec@1 94.040 Prec@5 99.870 Error@1 5.960

==>>[2018-05-03 18:43:19] [Epoch=252/500] [Need: 04:53:24] [learning_rate=0.0100] [Best : Accuracy=94.04, Error=5.96]
  Epoch: [252][000/391]   Time 0.352 (0.352)   Data 0.170 (0.170)   Loss 0.0174 (0.0174)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:43:19]
  Epoch: [252][200/391]   Time 0.162 (0.163)   Data 0.001 (0.002)   Loss 0.0506 (0.0187)   Prec@1 98.438 (99.464)   Prec@5 100.000 (100.000)   [2018-05-03 18:43:51]
  **Train** Prec@1 99.450 Prec@5 100.000 Error@1 0.550
  **Test** Prec@1 93.910 Prec@5 99.850 Error@1 6.090

==>>[2018-05-03 18:44:31] [Epoch=253/500] [Need: 04:52:14] [learning_rate=0.0100] [Best : Accuracy=94.04, Error=5.96]
  Epoch: [253][000/391]   Time 0.290 (0.290)   Data 0.123 (0.123)   Loss 0.0201 (0.0201)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:44:31]
  Epoch: [253][200/391]   Time 0.160 (0.157)   Data 0.001 (0.001)   Loss 0.0135 (0.0169)   Prec@1 99.219 (99.549)   Prec@5 100.000 (100.000)   [2018-05-03 18:45:02]
  **Train** Prec@1 99.522 Prec@5 100.000 Error@1 0.478
  **Test** Prec@1 94.100 Prec@5 99.850 Error@1 5.900

==>>[2018-05-03 18:45:41] [Epoch=254/500] [Need: 04:51:02] [learning_rate=0.0100] [Best : Accuracy=94.10, Error=5.90]
  Epoch: [254][000/391]   Time 0.319 (0.319)   Data 0.143 (0.143)   Loss 0.0206 (0.0206)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:45:41]
  Epoch: [254][200/391]   Time 0.165 (0.161)   Data 0.001 (0.002)   Loss 0.0277 (0.0132)   Prec@1 100.000 (99.677)   Prec@5 100.000 (100.000)   [2018-05-03 18:46:13]
  **Train** Prec@1 99.662 Prec@5 100.000 Error@1 0.338
  **Test** Prec@1 94.080 Prec@5 99.850 Error@1 5.920

==>>[2018-05-03 18:46:51] [Epoch=255/500] [Need: 04:49:50] [learning_rate=0.0100] [Best : Accuracy=94.10, Error=5.90]
  Epoch: [255][000/391]   Time 0.289 (0.289)   Data 0.125 (0.125)   Loss 0.0070 (0.0070)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:46:51]
  Epoch: [255][200/391]   Time 0.155 (0.157)   Data 0.001 (0.001)   Loss 0.0070 (0.0110)   Prec@1 100.000 (99.701)   Prec@5 100.000 (100.000)   [2018-05-03 18:47:22]
  **Train** Prec@1 99.680 Prec@5 99.998 Error@1 0.320
  **Test** Prec@1 94.300 Prec@5 99.840 Error@1 5.700

==>>[2018-05-03 18:48:00] [Epoch=256/500] [Need: 04:48:37] [learning_rate=0.0100] [Best : Accuracy=94.30, Error=5.70]
  Epoch: [256][000/391]   Time 0.333 (0.333)   Data 0.127 (0.127)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:48:01]
  Epoch: [256][200/391]   Time 0.162 (0.159)   Data 0.001 (0.001)   Loss 0.0142 (0.0101)   Prec@1 99.219 (99.724)   Prec@5 100.000 (100.000)   [2018-05-03 18:48:32]
  **Train** Prec@1 99.690 Prec@5 100.000 Error@1 0.310
  **Test** Prec@1 94.400 Prec@5 99.870 Error@1 5.600

==>>[2018-05-03 18:49:10] [Epoch=257/500] [Need: 04:47:26] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [257][000/391]   Time 0.336 (0.336)   Data 0.145 (0.145)   Loss 0.0267 (0.0267)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:49:11]
  Epoch: [257][200/391]   Time 0.165 (0.160)   Data 0.001 (0.002)   Loss 0.0070 (0.0101)   Prec@1 100.000 (99.701)   Prec@5 100.000 (100.000)   [2018-05-03 18:49:42]
  **Train** Prec@1 99.702 Prec@5 100.000 Error@1 0.298
  **Test** Prec@1 94.260 Prec@5 99.840 Error@1 5.740

==>>[2018-05-03 18:50:20] [Epoch=258/500] [Need: 04:46:13] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [258][000/391]   Time 0.330 (0.330)   Data 0.129 (0.129)   Loss 0.0260 (0.0260)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 18:50:20]
  Epoch: [258][200/391]   Time 0.154 (0.158)   Data 0.001 (0.001)   Loss 0.0017 (0.0087)   Prec@1 100.000 (99.794)   Prec@5 100.000 (100.000)   [2018-05-03 18:50:52]
  **Train** Prec@1 99.776 Prec@5 100.000 Error@1 0.224
  **Test** Prec@1 94.180 Prec@5 99.850 Error@1 5.820

==>>[2018-05-03 18:51:30] [Epoch=259/500] [Need: 04:45:01] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [259][000/391]   Time 0.348 (0.348)   Data 0.140 (0.140)   Loss 0.0065 (0.0065)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:51:30]
  Epoch: [259][200/391]   Time 0.157 (0.159)   Data 0.001 (0.002)   Loss 0.0071 (0.0089)   Prec@1 100.000 (99.775)   Prec@5 100.000 (100.000)   [2018-05-03 18:52:02]
  **Train** Prec@1 99.796 Prec@5 100.000 Error@1 0.204
  **Test** Prec@1 94.320 Prec@5 99.860 Error@1 5.680

==>>[2018-05-03 18:52:41] [Epoch=260/500] [Need: 04:43:50] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [260][000/391]   Time 0.354 (0.354)   Data 0.153 (0.153)   Loss 0.0031 (0.0031)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:52:41]
  Epoch: [260][200/391]   Time 0.132 (0.157)   Data 0.001 (0.001)   Loss 0.0031 (0.0089)   Prec@1 100.000 (99.767)   Prec@5 100.000 (100.000)   [2018-05-03 18:53:12]
  **Train** Prec@1 99.778 Prec@5 99.998 Error@1 0.222
  **Test** Prec@1 94.210 Prec@5 99.870 Error@1 5.790

==>>[2018-05-03 18:53:49] [Epoch=261/500] [Need: 04:42:37] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [261][000/391]   Time 0.330 (0.330)   Data 0.146 (0.146)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:53:50]
  Epoch: [261][200/391]   Time 0.137 (0.160)   Data 0.001 (0.002)   Loss 0.0153 (0.0070)   Prec@1 99.219 (99.821)   Prec@5 100.000 (100.000)   [2018-05-03 18:54:21]
  **Train** Prec@1 99.818 Prec@5 99.998 Error@1 0.182
  **Test** Prec@1 94.290 Prec@5 99.820 Error@1 5.710

==>>[2018-05-03 18:54:59] [Epoch=262/500] [Need: 04:41:25] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [262][000/391]   Time 0.337 (0.337)   Data 0.141 (0.141)   Loss 0.0055 (0.0055)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:54:59]
  Epoch: [262][200/391]   Time 0.146 (0.156)   Data 0.001 (0.002)   Loss 0.0039 (0.0070)   Prec@1 100.000 (99.833)   Prec@5 100.000 (99.996)   [2018-05-03 18:55:31]
  **Train** Prec@1 99.852 Prec@5 99.998 Error@1 0.148
  **Test** Prec@1 94.390 Prec@5 99.840 Error@1 5.610

==>>[2018-05-03 18:56:08] [Epoch=263/500] [Need: 04:40:13] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [263][000/391]   Time 0.340 (0.340)   Data 0.151 (0.151)   Loss 0.0053 (0.0053)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:56:09]
  Epoch: [263][200/391]   Time 0.160 (0.160)   Data 0.001 (0.001)   Loss 0.0009 (0.0068)   Prec@1 100.000 (99.837)   Prec@5 100.000 (100.000)   [2018-05-03 18:56:41]
  **Train** Prec@1 99.846 Prec@5 100.000 Error@1 0.154
  **Test** Prec@1 94.390 Prec@5 99.860 Error@1 5.610

==>>[2018-05-03 18:57:18] [Epoch=264/500] [Need: 04:39:01] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [264][000/391]   Time 0.328 (0.328)   Data 0.138 (0.138)   Loss 0.0059 (0.0059)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:57:18]
  Epoch: [264][200/391]   Time 0.172 (0.159)   Data 0.001 (0.002)   Loss 0.0009 (0.0062)   Prec@1 100.000 (99.872)   Prec@5 100.000 (100.000)   [2018-05-03 18:57:50]
  **Train** Prec@1 99.864 Prec@5 100.000 Error@1 0.136
  **Test** Prec@1 94.370 Prec@5 99.850 Error@1 5.630

==>>[2018-05-03 18:58:27] [Epoch=265/500] [Need: 04:37:48] [learning_rate=0.0100] [Best : Accuracy=94.40, Error=5.60]
  Epoch: [265][000/391]   Time 0.314 (0.314)   Data 0.133 (0.133)   Loss 0.0048 (0.0048)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:58:28]
  Epoch: [265][200/391]   Time 0.153 (0.157)   Data 0.001 (0.001)   Loss 0.0041 (0.0066)   Prec@1 100.000 (99.852)   Prec@5 100.000 (100.000)   [2018-05-03 18:58:59]
  **Train** Prec@1 99.854 Prec@5 100.000 Error@1 0.146
  **Test** Prec@1 94.420 Prec@5 99.820 Error@1 5.580

==>>[2018-05-03 18:59:36] [Epoch=266/500] [Need: 04:36:36] [learning_rate=0.0100] [Best : Accuracy=94.42, Error=5.58]
  Epoch: [266][000/391]   Time 0.357 (0.357)   Data 0.163 (0.163)   Loss 0.0069 (0.0069)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 18:59:37]
  Epoch: [266][200/391]   Time 0.155 (0.161)   Data 0.001 (0.002)   Loss 0.0062 (0.0049)   Prec@1 100.000 (99.883)   Prec@5 100.000 (100.000)   [2018-05-03 19:00:09]
  **Train** Prec@1 99.868 Prec@5 100.000 Error@1 0.132
  **Test** Prec@1 94.490 Prec@5 99.840 Error@1 5.510

==>>[2018-05-03 19:00:46] [Epoch=267/500] [Need: 04:35:24] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [267][000/391]   Time 0.372 (0.372)   Data 0.173 (0.173)   Loss 0.0027 (0.0027)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:00:46]
  Epoch: [267][200/391]   Time 0.165 (0.156)   Data 0.001 (0.002)   Loss 0.0027 (0.0042)   Prec@1 100.000 (99.914)   Prec@5 100.000 (100.000)   [2018-05-03 19:01:17]
  **Train** Prec@1 99.870 Prec@5 99.998 Error@1 0.130
  **Test** Prec@1 94.400 Prec@5 99.820 Error@1 5.600

==>>[2018-05-03 19:01:55] [Epoch=268/500] [Need: 04:34:11] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [268][000/391]   Time 0.294 (0.294)   Data 0.137 (0.137)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:01:55]
  Epoch: [268][200/391]   Time 0.159 (0.160)   Data 0.001 (0.002)   Loss 0.0021 (0.0048)   Prec@1 100.000 (99.876)   Prec@5 100.000 (100.000)   [2018-05-03 19:02:27]
  **Train** Prec@1 99.894 Prec@5 100.000 Error@1 0.106
  **Test** Prec@1 94.400 Prec@5 99.860 Error@1 5.600

==>>[2018-05-03 19:03:05] [Epoch=269/500] [Need: 04:32:59] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [269][000/391]   Time 0.366 (0.366)   Data 0.169 (0.169)   Loss 0.0140 (0.0140)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 19:03:05]
  Epoch: [269][200/391]   Time 0.141 (0.155)   Data 0.001 (0.002)   Loss 0.0013 (0.0051)   Prec@1 100.000 (99.883)   Prec@5 100.000 (100.000)   [2018-05-03 19:03:36]
  **Train** Prec@1 99.890 Prec@5 100.000 Error@1 0.110
  **Test** Prec@1 94.380 Prec@5 99.830 Error@1 5.620

==>>[2018-05-03 19:04:13] [Epoch=270/500] [Need: 04:31:46] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [270][000/391]   Time 0.326 (0.326)   Data 0.134 (0.134)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:04:13]
  Epoch: [270][200/391]   Time 0.175 (0.155)   Data 0.001 (0.001)   Loss 0.0034 (0.0046)   Prec@1 100.000 (99.880)   Prec@5 100.000 (100.000)   [2018-05-03 19:04:44]
  **Train** Prec@1 99.874 Prec@5 100.000 Error@1 0.126
  **Test** Prec@1 94.480 Prec@5 99.830 Error@1 5.520

==>>[2018-05-03 19:05:22] [Epoch=271/500] [Need: 04:30:33] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [271][000/391]   Time 0.290 (0.290)   Data 0.124 (0.124)   Loss 0.0067 (0.0067)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:05:22]
  Epoch: [271][200/391]   Time 0.158 (0.160)   Data 0.001 (0.001)   Loss 0.0018 (0.0045)   Prec@1 100.000 (99.872)   Prec@5 100.000 (100.000)   [2018-05-03 19:05:54]
  **Train** Prec@1 99.882 Prec@5 100.000 Error@1 0.118
  **Test** Prec@1 94.330 Prec@5 99.800 Error@1 5.670

==>>[2018-05-03 19:06:32] [Epoch=272/500] [Need: 04:29:22] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [272][000/391]   Time 0.302 (0.302)   Data 0.122 (0.122)   Loss 0.0012 (0.0012)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:06:32]
  Epoch: [272][200/391]   Time 0.151 (0.158)   Data 0.001 (0.001)   Loss 0.0013 (0.0035)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 19:07:04]
  **Train** Prec@1 99.912 Prec@5 100.000 Error@1 0.088
  **Test** Prec@1 94.410 Prec@5 99.810 Error@1 5.590

==>>[2018-05-03 19:07:41] [Epoch=273/500] [Need: 04:28:10] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [273][000/391]   Time 0.304 (0.304)   Data 0.127 (0.127)   Loss 0.0036 (0.0036)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:07:42]
  Epoch: [273][200/391]   Time 0.147 (0.154)   Data 0.000 (0.001)   Loss 0.0023 (0.0038)   Prec@1 100.000 (99.899)   Prec@5 100.000 (100.000)   [2018-05-03 19:08:12]
  **Train** Prec@1 99.892 Prec@5 100.000 Error@1 0.108
  **Test** Prec@1 94.260 Prec@5 99.780 Error@1 5.740

==>>[2018-05-03 19:08:51] [Epoch=274/500] [Need: 04:26:58] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [274][000/391]   Time 0.326 (0.326)   Data 0.151 (0.151)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:08:51]
  Epoch: [274][200/391]   Time 0.148 (0.157)   Data 0.001 (0.002)   Loss 0.0004 (0.0039)   Prec@1 100.000 (99.903)   Prec@5 100.000 (100.000)   [2018-05-03 19:09:22]
  **Train** Prec@1 99.916 Prec@5 100.000 Error@1 0.084
  **Test** Prec@1 94.330 Prec@5 99.820 Error@1 5.670

==>>[2018-05-03 19:10:00] [Epoch=275/500] [Need: 04:25:45] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [275][000/391]   Time 0.313 (0.313)   Data 0.125 (0.125)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:10:00]
  Epoch: [275][200/391]   Time 0.157 (0.159)   Data 0.001 (0.001)   Loss 0.0013 (0.0037)   Prec@1 100.000 (99.914)   Prec@5 100.000 (100.000)   [2018-05-03 19:10:32]
  **Train** Prec@1 99.908 Prec@5 100.000 Error@1 0.092
  **Test** Prec@1 94.420 Prec@5 99.790 Error@1 5.580

==>>[2018-05-03 19:11:10] [Epoch=276/500] [Need: 04:24:34] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [276][000/391]   Time 0.311 (0.311)   Data 0.126 (0.126)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:11:10]
  Epoch: [276][200/391]   Time 0.171 (0.162)   Data 0.001 (0.001)   Loss 0.0009 (0.0029)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 19:11:42]
  **Train** Prec@1 99.916 Prec@5 100.000 Error@1 0.084
  **Test** Prec@1 94.400 Prec@5 99.800 Error@1 5.600

==>>[2018-05-03 19:12:20] [Epoch=277/500] [Need: 04:23:22] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [277][000/391]   Time 0.329 (0.329)   Data 0.150 (0.150)   Loss 0.0101 (0.0101)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 19:12:20]
  Epoch: [277][200/391]   Time 0.157 (0.154)   Data 0.001 (0.002)   Loss 0.0011 (0.0034)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:12:51]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.380 Prec@5 99.810 Error@1 5.620

==>>[2018-05-03 19:13:29] [Epoch=278/500] [Need: 04:22:10] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [278][000/391]   Time 0.318 (0.318)   Data 0.125 (0.125)   Loss 0.0092 (0.0092)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:13:29]
  Epoch: [278][200/391]   Time 0.150 (0.154)   Data 0.001 (0.001)   Loss 0.0011 (0.0032)   Prec@1 100.000 (99.926)   Prec@5 100.000 (100.000)   [2018-05-03 19:14:00]
  **Train** Prec@1 99.924 Prec@5 100.000 Error@1 0.076
  **Test** Prec@1 94.420 Prec@5 99.800 Error@1 5.580

==>>[2018-05-03 19:14:40] [Epoch=279/500] [Need: 04:20:59] [learning_rate=0.0100] [Best : Accuracy=94.49, Error=5.51]
  Epoch: [279][000/391]   Time 0.296 (0.296)   Data 0.122 (0.122)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:14:40]
  Epoch: [279][200/391]   Time 0.144 (0.162)   Data 0.001 (0.001)   Loss 0.0025 (0.0032)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 19:15:12]
  **Train** Prec@1 99.924 Prec@5 100.000 Error@1 0.076
  **Test** Prec@1 94.550 Prec@5 99.830 Error@1 5.450

==>>[2018-05-03 19:15:50] [Epoch=280/500] [Need: 04:19:48] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [280][000/391]   Time 0.338 (0.338)   Data 0.150 (0.150)   Loss 0.0048 (0.0048)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:15:50]
  Epoch: [280][200/391]   Time 0.153 (0.160)   Data 0.001 (0.001)   Loss 0.0005 (0.0029)   Prec@1 100.000 (99.926)   Prec@5 100.000 (100.000)   [2018-05-03 19:16:22]
  **Train** Prec@1 99.916 Prec@5 100.000 Error@1 0.084
  **Test** Prec@1 94.450 Prec@5 99.800 Error@1 5.550

==>>[2018-05-03 19:17:00] [Epoch=281/500] [Need: 04:18:36] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [281][000/391]   Time 0.343 (0.343)   Data 0.155 (0.155)   Loss 0.0062 (0.0062)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:17:01]
  Epoch: [281][200/391]   Time 0.159 (0.162)   Data 0.001 (0.002)   Loss 0.0039 (0.0032)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 19:17:33]
  **Train** Prec@1 99.930 Prec@5 100.000 Error@1 0.070
  **Test** Prec@1 94.490 Prec@5 99.830 Error@1 5.510

==>>[2018-05-03 19:18:11] [Epoch=282/500] [Need: 04:17:25] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [282][000/391]   Time 0.292 (0.292)   Data 0.130 (0.130)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:18:11]
  Epoch: [282][200/391]   Time 0.145 (0.159)   Data 0.001 (0.001)   Loss 0.0090 (0.0028)   Prec@1 99.219 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 19:18:43]
  **Train** Prec@1 99.938 Prec@5 100.000 Error@1 0.062
  **Test** Prec@1 94.480 Prec@5 99.820 Error@1 5.520

==>>[2018-05-03 19:19:21] [Epoch=283/500] [Need: 04:16:14] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [283][000/391]   Time 0.310 (0.310)   Data 0.131 (0.131)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:19:21]
  Epoch: [283][200/391]   Time 0.153 (0.168)   Data 0.001 (0.001)   Loss 0.0010 (0.0032)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:19:55]
  **Train** Prec@1 99.918 Prec@5 100.000 Error@1 0.082
  **Test** Prec@1 94.460 Prec@5 99.840 Error@1 5.540

==>>[2018-05-03 19:20:33] [Epoch=284/500] [Need: 04:15:04] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [284][000/391]   Time 0.353 (0.353)   Data 0.182 (0.182)   Loss 0.0060 (0.0060)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:20:34]
  Epoch: [284][200/391]   Time 0.141 (0.163)   Data 0.001 (0.002)   Loss 0.0023 (0.0030)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:21:06]
  **Train** Prec@1 99.922 Prec@5 100.000 Error@1 0.078
  **Test** Prec@1 94.480 Prec@5 99.800 Error@1 5.520

==>>[2018-05-03 19:21:45] [Epoch=285/500] [Need: 04:13:54] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [285][000/391]   Time 0.335 (0.335)   Data 0.144 (0.144)   Loss 0.0116 (0.0116)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 19:21:45]
  Epoch: [285][200/391]   Time 0.165 (0.160)   Data 0.001 (0.001)   Loss 0.0042 (0.0031)   Prec@1 100.000 (99.903)   Prec@5 100.000 (100.000)   [2018-05-03 19:22:17]
  **Train** Prec@1 99.906 Prec@5 100.000 Error@1 0.094
  **Test** Prec@1 94.520 Prec@5 99.850 Error@1 5.480

==>>[2018-05-03 19:22:55] [Epoch=286/500] [Need: 04:12:42] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [286][000/391]   Time 0.290 (0.290)   Data 0.124 (0.124)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:22:55]
  Epoch: [286][200/391]   Time 0.160 (0.159)   Data 0.001 (0.001)   Loss 0.0004 (0.0030)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 19:23:27]
  **Train** Prec@1 99.946 Prec@5 100.000 Error@1 0.054
  **Test** Prec@1 94.490 Prec@5 99.820 Error@1 5.510

==>>[2018-05-03 19:24:06] [Epoch=287/500] [Need: 04:11:32] [learning_rate=0.0100] [Best : Accuracy=94.55, Error=5.45]
  Epoch: [287][000/391]   Time 0.328 (0.328)   Data 0.128 (0.128)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:24:06]
  Epoch: [287][200/391]   Time 0.151 (0.159)   Data 0.001 (0.002)   Loss 0.0198 (0.0022)   Prec@1 99.219 (99.969)   Prec@5 100.000 (100.000)   [2018-05-03 19:24:38]
  **Train** Prec@1 99.958 Prec@5 100.000 Error@1 0.042
  **Test** Prec@1 94.570 Prec@5 99.840 Error@1 5.430

==>>[2018-05-03 19:25:16] [Epoch=288/500] [Need: 04:10:20] [learning_rate=0.0100] [Best : Accuracy=94.57, Error=5.43]
  Epoch: [288][000/391]   Time 0.369 (0.369)   Data 0.169 (0.169)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:25:17]
  Epoch: [288][200/391]   Time 0.154 (0.162)   Data 0.001 (0.002)   Loss 0.0007 (0.0027)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 19:25:49]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.490 Prec@5 99.830 Error@1 5.510

==>>[2018-05-03 19:26:26] [Epoch=289/500] [Need: 04:09:09] [learning_rate=0.0100] [Best : Accuracy=94.57, Error=5.43]
  Epoch: [289][000/391]   Time 0.278 (0.278)   Data 0.121 (0.121)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:26:26]
  Epoch: [289][200/391]   Time 0.177 (0.160)   Data 0.001 (0.001)   Loss 0.0011 (0.0023)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 19:26:58]
  **Train** Prec@1 99.954 Prec@5 100.000 Error@1 0.046
  **Test** Prec@1 94.510 Prec@5 99.800 Error@1 5.490

==>>[2018-05-03 19:27:35] [Epoch=290/500] [Need: 04:07:57] [learning_rate=0.0100] [Best : Accuracy=94.57, Error=5.43]
  Epoch: [290][000/391]   Time 0.332 (0.332)   Data 0.149 (0.149)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:27:35]
  Epoch: [290][200/391]   Time 0.130 (0.157)   Data 0.000 (0.001)   Loss 0.0007 (0.0026)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 19:28:07]
  **Train** Prec@1 99.928 Prec@5 100.000 Error@1 0.072
  **Test** Prec@1 94.530 Prec@5 99.800 Error@1 5.470

==>>[2018-05-03 19:28:44] [Epoch=291/500] [Need: 04:06:45] [learning_rate=0.0100] [Best : Accuracy=94.57, Error=5.43]
  Epoch: [291][000/391]   Time 0.345 (0.345)   Data 0.148 (0.148)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:28:45]
  Epoch: [291][200/391]   Time 0.150 (0.158)   Data 0.001 (0.002)   Loss 0.0010 (0.0025)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 19:29:16]
  **Train** Prec@1 99.936 Prec@5 100.000 Error@1 0.064
  **Test** Prec@1 94.460 Prec@5 99.830 Error@1 5.540

==>>[2018-05-03 19:29:54] [Epoch=292/500] [Need: 04:05:33] [learning_rate=0.0100] [Best : Accuracy=94.57, Error=5.43]
  Epoch: [292][000/391]   Time 0.304 (0.304)   Data 0.130 (0.130)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:29:54]
  Epoch: [292][200/391]   Time 0.147 (0.157)   Data 0.001 (0.001)   Loss 0.0005 (0.0029)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 19:30:25]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.650 Prec@5 99.880 Error@1 5.350

==>>[2018-05-03 19:31:03] [Epoch=293/500] [Need: 04:04:21] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [293][000/391]   Time 0.337 (0.337)   Data 0.159 (0.159)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:31:03]
  Epoch: [293][200/391]   Time 0.156 (0.156)   Data 0.001 (0.001)   Loss 0.0285 (0.0028)   Prec@1 99.219 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 19:31:34]
  **Train** Prec@1 99.938 Prec@5 100.000 Error@1 0.062
  **Test** Prec@1 94.430 Prec@5 99.840 Error@1 5.570

==>>[2018-05-03 19:32:11] [Epoch=294/500] [Need: 04:03:08] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [294][000/391]   Time 0.291 (0.291)   Data 0.128 (0.128)   Loss 0.0059 (0.0059)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:32:11]
  Epoch: [294][200/391]   Time 0.163 (0.158)   Data 0.001 (0.001)   Loss 0.0002 (0.0022)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 19:32:43]
  **Train** Prec@1 99.926 Prec@5 100.000 Error@1 0.074
  **Test** Prec@1 94.430 Prec@5 99.820 Error@1 5.570

==>>[2018-05-03 19:33:20] [Epoch=295/500] [Need: 04:01:56] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [295][000/391]   Time 0.301 (0.301)   Data 0.124 (0.124)   Loss 0.0049 (0.0049)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:33:21]
  Epoch: [295][200/391]   Time 0.182 (0.155)   Data 0.001 (0.001)   Loss 0.0015 (0.0027)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 19:33:52]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.340 Prec@5 99.810 Error@1 5.660

==>>[2018-05-03 19:34:29] [Epoch=296/500] [Need: 04:00:44] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [296][000/391]   Time 0.317 (0.317)   Data 0.156 (0.156)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:34:30]
  Epoch: [296][200/391]   Time 0.186 (0.154)   Data 0.001 (0.002)   Loss 0.0008 (0.0024)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 19:35:00]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.420 Prec@5 99.840 Error@1 5.580

==>>[2018-05-03 19:35:38] [Epoch=297/500] [Need: 03:59:32] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [297][000/391]   Time 0.343 (0.343)   Data 0.151 (0.151)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:35:38]
  Epoch: [297][200/391]   Time 0.166 (0.157)   Data 0.001 (0.002)   Loss 0.0008 (0.0024)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 19:36:09]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.280 Prec@5 99.830 Error@1 5.720

==>>[2018-05-03 19:36:48] [Epoch=298/500] [Need: 03:58:20] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [298][000/391]   Time 0.357 (0.357)   Data 0.165 (0.165)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:36:48]
  Epoch: [298][200/391]   Time 0.178 (0.155)   Data 0.001 (0.001)   Loss 0.0040 (0.0026)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:37:19]
  **Train** Prec@1 99.928 Prec@5 100.000 Error@1 0.072
  **Test** Prec@1 94.390 Prec@5 99.780 Error@1 5.610

==>>[2018-05-03 19:37:56] [Epoch=299/500] [Need: 03:57:08] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [299][000/391]   Time 0.334 (0.334)   Data 0.156 (0.156)   Loss 0.0134 (0.0134)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 19:37:57]
  Epoch: [299][200/391]   Time 0.162 (0.162)   Data 0.001 (0.002)   Loss 0.0007 (0.0022)   Prec@1 100.000 (99.949)   Prec@5 100.000 (100.000)   [2018-05-03 19:38:29]
  **Train** Prec@1 99.960 Prec@5 100.000 Error@1 0.040
  **Test** Prec@1 94.470 Prec@5 99.800 Error@1 5.530

==>>[2018-05-03 19:39:06] [Epoch=300/500] [Need: 03:55:57] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [300][000/391]   Time 0.357 (0.357)   Data 0.190 (0.190)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:39:07]
  Epoch: [300][200/391]   Time 0.149 (0.159)   Data 0.001 (0.002)   Loss 0.0032 (0.0031)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:39:38]
  **Train** Prec@1 99.934 Prec@5 100.000 Error@1 0.066
  **Test** Prec@1 94.450 Prec@5 99.790 Error@1 5.550

==>>[2018-05-03 19:40:16] [Epoch=301/500] [Need: 03:54:45] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [301][000/391]   Time 0.383 (0.383)   Data 0.158 (0.158)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:40:16]
  Epoch: [301][200/391]   Time 0.166 (0.159)   Data 0.001 (0.002)   Loss 0.0168 (0.0026)   Prec@1 99.219 (99.926)   Prec@5 100.000 (100.000)   [2018-05-03 19:40:48]
  **Train** Prec@1 99.930 Prec@5 100.000 Error@1 0.070
  **Test** Prec@1 94.340 Prec@5 99.780 Error@1 5.660

==>>[2018-05-03 19:41:26] [Epoch=302/500] [Need: 03:53:34] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [302][000/391]   Time 0.363 (0.363)   Data 0.184 (0.184)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:41:26]
  Epoch: [302][200/391]   Time 0.143 (0.159)   Data 0.001 (0.002)   Loss 0.0007 (0.0023)   Prec@1 100.000 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 19:41:58]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.370 Prec@5 99.810 Error@1 5.630

==>>[2018-05-03 19:42:36] [Epoch=303/500] [Need: 03:52:23] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [303][000/391]   Time 0.326 (0.326)   Data 0.143 (0.143)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:42:36]
  Epoch: [303][200/391]   Time 0.164 (0.160)   Data 0.001 (0.001)   Loss 0.0004 (0.0022)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 19:43:08]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.500 Prec@5 99.790 Error@1 5.500

==>>[2018-05-03 19:43:46] [Epoch=304/500] [Need: 03:51:11] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [304][000/391]   Time 0.325 (0.325)   Data 0.131 (0.131)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:43:46]
  Epoch: [304][200/391]   Time 0.169 (0.158)   Data 0.001 (0.001)   Loss 0.0009 (0.0029)   Prec@1 100.000 (99.926)   Prec@5 100.000 (100.000)   [2018-05-03 19:44:17]
  **Train** Prec@1 99.934 Prec@5 100.000 Error@1 0.066
  **Test** Prec@1 94.370 Prec@5 99.790 Error@1 5.630

==>>[2018-05-03 19:44:55] [Epoch=305/500] [Need: 03:50:00] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [305][000/391]   Time 0.342 (0.342)   Data 0.173 (0.173)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:44:56]
  Epoch: [305][200/391]   Time 0.147 (0.155)   Data 0.001 (0.002)   Loss 0.0010 (0.0022)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 19:45:27]
  **Train** Prec@1 99.952 Prec@5 100.000 Error@1 0.048
  **Test** Prec@1 94.310 Prec@5 99.840 Error@1 5.690

==>>[2018-05-03 19:46:04] [Epoch=306/500] [Need: 03:48:48] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [306][000/391]   Time 0.327 (0.327)   Data 0.152 (0.152)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:46:05]
  Epoch: [306][200/391]   Time 0.151 (0.159)   Data 0.001 (0.002)   Loss 0.0042 (0.0024)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 19:46:36]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.210 Prec@5 99.810 Error@1 5.790

==>>[2018-05-03 19:47:15] [Epoch=307/500] [Need: 03:47:37] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [307][000/391]   Time 0.346 (0.346)   Data 0.143 (0.143)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:47:15]
  Epoch: [307][200/391]   Time 0.130 (0.159)   Data 0.001 (0.002)   Loss 0.0006 (0.0025)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 19:47:47]
  **Train** Prec@1 99.946 Prec@5 100.000 Error@1 0.054
  **Test** Prec@1 94.510 Prec@5 99.830 Error@1 5.490

==>>[2018-05-03 19:48:24] [Epoch=308/500] [Need: 03:46:25] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [308][000/391]   Time 0.355 (0.355)   Data 0.155 (0.155)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:48:25]
  Epoch: [308][200/391]   Time 0.148 (0.158)   Data 0.000 (0.001)   Loss 0.0025 (0.0018)   Prec@1 100.000 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 19:48:56]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.590 Prec@5 99.780 Error@1 5.410

==>>[2018-05-03 19:49:34] [Epoch=309/500] [Need: 03:45:14] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [309][000/391]   Time 0.294 (0.294)   Data 0.117 (0.117)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:49:34]
  Epoch: [309][200/391]   Time 0.158 (0.157)   Data 0.001 (0.001)   Loss 0.0030 (0.0021)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 19:50:06]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.640 Prec@5 99.800 Error@1 5.360

==>>[2018-05-03 19:50:43] [Epoch=310/500] [Need: 03:44:02] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [310][000/391]   Time 0.365 (0.365)   Data 0.168 (0.168)   Loss 0.0043 (0.0043)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:50:44]
  Epoch: [310][200/391]   Time 0.136 (0.158)   Data 0.001 (0.002)   Loss 0.0007 (0.0026)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 19:51:15]
  **Train** Prec@1 99.958 Prec@5 100.000 Error@1 0.042
  **Test** Prec@1 94.580 Prec@5 99.830 Error@1 5.420

==>>[2018-05-03 19:51:54] [Epoch=311/500] [Need: 03:42:51] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [311][000/391]   Time 0.315 (0.315)   Data 0.129 (0.129)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:51:54]
  Epoch: [311][200/391]   Time 0.160 (0.158)   Data 0.001 (0.001)   Loss 0.0011 (0.0021)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 19:52:26]
  **Train** Prec@1 99.968 Prec@5 100.000 Error@1 0.032
  **Test** Prec@1 94.370 Prec@5 99.840 Error@1 5.630

==>>[2018-05-03 19:53:03] [Epoch=312/500] [Need: 03:41:40] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [312][000/391]   Time 0.350 (0.350)   Data 0.154 (0.154)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:53:04]
  Epoch: [312][200/391]   Time 0.163 (0.158)   Data 0.001 (0.002)   Loss 0.0004 (0.0017)   Prec@1 100.000 (99.969)   Prec@5 100.000 (100.000)   [2018-05-03 19:53:35]
  **Train** Prec@1 99.934 Prec@5 100.000 Error@1 0.066
  **Test** Prec@1 94.390 Prec@5 99.760 Error@1 5.610

==>>[2018-05-03 19:54:13] [Epoch=313/500] [Need: 03:40:28] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [313][000/391]   Time 0.336 (0.336)   Data 0.148 (0.148)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:54:13]
  Epoch: [313][200/391]   Time 0.163 (0.157)   Data 0.001 (0.001)   Loss 0.0036 (0.0028)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 19:54:45]
  **Train** Prec@1 99.934 Prec@5 100.000 Error@1 0.066
  **Test** Prec@1 94.380 Prec@5 99.810 Error@1 5.620

==>>[2018-05-03 19:55:23] [Epoch=314/500] [Need: 03:39:17] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [314][000/391]   Time 0.375 (0.375)   Data 0.207 (0.207)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:55:23]
  Epoch: [314][200/391]   Time 0.159 (0.162)   Data 0.001 (0.002)   Loss 0.0005 (0.0020)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 19:55:55]
  **Train** Prec@1 99.958 Prec@5 100.000 Error@1 0.042
  **Test** Prec@1 94.260 Prec@5 99.810 Error@1 5.740

==>>[2018-05-03 19:56:33] [Epoch=315/500] [Need: 03:38:06] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [315][000/391]   Time 0.312 (0.312)   Data 0.141 (0.141)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:56:34]
  Epoch: [315][200/391]   Time 0.137 (0.166)   Data 0.001 (0.001)   Loss 0.0009 (0.0025)   Prec@1 100.000 (99.934)   Prec@5 100.000 (100.000)   [2018-05-03 19:57:07]
  **Train** Prec@1 99.920 Prec@5 100.000 Error@1 0.080
  **Test** Prec@1 94.290 Prec@5 99.850 Error@1 5.710

==>>[2018-05-03 19:57:45] [Epoch=316/500] [Need: 03:36:56] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [316][000/391]   Time 0.339 (0.339)   Data 0.157 (0.157)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:57:45]
  Epoch: [316][200/391]   Time 0.152 (0.159)   Data 0.001 (0.002)   Loss 0.0028 (0.0022)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 19:58:17]
  **Train** Prec@1 99.956 Prec@5 100.000 Error@1 0.044
  **Test** Prec@1 94.410 Prec@5 99.790 Error@1 5.590

==>>[2018-05-03 19:58:55] [Epoch=317/500] [Need: 03:35:45] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [317][000/391]   Time 0.324 (0.324)   Data 0.131 (0.131)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 19:58:55]
  Epoch: [317][200/391]   Time 0.149 (0.157)   Data 0.001 (0.001)   Loss 0.0023 (0.0022)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 19:59:26]
  **Train** Prec@1 99.942 Prec@5 100.000 Error@1 0.058
  **Test** Prec@1 94.460 Prec@5 99.830 Error@1 5.540

==>>[2018-05-03 20:00:04] [Epoch=318/500] [Need: 03:34:33] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [318][000/391]   Time 0.337 (0.337)   Data 0.145 (0.145)   Loss 0.0104 (0.0104)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 20:00:04]
  Epoch: [318][200/391]   Time 0.150 (0.158)   Data 0.001 (0.001)   Loss 0.0015 (0.0023)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 20:00:35]
  **Train** Prec@1 99.938 Prec@5 100.000 Error@1 0.062
  **Test** Prec@1 94.380 Prec@5 99.810 Error@1 5.620

==>>[2018-05-03 20:01:13] [Epoch=319/500] [Need: 03:33:21] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [319][000/391]   Time 0.288 (0.288)   Data 0.139 (0.139)   Loss 0.0011 (0.0011)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:01:13]
  Epoch: [319][200/391]   Time 0.146 (0.159)   Data 0.001 (0.002)   Loss 0.0010 (0.0023)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 20:01:45]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.330 Prec@5 99.800 Error@1 5.670

==>>[2018-05-03 20:02:23] [Epoch=320/500] [Need: 03:32:10] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [320][000/391]   Time 0.338 (0.338)   Data 0.140 (0.140)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:02:23]
  Epoch: [320][200/391]   Time 0.150 (0.160)   Data 0.001 (0.001)   Loss 0.0003 (0.0023)   Prec@1 100.000 (99.949)   Prec@5 100.000 (100.000)   [2018-05-03 20:02:55]
  **Train** Prec@1 99.958 Prec@5 100.000 Error@1 0.042
  **Test** Prec@1 94.480 Prec@5 99.800 Error@1 5.520

==>>[2018-05-03 20:03:32] [Epoch=321/500] [Need: 03:30:59] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [321][000/391]   Time 0.313 (0.313)   Data 0.144 (0.144)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:03:33]
  Epoch: [321][200/391]   Time 0.152 (0.156)   Data 0.001 (0.002)   Loss 0.0178 (0.0019)   Prec@1 98.438 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 20:04:04]
  **Train** Prec@1 99.956 Prec@5 100.000 Error@1 0.044
  **Test** Prec@1 94.400 Prec@5 99.820 Error@1 5.600

==>>[2018-05-03 20:04:42] [Epoch=322/500] [Need: 03:29:47] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [322][000/391]   Time 0.302 (0.302)   Data 0.136 (0.136)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:04:42]
  Epoch: [322][200/391]   Time 0.160 (0.157)   Data 0.001 (0.001)   Loss 0.0010 (0.0020)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 20:05:13]
  **Train** Prec@1 99.938 Prec@5 100.000 Error@1 0.062
  **Test** Prec@1 94.440 Prec@5 99.830 Error@1 5.560

==>>[2018-05-03 20:05:50] [Epoch=323/500] [Need: 03:28:35] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [323][000/391]   Time 0.378 (0.378)   Data 0.169 (0.169)   Loss 0.0012 (0.0012)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:05:51]
  Epoch: [323][200/391]   Time 0.159 (0.160)   Data 0.001 (0.002)   Loss 0.0011 (0.0019)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:06:22]
  **Train** Prec@1 99.960 Prec@5 100.000 Error@1 0.040
  **Test** Prec@1 94.390 Prec@5 99.810 Error@1 5.610

==>>[2018-05-03 20:07:01] [Epoch=324/500] [Need: 03:27:24] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [324][000/391]   Time 0.355 (0.355)   Data 0.166 (0.166)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:07:01]
  Epoch: [324][200/391]   Time 0.159 (0.154)   Data 0.001 (0.002)   Loss 0.0003 (0.0019)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:07:32]
  **Train** Prec@1 99.952 Prec@5 100.000 Error@1 0.048
  **Test** Prec@1 94.410 Prec@5 99.780 Error@1 5.590

==>>[2018-05-03 20:08:08] [Epoch=325/500] [Need: 03:26:12] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [325][000/391]   Time 0.306 (0.306)   Data 0.137 (0.137)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:08:09]
  Epoch: [325][200/391]   Time 0.160 (0.159)   Data 0.001 (0.001)   Loss 0.0156 (0.0026)   Prec@1 99.219 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 20:08:40]
  **Train** Prec@1 99.912 Prec@5 100.000 Error@1 0.088
  **Test** Prec@1 94.350 Prec@5 99.820 Error@1 5.650

==>>[2018-05-03 20:09:19] [Epoch=326/500] [Need: 03:25:01] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [326][000/391]   Time 0.344 (0.344)   Data 0.154 (0.154)   Loss 0.0032 (0.0032)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:09:19]
  Epoch: [326][200/391]   Time 0.169 (0.163)   Data 0.001 (0.002)   Loss 0.0071 (0.0026)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 20:09:51]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.630 Prec@5 99.820 Error@1 5.370

==>>[2018-05-03 20:10:29] [Epoch=327/500] [Need: 03:23:51] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [327][000/391]   Time 0.355 (0.355)   Data 0.151 (0.151)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:10:30]
  Epoch: [327][200/391]   Time 0.157 (0.155)   Data 0.001 (0.002)   Loss 0.0091 (0.0021)   Prec@1 99.219 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:11:01]
  **Train** Prec@1 99.944 Prec@5 99.998 Error@1 0.056
  **Test** Prec@1 94.340 Prec@5 99.800 Error@1 5.660

==>>[2018-05-03 20:11:39] [Epoch=328/500] [Need: 03:22:39] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [328][000/391]   Time 0.350 (0.350)   Data 0.163 (0.163)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:11:39]
  Epoch: [328][200/391]   Time 0.161 (0.159)   Data 0.000 (0.001)   Loss 0.0010 (0.0023)   Prec@1 100.000 (99.934)   Prec@5 100.000 (100.000)   [2018-05-03 20:12:11]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.340 Prec@5 99.800 Error@1 5.660

==>>[2018-05-03 20:12:48] [Epoch=329/500] [Need: 03:21:28] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [329][000/391]   Time 0.308 (0.308)   Data 0.138 (0.138)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:12:49]
  Epoch: [329][200/391]   Time 0.169 (0.158)   Data 0.001 (0.002)   Loss 0.0008 (0.0028)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 20:13:20]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.450 Prec@5 99.800 Error@1 5.550

==>>[2018-05-03 20:13:59] [Epoch=330/500] [Need: 03:20:17] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [330][000/391]   Time 0.327 (0.327)   Data 0.142 (0.142)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:14:00]
  Epoch: [330][200/391]   Time 0.140 (0.161)   Data 0.001 (0.001)   Loss 0.0006 (0.0025)   Prec@1 100.000 (99.949)   Prec@5 100.000 (100.000)   [2018-05-03 20:14:32]
  **Train** Prec@1 99.948 Prec@5 100.000 Error@1 0.052
  **Test** Prec@1 94.530 Prec@5 99.760 Error@1 5.470

==>>[2018-05-03 20:15:12] [Epoch=331/500] [Need: 03:19:08] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [331][000/391]   Time 0.357 (0.357)   Data 0.169 (0.169)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:15:13]
  Epoch: [331][200/391]   Time 0.149 (0.163)   Data 0.001 (0.002)   Loss 0.0004 (0.0020)   Prec@1 100.000 (99.949)   Prec@5 100.000 (100.000)   [2018-05-03 20:15:45]
  **Train** Prec@1 99.952 Prec@5 100.000 Error@1 0.048
  **Test** Prec@1 94.520 Prec@5 99.830 Error@1 5.480

==>>[2018-05-03 20:16:26] [Epoch=332/500] [Need: 03:17:58] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [332][000/391]   Time 0.341 (0.341)   Data 0.148 (0.148)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:16:26]
  Epoch: [332][200/391]   Time 0.188 (0.172)   Data 0.001 (0.002)   Loss 0.0015 (0.0024)   Prec@1 100.000 (99.934)   Prec@5 100.000 (100.000)   [2018-05-03 20:17:00]
  **Train** Prec@1 99.940 Prec@5 100.000 Error@1 0.060
  **Test** Prec@1 94.410 Prec@5 99.820 Error@1 5.590

==>>[2018-05-03 20:17:41] [Epoch=333/500] [Need: 03:16:50] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [333][000/391]   Time 0.365 (0.365)   Data 0.171 (0.171)   Loss 0.0169 (0.0169)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 20:17:41]
  Epoch: [333][200/391]   Time 0.173 (0.179)   Data 0.001 (0.002)   Loss 0.0005 (0.0030)   Prec@1 100.000 (99.918)   Prec@5 100.000 (100.000)   [2018-05-03 20:18:16]
  **Train** Prec@1 99.942 Prec@5 100.000 Error@1 0.058
  **Test** Prec@1 94.380 Prec@5 99.770 Error@1 5.620

==>>[2018-05-03 20:18:56] [Epoch=334/500] [Need: 03:15:42] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [334][000/391]   Time 0.316 (0.316)   Data 0.142 (0.142)   Loss 0.0015 (0.0015)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:18:57]
  Epoch: [334][200/391]   Time 0.133 (0.161)   Data 0.001 (0.001)   Loss 0.0003 (0.0022)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 20:19:29]
  **Train** Prec@1 99.934 Prec@5 100.000 Error@1 0.066
  **Test** Prec@1 94.250 Prec@5 99.740 Error@1 5.750

==>>[2018-05-03 20:20:09] [Epoch=335/500] [Need: 03:14:32] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [335][000/391]   Time 0.358 (0.358)   Data 0.164 (0.164)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:20:09]
  Epoch: [335][200/391]   Time 0.143 (0.156)   Data 0.001 (0.002)   Loss 0.0038 (0.0019)   Prec@1 100.000 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 20:20:40]
  **Train** Prec@1 99.932 Prec@5 100.000 Error@1 0.068
  **Test** Prec@1 94.280 Prec@5 99.750 Error@1 5.720

==>>[2018-05-03 20:21:19] [Epoch=336/500] [Need: 03:13:21] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [336][000/391]   Time 0.357 (0.357)   Data 0.166 (0.166)   Loss 0.0015 (0.0015)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:21:19]
  Epoch: [336][200/391]   Time 0.154 (0.162)   Data 0.001 (0.002)   Loss 0.0010 (0.0024)   Prec@1 100.000 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 20:21:51]
  **Train** Prec@1 99.942 Prec@5 100.000 Error@1 0.058
  **Test** Prec@1 94.430 Prec@5 99.770 Error@1 5.570

==>>[2018-05-03 20:22:30] [Epoch=337/500] [Need: 03:12:10] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [337][000/391]   Time 0.373 (0.373)   Data 0.197 (0.197)   Loss 0.0056 (0.0056)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:22:30]
  Epoch: [337][200/391]   Time 0.173 (0.157)   Data 0.001 (0.002)   Loss 0.0006 (0.0017)   Prec@1 100.000 (99.965)   Prec@5 100.000 (100.000)   [2018-05-03 20:23:01]
  **Train** Prec@1 99.954 Prec@5 100.000 Error@1 0.046
  **Test** Prec@1 94.490 Prec@5 99.810 Error@1 5.510

==>>[2018-05-03 20:23:39] [Epoch=338/500] [Need: 03:10:59] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [338][000/391]   Time 0.304 (0.304)   Data 0.123 (0.123)   Loss 0.0011 (0.0011)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:23:39]
  Epoch: [338][200/391]   Time 0.168 (0.156)   Data 0.001 (0.001)   Loss 0.0004 (0.0031)   Prec@1 100.000 (99.907)   Prec@5 100.000 (100.000)   [2018-05-03 20:24:10]
  **Train** Prec@1 99.928 Prec@5 100.000 Error@1 0.072
  **Test** Prec@1 94.290 Prec@5 99.760 Error@1 5.710

==>>[2018-05-03 20:24:48] [Epoch=339/500] [Need: 03:09:47] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [339][000/391]   Time 0.282 (0.282)   Data 0.126 (0.126)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:24:49]
  Epoch: [339][200/391]   Time 0.162 (0.155)   Data 0.001 (0.001)   Loss 0.0003 (0.0018)   Prec@1 100.000 (99.965)   Prec@5 100.000 (100.000)   [2018-05-03 20:25:19]
  **Train** Prec@1 99.964 Prec@5 100.000 Error@1 0.036
  **Test** Prec@1 94.370 Prec@5 99.770 Error@1 5.630

==>>[2018-05-03 20:25:58] [Epoch=340/500] [Need: 03:08:36] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [340][000/391]   Time 0.329 (0.329)   Data 0.145 (0.145)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:25:58]
  Epoch: [340][200/391]   Time 0.158 (0.158)   Data 0.001 (0.002)   Loss 0.0017 (0.0015)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:26:30]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.380 Prec@5 99.760 Error@1 5.620

==>>[2018-05-03 20:27:08] [Epoch=341/500] [Need: 03:07:25] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [341][000/391]   Time 0.339 (0.339)   Data 0.173 (0.173)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:27:08]
  Epoch: [341][200/391]   Time 0.153 (0.156)   Data 0.001 (0.002)   Loss 0.0068 (0.0011)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 20:27:39]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.480 Prec@5 99.740 Error@1 5.520

==>>[2018-05-03 20:28:17] [Epoch=342/500] [Need: 03:06:14] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [342][000/391]   Time 0.271 (0.271)   Data 0.118 (0.118)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:28:18]
  Epoch: [342][200/391]   Time 0.168 (0.158)   Data 0.001 (0.001)   Loss 0.0316 (0.0032)   Prec@1 99.219 (99.907)   Prec@5 100.000 (100.000)   [2018-05-03 20:28:49]
  **Train** Prec@1 99.898 Prec@5 100.000 Error@1 0.102
  **Test** Prec@1 94.010 Prec@5 99.770 Error@1 5.990

==>>[2018-05-03 20:29:28] [Epoch=343/500] [Need: 03:05:03] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [343][000/391]   Time 0.286 (0.286)   Data 0.123 (0.123)   Loss 0.0281 (0.0281)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 20:29:28]
  Epoch: [343][200/391]   Time 0.169 (0.161)   Data 0.000 (0.001)   Loss 0.0119 (0.0026)   Prec@1 99.219 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 20:30:01]
  **Train** Prec@1 99.950 Prec@5 100.000 Error@1 0.050
  **Test** Prec@1 94.310 Prec@5 99.800 Error@1 5.690

==>>[2018-05-03 20:30:39] [Epoch=344/500] [Need: 03:03:52] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [344][000/391]   Time 0.320 (0.320)   Data 0.126 (0.126)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:30:40]
  Epoch: [344][200/391]   Time 0.164 (0.160)   Data 0.001 (0.001)   Loss 0.0001 (0.0022)   Prec@1 100.000 (99.930)   Prec@5 100.000 (100.000)   [2018-05-03 20:31:11]
  **Train** Prec@1 99.952 Prec@5 100.000 Error@1 0.048
  **Test** Prec@1 94.410 Prec@5 99.830 Error@1 5.590

==>>[2018-05-03 20:31:49] [Epoch=345/500] [Need: 03:02:41] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [345][000/391]   Time 0.289 (0.289)   Data 0.123 (0.123)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:31:49]
  Epoch: [345][200/391]   Time 0.126 (0.156)   Data 0.000 (0.001)   Loss 0.0042 (0.0020)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 20:32:21]
  **Train** Prec@1 99.936 Prec@5 100.000 Error@1 0.064
  **Test** Prec@1 94.370 Prec@5 99.780 Error@1 5.630

==>>[2018-05-03 20:33:00] [Epoch=346/500] [Need: 03:01:31] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [346][000/391]   Time 0.313 (0.313)   Data 0.158 (0.158)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:33:00]
  Epoch: [346][200/391]   Time 0.160 (0.156)   Data 0.001 (0.002)   Loss 0.0030 (0.0020)   Prec@1 100.000 (99.965)   Prec@5 100.000 (100.000)   [2018-05-03 20:33:31]
  **Train** Prec@1 99.958 Prec@5 100.000 Error@1 0.042
  **Test** Prec@1 94.430 Prec@5 99.830 Error@1 5.570

==>>[2018-05-03 20:34:11] [Epoch=347/500] [Need: 03:00:20] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [347][000/391]   Time 0.305 (0.305)   Data 0.133 (0.133)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:34:11]
  Epoch: [347][200/391]   Time 0.148 (0.158)   Data 0.001 (0.002)   Loss 0.0008 (0.0019)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:34:42]
  **Train** Prec@1 99.936 Prec@5 100.000 Error@1 0.064
  **Test** Prec@1 94.430 Prec@5 99.780 Error@1 5.570

==>>[2018-05-03 20:35:20] [Epoch=348/500] [Need: 02:59:09] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [348][000/391]   Time 0.345 (0.345)   Data 0.162 (0.162)   Loss 0.0103 (0.0103)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 20:35:21]
  Epoch: [348][200/391]   Time 0.187 (0.156)   Data 0.001 (0.001)   Loss 0.0018 (0.0027)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 20:35:51]
  **Train** Prec@1 99.932 Prec@5 100.000 Error@1 0.068
  **Test** Prec@1 94.480 Prec@5 99.760 Error@1 5.520

==>>[2018-05-03 20:36:30] [Epoch=349/500] [Need: 02:57:57] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [349][000/391]   Time 0.354 (0.354)   Data 0.148 (0.148)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:36:30]
  Epoch: [349][200/391]   Time 0.159 (0.160)   Data 0.001 (0.002)   Loss 0.0006 (0.0035)   Prec@1 100.000 (99.903)   Prec@5 100.000 (100.000)   [2018-05-03 20:37:02]
  **Train** Prec@1 99.904 Prec@5 100.000 Error@1 0.096
  **Test** Prec@1 94.350 Prec@5 99.790 Error@1 5.650

==>>[2018-05-03 20:37:38] [Epoch=350/500] [Need: 02:56:46] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [350][000/391]   Time 0.336 (0.336)   Data 0.132 (0.132)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:37:39]
  Epoch: [350][200/391]   Time 0.156 (0.157)   Data 0.001 (0.002)   Loss 0.0030 (0.0027)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 20:38:10]
  **Train** Prec@1 99.918 Prec@5 100.000 Error@1 0.082
  **Test** Prec@1 94.250 Prec@5 99.830 Error@1 5.750

==>>[2018-05-03 20:38:48] [Epoch=351/500] [Need: 02:55:35] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [351][000/391]   Time 0.323 (0.323)   Data 0.130 (0.130)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:38:49]
  Epoch: [351][200/391]   Time 0.150 (0.158)   Data 0.001 (0.001)   Loss 0.0007 (0.0024)   Prec@1 100.000 (99.946)   Prec@5 100.000 (100.000)   [2018-05-03 20:39:20]
  **Train** Prec@1 99.936 Prec@5 100.000 Error@1 0.064
  **Test** Prec@1 94.320 Prec@5 99.830 Error@1 5.680

==>>[2018-05-03 20:39:58] [Epoch=352/500] [Need: 02:54:24] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [352][000/391]   Time 0.382 (0.382)   Data 0.166 (0.166)   Loss 0.0023 (0.0023)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:39:59]
  Epoch: [352][200/391]   Time 0.157 (0.160)   Data 0.001 (0.002)   Loss 0.0023 (0.0036)   Prec@1 100.000 (99.899)   Prec@5 100.000 (100.000)   [2018-05-03 20:40:31]
  **Train** Prec@1 99.916 Prec@5 100.000 Error@1 0.084
  **Test** Prec@1 94.510 Prec@5 99.780 Error@1 5.490

==>>[2018-05-03 20:41:08] [Epoch=353/500] [Need: 02:53:13] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [353][000/391]   Time 0.322 (0.322)   Data 0.143 (0.143)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:41:08]
  Epoch: [353][200/391]   Time 0.137 (0.153)   Data 0.000 (0.001)   Loss 0.0005 (0.0033)   Prec@1 100.000 (99.907)   Prec@5 100.000 (100.000)   [2018-05-03 20:41:39]
  **Train** Prec@1 99.904 Prec@5 100.000 Error@1 0.096
  **Test** Prec@1 94.210 Prec@5 99.790 Error@1 5.790

==>>[2018-05-03 20:42:16] [Epoch=354/500] [Need: 02:52:01] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [354][000/391]   Time 0.352 (0.352)   Data 0.148 (0.148)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:42:16]
  Epoch: [354][200/391]   Time 0.177 (0.155)   Data 0.001 (0.002)   Loss 0.0005 (0.0024)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:42:47]
  **Train** Prec@1 99.956 Prec@5 100.000 Error@1 0.044
  **Test** Prec@1 94.280 Prec@5 99.740 Error@1 5.720

==>>[2018-05-03 20:43:24] [Epoch=355/500] [Need: 02:50:49] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [355][000/391]   Time 0.340 (0.340)   Data 0.150 (0.150)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:43:25]
  Epoch: [355][200/391]   Time 0.163 (0.155)   Data 0.001 (0.001)   Loss 0.0020 (0.0022)   Prec@1 100.000 (99.942)   Prec@5 100.000 (100.000)   [2018-05-03 20:43:55]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 94.270 Prec@5 99.760 Error@1 5.730

==>>[2018-05-03 20:44:32] [Epoch=356/500] [Need: 02:49:37] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [356][000/391]   Time 0.321 (0.321)   Data 0.130 (0.130)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:44:33]
  Epoch: [356][200/391]   Time 0.154 (0.157)   Data 0.001 (0.002)   Loss 0.0038 (0.0028)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 20:45:04]
  **Train** Prec@1 99.936 Prec@5 100.000 Error@1 0.064
  **Test** Prec@1 94.090 Prec@5 99.710 Error@1 5.910

==>>[2018-05-03 20:45:42] [Epoch=357/500] [Need: 02:48:26] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [357][000/391]   Time 0.293 (0.293)   Data 0.129 (0.129)   Loss 0.0031 (0.0031)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:45:42]
  Epoch: [357][200/391]   Time 0.155 (0.155)   Data 0.001 (0.002)   Loss 0.0011 (0.0039)   Prec@1 100.000 (99.891)   Prec@5 100.000 (100.000)   [2018-05-03 20:46:13]
  **Train** Prec@1 99.914 Prec@5 100.000 Error@1 0.086
  **Test** Prec@1 94.100 Prec@5 99.810 Error@1 5.900

==>>[2018-05-03 20:46:50] [Epoch=358/500] [Need: 02:47:14] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [358][000/391]   Time 0.336 (0.336)   Data 0.156 (0.156)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:46:50]
  Epoch: [358][200/391]   Time 0.159 (0.157)   Data 0.001 (0.001)   Loss 0.0011 (0.0043)   Prec@1 100.000 (99.868)   Prec@5 100.000 (100.000)   [2018-05-03 20:47:21]
  **Train** Prec@1 99.880 Prec@5 100.000 Error@1 0.120
  **Test** Prec@1 94.380 Prec@5 99.790 Error@1 5.620

==>>[2018-05-03 20:47:59] [Epoch=359/500] [Need: 02:46:03] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [359][000/391]   Time 0.361 (0.361)   Data 0.162 (0.162)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:47:59]
  Epoch: [359][200/391]   Time 0.142 (0.156)   Data 0.001 (0.002)   Loss 0.0027 (0.0040)   Prec@1 100.000 (99.887)   Prec@5 100.000 (100.000)   [2018-05-03 20:48:30]
  **Train** Prec@1 99.900 Prec@5 100.000 Error@1 0.100
  **Test** Prec@1 93.950 Prec@5 99.770 Error@1 6.050

==>>[2018-05-03 20:49:08] [Epoch=360/500] [Need: 02:44:52] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [360][000/391]   Time 0.338 (0.338)   Data 0.144 (0.144)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:49:08]
  Epoch: [360][200/391]   Time 0.145 (0.155)   Data 0.001 (0.002)   Loss 0.0007 (0.0034)   Prec@1 100.000 (99.907)   Prec@5 100.000 (100.000)   [2018-05-03 20:49:39]
  **Train** Prec@1 99.898 Prec@5 100.000 Error@1 0.102
  **Test** Prec@1 94.030 Prec@5 99.750 Error@1 5.970

==>>[2018-05-03 20:50:17] [Epoch=361/500] [Need: 02:43:41] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [361][000/391]   Time 0.292 (0.292)   Data 0.126 (0.126)   Loss 0.0059 (0.0059)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:50:17]
  Epoch: [361][200/391]   Time 0.152 (0.157)   Data 0.001 (0.001)   Loss 0.0005 (0.0034)   Prec@1 100.000 (99.922)   Prec@5 100.000 (100.000)   [2018-05-03 20:50:49]
  **Train** Prec@1 99.922 Prec@5 100.000 Error@1 0.078
  **Test** Prec@1 94.020 Prec@5 99.760 Error@1 5.980

==>>[2018-05-03 20:51:27] [Epoch=362/500] [Need: 02:42:30] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [362][000/391]   Time 0.290 (0.290)   Data 0.131 (0.131)   Loss 0.0011 (0.0011)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:51:27]
  Epoch: [362][200/391]   Time 0.145 (0.158)   Data 0.001 (0.002)   Loss 0.0026 (0.0029)   Prec@1 100.000 (99.914)   Prec@5 100.000 (100.000)   [2018-05-03 20:51:59]
  **Train** Prec@1 99.918 Prec@5 100.000 Error@1 0.082
  **Test** Prec@1 94.170 Prec@5 99.760 Error@1 5.830

==>>[2018-05-03 20:52:36] [Epoch=363/500] [Need: 02:41:18] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [363][000/391]   Time 0.320 (0.320)   Data 0.131 (0.131)   Loss 0.0053 (0.0053)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:52:37]
  Epoch: [363][200/391]   Time 0.162 (0.158)   Data 0.001 (0.001)   Loss 0.0026 (0.0035)   Prec@1 100.000 (99.899)   Prec@5 100.000 (100.000)   [2018-05-03 20:53:08]
  **Train** Prec@1 99.906 Prec@5 100.000 Error@1 0.094
  **Test** Prec@1 94.520 Prec@5 99.730 Error@1 5.480

==>>[2018-05-03 20:53:46] [Epoch=364/500] [Need: 02:40:07] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [364][000/391]   Time 0.327 (0.327)   Data 0.155 (0.155)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:53:46]
  Epoch: [364][200/391]   Time 0.178 (0.161)   Data 0.001 (0.002)   Loss 0.0004 (0.0020)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 20:54:18]
  **Train** Prec@1 99.946 Prec@5 100.000 Error@1 0.054
  **Test** Prec@1 94.040 Prec@5 99.670 Error@1 5.960

==>>[2018-05-03 20:54:57] [Epoch=365/500] [Need: 02:38:57] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [365][000/391]   Time 0.388 (0.388)   Data 0.184 (0.184)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:54:57]
  Epoch: [365][200/391]   Time 0.170 (0.160)   Data 0.001 (0.002)   Loss 0.0046 (0.0041)   Prec@1 100.000 (99.880)   Prec@5 100.000 (100.000)   [2018-05-03 20:55:29]
  **Train** Prec@1 99.858 Prec@5 100.000 Error@1 0.142
  **Test** Prec@1 94.050 Prec@5 99.660 Error@1 5.950

==>>[2018-05-03 20:56:07] [Epoch=366/500] [Need: 02:37:46] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [366][000/391]   Time 0.291 (0.291)   Data 0.133 (0.133)   Loss 0.0041 (0.0041)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:56:07]
  Epoch: [366][200/391]   Time 0.143 (0.159)   Data 0.001 (0.002)   Loss 0.0004 (0.0030)   Prec@1 100.000 (99.918)   Prec@5 100.000 (100.000)   [2018-05-03 20:56:39]
  **Train** Prec@1 99.920 Prec@5 100.000 Error@1 0.080
  **Test** Prec@1 94.290 Prec@5 99.760 Error@1 5.710

==>>[2018-05-03 20:57:18] [Epoch=367/500] [Need: 02:36:36] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [367][000/391]   Time 0.350 (0.350)   Data 0.150 (0.150)   Loss 0.0123 (0.0123)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 20:57:19]
  Epoch: [367][200/391]   Time 0.166 (0.162)   Data 0.001 (0.002)   Loss 0.0011 (0.0026)   Prec@1 100.000 (99.938)   Prec@5 100.000 (100.000)   [2018-05-03 20:57:51]
  **Train** Prec@1 99.908 Prec@5 100.000 Error@1 0.092
  **Test** Prec@1 93.980 Prec@5 99.690 Error@1 6.020

==>>[2018-05-03 20:58:29] [Epoch=368/500] [Need: 02:35:25] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [368][000/391]   Time 0.314 (0.314)   Data 0.125 (0.125)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:58:29]
  Epoch: [368][200/391]   Time 0.160 (0.170)   Data 0.001 (0.001)   Loss 0.0018 (0.0029)   Prec@1 100.000 (99.926)   Prec@5 100.000 (100.000)   [2018-05-03 20:59:03]
  **Train** Prec@1 99.898 Prec@5 100.000 Error@1 0.102
  **Test** Prec@1 93.940 Prec@5 99.760 Error@1 6.060

==>>[2018-05-03 20:59:43] [Epoch=369/500] [Need: 02:34:15] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [369][000/391]   Time 0.362 (0.362)   Data 0.165 (0.165)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 20:59:43]
  Epoch: [369][200/391]   Time 0.157 (0.160)   Data 0.001 (0.002)   Loss 0.0091 (0.0035)   Prec@1 99.219 (99.914)   Prec@5 100.000 (100.000)   [2018-05-03 21:00:15]
  **Train** Prec@1 99.896 Prec@5 100.000 Error@1 0.104
  **Test** Prec@1 94.230 Prec@5 99.770 Error@1 5.770

==>>[2018-05-03 21:00:53] [Epoch=370/500] [Need: 02:33:05] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [370][000/391]   Time 0.358 (0.358)   Data 0.170 (0.170)   Loss 0.0263 (0.0263)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 21:00:53]
  Epoch: [370][200/391]   Time 0.156 (0.157)   Data 0.001 (0.001)   Loss 0.0003 (0.0030)   Prec@1 100.000 (99.903)   Prec@5 100.000 (100.000)   [2018-05-03 21:01:25]
  **Train** Prec@1 99.904 Prec@5 100.000 Error@1 0.096
  **Test** Prec@1 94.250 Prec@5 99.830 Error@1 5.750

==>>[2018-05-03 21:02:02] [Epoch=371/500] [Need: 02:31:54] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [371][000/391]   Time 0.331 (0.331)   Data 0.145 (0.145)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:02:03]
  Epoch: [371][200/391]   Time 0.157 (0.159)   Data 0.001 (0.002)   Loss 0.0062 (0.0062)   Prec@1 100.000 (99.817)   Prec@5 100.000 (100.000)   [2018-05-03 21:02:34]
  **Train** Prec@1 99.854 Prec@5 100.000 Error@1 0.146
  **Test** Prec@1 94.240 Prec@5 99.690 Error@1 5.760

==>>[2018-05-03 21:03:12] [Epoch=372/500] [Need: 02:30:43] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [372][000/391]   Time 0.304 (0.304)   Data 0.130 (0.130)   Loss 0.0064 (0.0064)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:03:13]
  Epoch: [372][200/391]   Time 0.127 (0.154)   Data 0.001 (0.001)   Loss 0.0007 (0.0033)   Prec@1 100.000 (99.891)   Prec@5 100.000 (100.000)   [2018-05-03 21:03:43]
  **Train** Prec@1 99.900 Prec@5 100.000 Error@1 0.100
  **Test** Prec@1 93.910 Prec@5 99.740 Error@1 6.090

==>>[2018-05-03 21:04:20] [Epoch=373/500] [Need: 02:29:31] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [373][000/391]   Time 0.371 (0.371)   Data 0.173 (0.173)   Loss 0.0293 (0.0293)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-05-03 21:04:20]
  Epoch: [373][200/391]   Time 0.169 (0.154)   Data 0.001 (0.001)   Loss 0.0049 (0.0054)   Prec@1 100.000 (99.837)   Prec@5 100.000 (100.000)   [2018-05-03 21:04:51]
  **Train** Prec@1 99.876 Prec@5 100.000 Error@1 0.124
  **Test** Prec@1 94.420 Prec@5 99.780 Error@1 5.580

==>>[2018-05-03 21:05:29] [Epoch=374/500] [Need: 02:28:20] [learning_rate=0.0100] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [374][000/391]   Time 0.291 (0.291)   Data 0.133 (0.133)   Loss 0.0137 (0.0137)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 21:05:29]
  Epoch: [374][200/391]   Time 0.174 (0.157)   Data 0.001 (0.002)   Loss 0.0052 (0.0036)   Prec@1 100.000 (99.891)   Prec@5 100.000 (100.000)   [2018-05-03 21:06:00]
  **Train** Prec@1 99.892 Prec@5 100.000 Error@1 0.108
  **Test** Prec@1 94.120 Prec@5 99.770 Error@1 5.880

==>>[2018-05-03 21:06:38] [Epoch=375/500] [Need: 02:27:09] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [375][000/391]   Time 0.325 (0.325)   Data 0.122 (0.122)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:06:38]
  Epoch: [375][200/391]   Time 0.143 (0.158)   Data 0.001 (0.001)   Loss 0.0019 (0.0035)   Prec@1 100.000 (99.911)   Prec@5 100.000 (100.000)   [2018-05-03 21:07:10]
  **Train** Prec@1 99.912 Prec@5 100.000 Error@1 0.088
  **Test** Prec@1 94.280 Prec@5 99.760 Error@1 5.720

==>>[2018-05-03 21:07:48] [Epoch=376/500] [Need: 02:25:58] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [376][000/391]   Time 0.270 (0.270)   Data 0.128 (0.128)   Loss 0.0034 (0.0034)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:07:48]
  Epoch: [376][200/391]   Time 0.141 (0.159)   Data 0.001 (0.001)   Loss 0.0005 (0.0020)   Prec@1 100.000 (99.961)   Prec@5 100.000 (100.000)   [2018-05-03 21:08:20]
  **Train** Prec@1 99.952 Prec@5 100.000 Error@1 0.048
  **Test** Prec@1 94.390 Prec@5 99.760 Error@1 5.610

==>>[2018-05-03 21:08:58] [Epoch=377/500] [Need: 02:24:47] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [377][000/391]   Time 0.345 (0.345)   Data 0.145 (0.145)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:08:58]
  Epoch: [377][200/391]   Time 0.170 (0.157)   Data 0.001 (0.002)   Loss 0.0012 (0.0021)   Prec@1 100.000 (99.953)   Prec@5 100.000 (100.000)   [2018-05-03 21:09:29]
  **Train** Prec@1 99.960 Prec@5 100.000 Error@1 0.040
  **Test** Prec@1 94.460 Prec@5 99.740 Error@1 5.540

==>>[2018-05-03 21:10:07] [Epoch=378/500] [Need: 02:23:36] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [378][000/391]   Time 0.342 (0.342)   Data 0.177 (0.177)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:10:08]
  Epoch: [378][200/391]   Time 0.155 (0.161)   Data 0.000 (0.002)   Loss 0.0004 (0.0021)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 21:10:40]
  **Train** Prec@1 99.962 Prec@5 100.000 Error@1 0.038
  **Test** Prec@1 94.370 Prec@5 99.740 Error@1 5.630

==>>[2018-05-03 21:11:18] [Epoch=379/500] [Need: 02:22:25] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [379][000/391]   Time 0.295 (0.295)   Data 0.131 (0.131)   Loss 0.0032 (0.0032)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:11:18]
  Epoch: [379][200/391]   Time 0.173 (0.155)   Data 0.001 (0.001)   Loss 0.0054 (0.0017)   Prec@1 100.000 (99.969)   Prec@5 100.000 (100.000)   [2018-05-03 21:11:49]
  **Train** Prec@1 99.966 Prec@5 100.000 Error@1 0.034
  **Test** Prec@1 94.550 Prec@5 99.780 Error@1 5.450

==>>[2018-05-03 21:12:27] [Epoch=380/500] [Need: 02:21:14] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [380][000/391]   Time 0.304 (0.304)   Data 0.121 (0.121)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:12:28]
  Epoch: [380][200/391]   Time 0.140 (0.155)   Data 0.001 (0.001)   Loss 0.0003 (0.0014)   Prec@1 100.000 (99.973)   Prec@5 100.000 (100.000)   [2018-05-03 21:12:58]
  **Train** Prec@1 99.974 Prec@5 100.000 Error@1 0.026
  **Test** Prec@1 94.420 Prec@5 99.790 Error@1 5.580

==>>[2018-05-03 21:13:35] [Epoch=381/500] [Need: 02:20:03] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [381][000/391]   Time 0.362 (0.362)   Data 0.177 (0.177)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:13:35]
  Epoch: [381][200/391]   Time 0.150 (0.157)   Data 0.001 (0.002)   Loss 0.0041 (0.0014)   Prec@1 100.000 (99.969)   Prec@5 100.000 (100.000)   [2018-05-03 21:14:07]
  **Train** Prec@1 99.966 Prec@5 100.000 Error@1 0.034
  **Test** Prec@1 94.460 Prec@5 99.770 Error@1 5.540

==>>[2018-05-03 21:14:44] [Epoch=382/500] [Need: 02:18:52] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [382][000/391]   Time 0.288 (0.288)   Data 0.123 (0.123)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:14:44]
  Epoch: [382][200/391]   Time 0.170 (0.156)   Data 0.001 (0.001)   Loss 0.0013 (0.0011)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:15:15]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.450 Prec@5 99.760 Error@1 5.550

==>>[2018-05-03 21:15:53] [Epoch=383/500] [Need: 02:17:41] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [383][000/391]   Time 0.322 (0.322)   Data 0.131 (0.131)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:15:53]
  Epoch: [383][200/391]   Time 0.147 (0.155)   Data 0.000 (0.001)   Loss 0.0006 (0.0010)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:16:24]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.560 Prec@5 99.790 Error@1 5.440

==>>[2018-05-03 21:17:02] [Epoch=384/500] [Need: 02:16:29] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [384][000/391]   Time 0.314 (0.314)   Data 0.134 (0.134)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:17:02]
  Epoch: [384][200/391]   Time 0.163 (0.155)   Data 0.001 (0.001)   Loss 0.0068 (0.0012)   Prec@1 100.000 (99.977)   Prec@5 100.000 (100.000)   [2018-05-03 21:17:33]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.380 Prec@5 99.760 Error@1 5.620

==>>[2018-05-03 21:18:11] [Epoch=385/500] [Need: 02:15:18] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [385][000/391]   Time 0.362 (0.362)   Data 0.171 (0.171)   Loss 0.0064 (0.0064)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:18:12]
  Epoch: [385][200/391]   Time 0.127 (0.159)   Data 0.000 (0.002)   Loss 0.0004 (0.0011)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:18:43]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.560 Prec@5 99.760 Error@1 5.440

==>>[2018-05-03 21:19:20] [Epoch=386/500] [Need: 02:14:07] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [386][000/391]   Time 0.346 (0.346)   Data 0.167 (0.167)   Loss 0.0007 (0.0007)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:19:20]
  Epoch: [386][200/391]   Time 0.143 (0.159)   Data 0.001 (0.002)   Loss 0.0004 (0.0012)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:19:52]
  **Train** Prec@1 99.982 Prec@5 100.000 Error@1 0.018
  **Test** Prec@1 94.540 Prec@5 99.740 Error@1 5.460

==>>[2018-05-03 21:20:30] [Epoch=387/500] [Need: 02:12:56] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [387][000/391]   Time 0.320 (0.320)   Data 0.133 (0.133)   Loss 0.0024 (0.0024)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:20:30]
  Epoch: [387][200/391]   Time 0.167 (0.160)   Data 0.001 (0.002)   Loss 0.0006 (0.0011)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:21:02]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.540 Prec@5 99.750 Error@1 5.460

==>>[2018-05-03 21:21:39] [Epoch=388/500] [Need: 02:11:45] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [388][000/391]   Time 0.337 (0.337)   Data 0.146 (0.146)   Loss 0.0024 (0.0024)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:21:39]
  Epoch: [388][200/391]   Time 0.166 (0.162)   Data 0.001 (0.001)   Loss 0.0003 (0.0009)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:22:11]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.620 Prec@5 99.780 Error@1 5.380

==>>[2018-05-03 21:22:49] [Epoch=389/500] [Need: 02:10:35] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [389][000/391]   Time 0.349 (0.349)   Data 0.149 (0.149)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:22:49]
  Epoch: [389][200/391]   Time 0.144 (0.157)   Data 0.001 (0.002)   Loss 0.0004 (0.0013)   Prec@1 100.000 (99.973)   Prec@5 100.000 (100.000)   [2018-05-03 21:23:20]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.620 Prec@5 99.790 Error@1 5.380

==>>[2018-05-03 21:23:58] [Epoch=390/500] [Need: 02:09:24] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [390][000/391]   Time 0.297 (0.297)   Data 0.121 (0.121)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:23:59]
  Epoch: [390][200/391]   Time 0.139 (0.159)   Data 0.000 (0.001)   Loss 0.0002 (0.0010)   Prec@1 100.000 (99.981)   Prec@5 100.000 (100.000)   [2018-05-03 21:24:30]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.590 Prec@5 99.780 Error@1 5.410

==>>[2018-05-03 21:25:09] [Epoch=391/500] [Need: 02:08:13] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [391][000/391]   Time 0.343 (0.343)   Data 0.135 (0.135)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:25:09]
  Epoch: [391][200/391]   Time 0.150 (0.158)   Data 0.001 (0.002)   Loss 0.0008 (0.0011)   Prec@1 100.000 (99.973)   Prec@5 100.000 (100.000)   [2018-05-03 21:25:41]
  **Train** Prec@1 99.974 Prec@5 100.000 Error@1 0.026
  **Test** Prec@1 94.630 Prec@5 99.780 Error@1 5.370

==>>[2018-05-03 21:26:19] [Epoch=392/500] [Need: 02:07:03] [learning_rate=0.0010] [Best : Accuracy=94.65, Error=5.35]
  Epoch: [392][000/391]   Time 0.337 (0.337)   Data 0.145 (0.145)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:26:19]
  Epoch: [392][200/391]   Time 0.154 (0.157)   Data 0.001 (0.002)   Loss 0.0001 (0.0011)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:26:51]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.660 Prec@5 99.790 Error@1 5.340

==>>[2018-05-03 21:27:28] [Epoch=393/500] [Need: 02:05:51] [learning_rate=0.0010] [Best : Accuracy=94.66, Error=5.34]
  Epoch: [393][000/391]   Time 0.351 (0.351)   Data 0.155 (0.155)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:27:28]
  Epoch: [393][200/391]   Time 0.142 (0.157)   Data 0.001 (0.001)   Loss 0.0003 (0.0010)   Prec@1 100.000 (99.977)   Prec@5 100.000 (100.000)   [2018-05-03 21:28:00]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.700 Prec@5 99.810 Error@1 5.300

==>>[2018-05-03 21:28:38] [Epoch=394/500] [Need: 02:04:41] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [394][000/391]   Time 0.340 (0.340)   Data 0.154 (0.154)   Loss 0.0109 (0.0109)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 21:28:38]
  Epoch: [394][200/391]   Time 0.180 (0.156)   Data 0.001 (0.002)   Loss 0.0004 (0.0009)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:29:09]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.620 Prec@5 99.800 Error@1 5.380

==>>[2018-05-03 21:29:47] [Epoch=395/500] [Need: 02:03:30] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [395][000/391]   Time 0.299 (0.299)   Data 0.137 (0.137)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:29:47]
  Epoch: [395][200/391]   Time 0.161 (0.159)   Data 0.001 (0.002)   Loss 0.0003 (0.0011)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:30:19]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.590 Prec@5 99.780 Error@1 5.410

==>>[2018-05-03 21:30:57] [Epoch=396/500] [Need: 02:02:19] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [396][000/391]   Time 0.327 (0.327)   Data 0.141 (0.141)   Loss 0.0064 (0.0064)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-05-03 21:30:58]
  Epoch: [396][200/391]   Time 0.164 (0.158)   Data 0.001 (0.002)   Loss 0.0005 (0.0012)   Prec@1 100.000 (99.957)   Prec@5 100.000 (100.000)   [2018-05-03 21:31:29]
  **Train** Prec@1 99.966 Prec@5 100.000 Error@1 0.034
  **Test** Prec@1 94.570 Prec@5 99.780 Error@1 5.430

==>>[2018-05-03 21:32:06] [Epoch=397/500] [Need: 02:01:08] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [397][000/391]   Time 0.338 (0.338)   Data 0.163 (0.163)   Loss 0.0012 (0.0012)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:32:07]
  Epoch: [397][200/391]   Time 0.177 (0.156)   Data 0.001 (0.002)   Loss 0.0006 (0.0009)   Prec@1 100.000 (99.977)   Prec@5 100.000 (100.000)   [2018-05-03 21:32:38]
  **Train** Prec@1 99.980 Prec@5 100.000 Error@1 0.020
  **Test** Prec@1 94.600 Prec@5 99.760 Error@1 5.400

==>>[2018-05-03 21:33:15] [Epoch=398/500] [Need: 01:59:57] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [398][000/391]   Time 0.364 (0.364)   Data 0.153 (0.153)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:33:15]
  Epoch: [398][200/391]   Time 0.147 (0.156)   Data 0.001 (0.001)   Loss 0.0003 (0.0011)   Prec@1 100.000 (99.981)   Prec@5 100.000 (100.000)   [2018-05-03 21:33:46]
  **Train** Prec@1 99.982 Prec@5 100.000 Error@1 0.018
  **Test** Prec@1 94.610 Prec@5 99.790 Error@1 5.390

==>>[2018-05-03 21:34:25] [Epoch=399/500] [Need: 01:58:46] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [399][000/391]   Time 0.321 (0.321)   Data 0.123 (0.123)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:34:26]
  Epoch: [399][200/391]   Time 0.154 (0.159)   Data 0.001 (0.001)   Loss 0.0006 (0.0009)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:34:57]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.530 Prec@5 99.760 Error@1 5.470

==>>[2018-05-03 21:35:36] [Epoch=400/500] [Need: 01:57:36] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [400][000/391]   Time 0.312 (0.312)   Data 0.127 (0.127)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:35:36]
  Epoch: [400][200/391]   Time 0.170 (0.163)   Data 0.001 (0.001)   Loss 0.0018 (0.0007)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:36:09]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.660 Prec@5 99.800 Error@1 5.340

==>>[2018-05-03 21:36:47] [Epoch=401/500] [Need: 01:56:25] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [401][000/391]   Time 0.340 (0.340)   Data 0.135 (0.135)   Loss 0.0011 (0.0011)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:36:47]
  Epoch: [401][200/391]   Time 0.152 (0.156)   Data 0.001 (0.002)   Loss 0.0104 (0.0009)   Prec@1 99.219 (99.977)   Prec@5 100.000 (100.000)   [2018-05-03 21:37:18]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.630 Prec@5 99.800 Error@1 5.370

==>>[2018-05-03 21:37:55] [Epoch=402/500] [Need: 01:55:14] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [402][000/391]   Time 0.333 (0.333)   Data 0.170 (0.170)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:37:55]
  Epoch: [402][200/391]   Time 0.187 (0.160)   Data 0.001 (0.002)   Loss 0.0004 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:38:27]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.680 Prec@5 99.800 Error@1 5.320

==>>[2018-05-03 21:39:06] [Epoch=403/500] [Need: 01:54:04] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [403][000/391]   Time 0.360 (0.360)   Data 0.160 (0.160)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:39:06]
  Epoch: [403][200/391]   Time 0.155 (0.164)   Data 0.001 (0.002)   Loss 0.0004 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:39:39]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.570 Prec@5 99.780 Error@1 5.430

==>>[2018-05-03 21:40:18] [Epoch=404/500] [Need: 01:52:54] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [404][000/391]   Time 0.296 (0.296)   Data 0.123 (0.123)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:40:18]
  Epoch: [404][200/391]   Time 0.166 (0.160)   Data 0.001 (0.002)   Loss 0.0007 (0.0008)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:40:50]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.540 Prec@5 99.770 Error@1 5.460

==>>[2018-05-03 21:41:28] [Epoch=405/500] [Need: 01:51:43] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [405][000/391]   Time 0.351 (0.351)   Data 0.154 (0.154)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:41:28]
  Epoch: [405][200/391]   Time 0.175 (0.157)   Data 0.001 (0.002)   Loss 0.0036 (0.0009)   Prec@1 100.000 (99.977)   Prec@5 100.000 (100.000)   [2018-05-03 21:42:00]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.620 Prec@5 99.800 Error@1 5.380

==>>[2018-05-03 21:42:38] [Epoch=406/500] [Need: 01:50:32] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [406][000/391]   Time 0.314 (0.314)   Data 0.154 (0.154)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:42:38]
  Epoch: [406][200/391]   Time 0.142 (0.156)   Data 0.001 (0.002)   Loss 0.0018 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:43:10]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.650 Prec@5 99.770 Error@1 5.350

==>>[2018-05-03 21:43:47] [Epoch=407/500] [Need: 01:49:21] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [407][000/391]   Time 0.294 (0.294)   Data 0.132 (0.132)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:43:48]
  Epoch: [407][200/391]   Time 0.150 (0.157)   Data 0.001 (0.002)   Loss 0.0002 (0.0008)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:44:19]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.600 Prec@5 99.780 Error@1 5.400

==>>[2018-05-03 21:44:57] [Epoch=408/500] [Need: 01:48:11] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [408][000/391]   Time 0.323 (0.323)   Data 0.134 (0.134)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:44:57]
  Epoch: [408][200/391]   Time 0.152 (0.155)   Data 0.001 (0.001)   Loss 0.0005 (0.0010)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:45:28]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.570 Prec@5 99.790 Error@1 5.430

==>>[2018-05-03 21:46:06] [Epoch=409/500] [Need: 01:47:00] [learning_rate=0.0010] [Best : Accuracy=94.70, Error=5.30]
  Epoch: [409][000/391]   Time 0.384 (0.384)   Data 0.186 (0.186)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:46:07]
  Epoch: [409][200/391]   Time 0.169 (0.158)   Data 0.001 (0.002)   Loss 0.0023 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:46:38]
  **Train** Prec@1 99.976 Prec@5 100.000 Error@1 0.024
  **Test** Prec@1 94.710 Prec@5 99.800 Error@1 5.290

==>>[2018-05-03 21:47:16] [Epoch=410/500] [Need: 01:45:49] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [410][000/391]   Time 0.373 (0.373)   Data 0.179 (0.179)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:47:16]
  Epoch: [410][200/391]   Time 0.159 (0.155)   Data 0.000 (0.002)   Loss 0.0004 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 21:47:47]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.600 Prec@5 99.800 Error@1 5.400

==>>[2018-05-03 21:48:24] [Epoch=411/500] [Need: 01:44:38] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [411][000/391]   Time 0.312 (0.312)   Data 0.127 (0.127)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:48:24]
  Epoch: [411][200/391]   Time 0.157 (0.159)   Data 0.001 (0.001)   Loss 0.0017 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:48:56]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.600 Prec@5 99.800 Error@1 5.400

==>>[2018-05-03 21:49:34] [Epoch=412/500] [Need: 01:43:27] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [412][000/391]   Time 0.328 (0.328)   Data 0.134 (0.134)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:49:34]
  Epoch: [412][200/391]   Time 0.178 (0.158)   Data 0.001 (0.002)   Loss 0.0002 (0.0008)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:50:06]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.670 Prec@5 99.780 Error@1 5.330

==>>[2018-05-03 21:50:43] [Epoch=413/500] [Need: 01:42:16] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [413][000/391]   Time 0.341 (0.341)   Data 0.142 (0.142)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:50:44]
  Epoch: [413][200/391]   Time 0.148 (0.155)   Data 0.001 (0.001)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:51:14]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.590 Prec@5 99.810 Error@1 5.410

==>>[2018-05-03 21:51:52] [Epoch=414/500] [Need: 01:41:06] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [414][000/391]   Time 0.315 (0.315)   Data 0.139 (0.139)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:51:52]
  Epoch: [414][200/391]   Time 0.157 (0.158)   Data 0.001 (0.002)   Loss 0.0003 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:52:24]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.620 Prec@5 99.790 Error@1 5.380

==>>[2018-05-03 21:53:02] [Epoch=415/500] [Need: 01:39:55] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [415][000/391]   Time 0.324 (0.324)   Data 0.148 (0.148)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:53:02]
  Epoch: [415][200/391]   Time 0.143 (0.156)   Data 0.001 (0.002)   Loss 0.0006 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:53:33]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.600 Prec@5 99.790 Error@1 5.400

==>>[2018-05-03 21:54:11] [Epoch=416/500] [Need: 01:38:44] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [416][000/391]   Time 0.308 (0.308)   Data 0.123 (0.123)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:54:11]
  Epoch: [416][200/391]   Time 0.177 (0.158)   Data 0.001 (0.001)   Loss 0.0003 (0.0006)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:54:42]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.540 Prec@5 99.790 Error@1 5.460

==>>[2018-05-03 21:55:20] [Epoch=417/500] [Need: 01:37:33] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [417][000/391]   Time 0.333 (0.333)   Data 0.149 (0.149)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:55:20]
  Epoch: [417][200/391]   Time 0.153 (0.159)   Data 0.001 (0.002)   Loss 0.0007 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:55:52]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.580 Prec@5 99.780 Error@1 5.420

==>>[2018-05-03 21:56:29] [Epoch=418/500] [Need: 01:36:22] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [418][000/391]   Time 0.358 (0.358)   Data 0.157 (0.157)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:56:30]
  Epoch: [418][200/391]   Time 0.163 (0.156)   Data 0.001 (0.001)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 21:57:01]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.580 Prec@5 99.770 Error@1 5.420

==>>[2018-05-03 21:57:39] [Epoch=419/500] [Need: 01:35:12] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [419][000/391]   Time 0.319 (0.319)   Data 0.126 (0.126)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:57:39]
  Epoch: [419][200/391]   Time 0.139 (0.157)   Data 0.001 (0.001)   Loss 0.0007 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 21:58:10]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.670 Prec@5 99.780 Error@1 5.330

==>>[2018-05-03 21:58:48] [Epoch=420/500] [Need: 01:34:01] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [420][000/391]   Time 0.323 (0.323)   Data 0.137 (0.137)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:58:48]
  Epoch: [420][200/391]   Time 0.156 (0.160)   Data 0.001 (0.002)   Loss 0.0017 (0.0007)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 21:59:20]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.590 Prec@5 99.740 Error@1 5.410

==>>[2018-05-03 21:59:58] [Epoch=421/500] [Need: 01:32:50] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [421][000/391]   Time 0.275 (0.275)   Data 0.121 (0.121)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 21:59:58]
  Epoch: [421][200/391]   Time 0.173 (0.157)   Data 0.001 (0.001)   Loss 0.0002 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:00:29]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.620 Prec@5 99.790 Error@1 5.380

==>>[2018-05-03 22:01:06] [Epoch=422/500] [Need: 01:31:39] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [422][000/391]   Time 0.315 (0.315)   Data 0.142 (0.142)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:01:07]
  Epoch: [422][200/391]   Time 0.168 (0.155)   Data 0.001 (0.002)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:01:37]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.580 Prec@5 99.800 Error@1 5.420

==>>[2018-05-03 22:02:15] [Epoch=423/500] [Need: 01:30:29] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [423][000/391]   Time 0.283 (0.283)   Data 0.126 (0.126)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:02:15]
  Epoch: [423][200/391]   Time 0.173 (0.159)   Data 0.001 (0.001)   Loss 0.0003 (0.0010)   Prec@1 100.000 (99.981)   Prec@5 100.000 (100.000)   [2018-05-03 22:02:47]
  **Train** Prec@1 99.984 Prec@5 100.000 Error@1 0.016
  **Test** Prec@1 94.550 Prec@5 99.780 Error@1 5.450

==>>[2018-05-03 22:03:24] [Epoch=424/500] [Need: 01:29:18] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [424][000/391]   Time 0.333 (0.333)   Data 0.143 (0.143)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:03:25]
  Epoch: [424][200/391]   Time 0.154 (0.157)   Data 0.001 (0.002)   Loss 0.0005 (0.0008)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:03:56]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.630 Prec@5 99.790 Error@1 5.370

==>>[2018-05-03 22:04:35] [Epoch=425/500] [Need: 01:28:07] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [425][000/391]   Time 0.314 (0.314)   Data 0.143 (0.143)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:04:35]
  Epoch: [425][200/391]   Time 0.174 (0.158)   Data 0.001 (0.002)   Loss 0.0008 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 22:05:06]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.620 Prec@5 99.800 Error@1 5.380

==>>[2018-05-03 22:05:44] [Epoch=426/500] [Need: 01:26:57] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [426][000/391]   Time 0.361 (0.361)   Data 0.152 (0.152)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:05:45]
  Epoch: [426][200/391]   Time 0.148 (0.160)   Data 0.001 (0.002)   Loss 0.0002 (0.0006)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:06:17]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.610 Prec@5 99.790 Error@1 5.390

==>>[2018-05-03 22:06:54] [Epoch=427/500] [Need: 01:25:46] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [427][000/391]   Time 0.328 (0.328)   Data 0.127 (0.127)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:06:55]
  Epoch: [427][200/391]   Time 0.178 (0.157)   Data 0.001 (0.001)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:07:26]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.520 Prec@5 99.780 Error@1 5.480

==>>[2018-05-03 22:08:03] [Epoch=428/500] [Need: 01:24:35] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [428][000/391]   Time 0.323 (0.323)   Data 0.151 (0.151)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:08:04]
  Epoch: [428][200/391]   Time 0.147 (0.155)   Data 0.000 (0.001)   Loss 0.0008 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:08:35]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.600 Prec@5 99.760 Error@1 5.400

==>>[2018-05-03 22:09:13] [Epoch=429/500] [Need: 01:23:25] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [429][000/391]   Time 0.301 (0.301)   Data 0.143 (0.143)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:09:13]
  Epoch: [429][200/391]   Time 0.183 (0.157)   Data 0.001 (0.002)   Loss 0.0003 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:09:44]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.610 Prec@5 99.800 Error@1 5.390

==>>[2018-05-03 22:10:23] [Epoch=430/500] [Need: 01:22:14] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [430][000/391]   Time 0.291 (0.291)   Data 0.124 (0.124)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:10:23]
  Epoch: [430][200/391]   Time 0.154 (0.157)   Data 0.001 (0.001)   Loss 0.0009 (0.0007)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 22:10:54]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.580 Prec@5 99.790 Error@1 5.420

==>>[2018-05-03 22:11:31] [Epoch=431/500] [Need: 01:21:03] [learning_rate=0.0010] [Best : Accuracy=94.71, Error=5.29]
  Epoch: [431][000/391]   Time 0.293 (0.293)   Data 0.123 (0.123)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:11:31]
  Epoch: [431][200/391]   Time 0.162 (0.157)   Data 0.001 (0.001)   Loss 0.0016 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:12:03]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.750 Prec@5 99.760 Error@1 5.250

==>>[2018-05-03 22:12:41] [Epoch=432/500] [Need: 01:19:53] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [432][000/391]   Time 0.342 (0.342)   Data 0.156 (0.156)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:12:41]
  Epoch: [432][200/391]   Time 0.137 (0.157)   Data 0.001 (0.002)   Loss 0.0004 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:13:13]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.700 Prec@5 99.770 Error@1 5.300

==>>[2018-05-03 22:13:50] [Epoch=433/500] [Need: 01:18:42] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [433][000/391]   Time 0.340 (0.340)   Data 0.148 (0.148)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:13:50]
  Epoch: [433][200/391]   Time 0.156 (0.158)   Data 0.001 (0.001)   Loss 0.0007 (0.0006)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:14:21]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.620 Prec@5 99.760 Error@1 5.380

==>>[2018-05-03 22:14:58] [Epoch=434/500] [Need: 01:17:31] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [434][000/391]   Time 0.325 (0.325)   Data 0.149 (0.149)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:14:59]
  Epoch: [434][200/391]   Time 0.161 (0.157)   Data 0.001 (0.002)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:15:30]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.600 Prec@5 99.780 Error@1 5.400

==>>[2018-05-03 22:16:08] [Epoch=435/500] [Need: 01:16:21] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [435][000/391]   Time 0.342 (0.342)   Data 0.145 (0.145)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:16:08]
  Epoch: [435][200/391]   Time 0.160 (0.158)   Data 0.001 (0.001)   Loss 0.0010 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:16:40]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.430 Prec@5 99.790 Error@1 5.570

==>>[2018-05-03 22:17:17] [Epoch=436/500] [Need: 01:15:10] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [436][000/391]   Time 0.340 (0.340)   Data 0.124 (0.124)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:17:17]
  Epoch: [436][200/391]   Time 0.145 (0.158)   Data 0.001 (0.001)   Loss 0.0004 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:17:49]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.660 Prec@5 99.800 Error@1 5.340

==>>[2018-05-03 22:18:27] [Epoch=437/500] [Need: 01:13:59] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [437][000/391]   Time 0.382 (0.382)   Data 0.165 (0.165)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:18:27]
  Epoch: [437][200/391]   Time 0.151 (0.161)   Data 0.001 (0.002)   Loss 0.0005 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 22:18:59]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.630 Prec@5 99.780 Error@1 5.370

==>>[2018-05-03 22:19:38] [Epoch=438/500] [Need: 01:12:49] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [438][000/391]   Time 0.342 (0.342)   Data 0.147 (0.147)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:19:38]
  Epoch: [438][200/391]   Time 0.155 (0.162)   Data 0.000 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:20:10]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.640 Prec@5 99.770 Error@1 5.360

==>>[2018-05-03 22:20:48] [Epoch=439/500] [Need: 01:11:38] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [439][000/391]   Time 0.307 (0.307)   Data 0.143 (0.143)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:20:49]
  Epoch: [439][200/391]   Time 0.151 (0.153)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:21:19]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.670 Prec@5 99.790 Error@1 5.330

==>>[2018-05-03 22:21:56] [Epoch=440/500] [Need: 01:10:28] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [440][000/391]   Time 0.329 (0.329)   Data 0.128 (0.128)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:21:56]
  Epoch: [440][200/391]   Time 0.149 (0.158)   Data 0.001 (0.001)   Loss 0.0003 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:22:28]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.700 Prec@5 99.780 Error@1 5.300

==>>[2018-05-03 22:23:06] [Epoch=441/500] [Need: 01:09:17] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [441][000/391]   Time 0.330 (0.330)   Data 0.154 (0.154)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:23:06]
  Epoch: [441][200/391]   Time 0.138 (0.159)   Data 0.001 (0.002)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:23:38]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.600 Prec@5 99.790 Error@1 5.400

==>>[2018-05-03 22:24:16] [Epoch=442/500] [Need: 01:08:07] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [442][000/391]   Time 0.353 (0.353)   Data 0.148 (0.148)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:24:17]
  Epoch: [442][200/391]   Time 0.158 (0.162)   Data 0.001 (0.002)   Loss 0.0003 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:24:49]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.660 Prec@5 99.760 Error@1 5.340

==>>[2018-05-03 22:25:27] [Epoch=443/500] [Need: 01:06:56] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [443][000/391]   Time 0.348 (0.348)   Data 0.144 (0.144)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:25:28]
  Epoch: [443][200/391]   Time 0.142 (0.155)   Data 0.001 (0.001)   Loss 0.0006 (0.0007)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:25:59]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.690 Prec@5 99.790 Error@1 5.310

==>>[2018-05-03 22:26:37] [Epoch=444/500] [Need: 01:05:46] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [444][000/391]   Time 0.314 (0.314)   Data 0.127 (0.127)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:26:37]
  Epoch: [444][200/391]   Time 0.151 (0.155)   Data 0.001 (0.001)   Loss 0.0002 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:27:08]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.700 Prec@5 99.780 Error@1 5.300

==>>[2018-05-03 22:27:46] [Epoch=445/500] [Need: 01:04:35] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [445][000/391]   Time 0.312 (0.312)   Data 0.138 (0.138)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:27:47]
  Epoch: [445][200/391]   Time 0.171 (0.158)   Data 0.001 (0.002)   Loss 0.0019 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:28:18]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.640 Prec@5 99.800 Error@1 5.360

==>>[2018-05-03 22:28:57] [Epoch=446/500] [Need: 01:03:24] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [446][000/391]   Time 0.306 (0.306)   Data 0.154 (0.154)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:28:57]
  Epoch: [446][200/391]   Time 0.179 (0.158)   Data 0.001 (0.002)   Loss 0.0004 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:29:28]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.720 Prec@5 99.790 Error@1 5.280

==>>[2018-05-03 22:30:07] [Epoch=447/500] [Need: 01:02:14] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [447][000/391]   Time 0.325 (0.325)   Data 0.125 (0.125)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:30:07]
  Epoch: [447][200/391]   Time 0.177 (0.160)   Data 0.001 (0.001)   Loss 0.0006 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:30:39]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.680 Prec@5 99.760 Error@1 5.320

==>>[2018-05-03 22:31:17] [Epoch=448/500] [Need: 01:01:03] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [448][000/391]   Time 0.311 (0.311)   Data 0.128 (0.128)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:31:17]
  Epoch: [448][200/391]   Time 0.159 (0.157)   Data 0.001 (0.001)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:31:48]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.700 Prec@5 99.780 Error@1 5.300

==>>[2018-05-03 22:32:27] [Epoch=449/500] [Need: 00:59:53] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [449][000/391]   Time 0.283 (0.283)   Data 0.127 (0.127)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:32:27]
  Epoch: [449][200/391]   Time 0.172 (0.159)   Data 0.001 (0.001)   Loss 0.0004 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:32:59]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.730 Prec@5 99.790 Error@1 5.270

==>>[2018-05-03 22:33:36] [Epoch=450/500] [Need: 00:58:42] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [450][000/391]   Time 0.329 (0.329)   Data 0.139 (0.139)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:33:37]
  Epoch: [450][200/391]   Time 0.137 (0.159)   Data 0.001 (0.002)   Loss 0.0002 (0.0008)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:34:08]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.670 Prec@5 99.760 Error@1 5.330

==>>[2018-05-03 22:34:46] [Epoch=451/500] [Need: 00:57:32] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [451][000/391]   Time 0.291 (0.291)   Data 0.124 (0.124)   Loss 0.0001 (0.0001)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:34:47]
  Epoch: [451][200/391]   Time 0.142 (0.157)   Data 0.001 (0.001)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:35:18]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.750 Prec@5 99.800 Error@1 5.250

==>>[2018-05-03 22:35:56] [Epoch=452/500] [Need: 00:56:21] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [452][000/391]   Time 0.339 (0.339)   Data 0.139 (0.139)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:35:56]
  Epoch: [452][200/391]   Time 0.149 (0.159)   Data 0.001 (0.002)   Loss 0.0008 (0.0007)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:36:28]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.630 Prec@5 99.800 Error@1 5.370

==>>[2018-05-03 22:37:06] [Epoch=453/500] [Need: 00:55:11] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [453][000/391]   Time 0.322 (0.322)   Data 0.137 (0.137)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:37:06]
  Epoch: [453][200/391]   Time 0.137 (0.158)   Data 0.000 (0.001)   Loss 0.0003 (0.0009)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:37:37]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.630 Prec@5 99.770 Error@1 5.370

==>>[2018-05-03 22:38:14] [Epoch=454/500] [Need: 00:54:00] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [454][000/391]   Time 0.380 (0.380)   Data 0.163 (0.163)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:38:15]
  Epoch: [454][200/391]   Time 0.171 (0.158)   Data 0.001 (0.002)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:38:46]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.690 Prec@5 99.770 Error@1 5.310

==>>[2018-05-03 22:39:24] [Epoch=455/500] [Need: 00:52:50] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [455][000/391]   Time 0.353 (0.353)   Data 0.138 (0.138)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:39:24]
  Epoch: [455][200/391]   Time 0.174 (0.157)   Data 0.001 (0.002)   Loss 0.0002 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:39:56]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.730 Prec@5 99.780 Error@1 5.270

==>>[2018-05-03 22:40:33] [Epoch=456/500] [Need: 00:51:39] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [456][000/391]   Time 0.273 (0.273)   Data 0.123 (0.123)   Loss 0.0040 (0.0040)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:40:33]
  Epoch: [456][200/391]   Time 0.156 (0.161)   Data 0.001 (0.002)   Loss 0.0003 (0.0007)   Prec@1 100.000 (99.981)   Prec@5 100.000 (100.000)   [2018-05-03 22:41:05]
  **Train** Prec@1 99.986 Prec@5 100.000 Error@1 0.014
  **Test** Prec@1 94.680 Prec@5 99.790 Error@1 5.320

==>>[2018-05-03 22:41:43] [Epoch=457/500] [Need: 00:50:29] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [457][000/391]   Time 0.339 (0.339)   Data 0.159 (0.159)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:41:44]
  Epoch: [457][200/391]   Time 0.169 (0.155)   Data 0.001 (0.002)   Loss 0.0007 (0.0007)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:42:15]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.710 Prec@5 99.790 Error@1 5.290

==>>[2018-05-03 22:42:53] [Epoch=458/500] [Need: 00:49:18] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [458][000/391]   Time 0.322 (0.322)   Data 0.141 (0.141)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:42:53]
  Epoch: [458][200/391]   Time 0.169 (0.156)   Data 0.001 (0.001)   Loss 0.0002 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:43:24]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.730 Prec@5 99.780 Error@1 5.270

==>>[2018-05-03 22:44:02] [Epoch=459/500] [Need: 00:48:08] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [459][000/391]   Time 0.346 (0.346)   Data 0.144 (0.144)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:44:02]
  Epoch: [459][200/391]   Time 0.155 (0.158)   Data 0.001 (0.002)   Loss 0.0002 (0.0006)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:44:34]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.620 Prec@5 99.770 Error@1 5.380

==>>[2018-05-03 22:45:11] [Epoch=460/500] [Need: 00:46:57] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [460][000/391]   Time 0.314 (0.314)   Data 0.130 (0.130)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:45:11]
  Epoch: [460][200/391]   Time 0.151 (0.159)   Data 0.001 (0.001)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:45:43]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 94.680 Prec@5 99.770 Error@1 5.320

==>>[2018-05-03 22:46:20] [Epoch=461/500] [Need: 00:45:46] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [461][000/391]   Time 0.336 (0.336)   Data 0.161 (0.161)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:46:21]
  Epoch: [461][200/391]   Time 0.169 (0.156)   Data 0.001 (0.002)   Loss 0.0002 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:46:52]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.680 Prec@5 99.780 Error@1 5.320

==>>[2018-05-03 22:47:29] [Epoch=462/500] [Need: 00:44:36] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [462][000/391]   Time 0.317 (0.317)   Data 0.152 (0.152)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:47:29]
  Epoch: [462][200/391]   Time 0.163 (0.160)   Data 0.001 (0.002)   Loss 0.0005 (0.0006)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:48:01]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.730 Prec@5 99.780 Error@1 5.270

==>>[2018-05-03 22:48:39] [Epoch=463/500] [Need: 00:43:25] [learning_rate=0.0010] [Best : Accuracy=94.75, Error=5.25]
  Epoch: [463][000/391]   Time 0.338 (0.338)   Data 0.126 (0.126)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:48:39]
  Epoch: [463][200/391]   Time 0.182 (0.156)   Data 0.001 (0.001)   Loss 0.0034 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:49:10]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.770 Prec@5 99.730 Error@1 5.230

==>>[2018-05-03 22:49:49] [Epoch=464/500] [Need: 00:42:15] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [464][000/391]   Time 0.324 (0.324)   Data 0.150 (0.150)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:49:49]
  Epoch: [464][200/391]   Time 0.148 (0.158)   Data 0.001 (0.002)   Loss 0.0003 (0.0008)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:50:20]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.730 Prec@5 99.780 Error@1 5.270

==>>[2018-05-03 22:50:58] [Epoch=465/500] [Need: 00:41:04] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [465][000/391]   Time 0.345 (0.345)   Data 0.168 (0.168)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:50:59]
  Epoch: [465][200/391]   Time 0.156 (0.155)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:51:30]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 94.740 Prec@5 99.790 Error@1 5.260

==>>[2018-05-03 22:52:07] [Epoch=466/500] [Need: 00:39:54] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [466][000/391]   Time 0.347 (0.347)   Data 0.182 (0.182)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:52:08]
  Epoch: [466][200/391]   Time 0.135 (0.160)   Data 0.001 (0.002)   Loss 0.0002 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:52:39]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 94.730 Prec@5 99.780 Error@1 5.270

==>>[2018-05-03 22:53:17] [Epoch=467/500] [Need: 00:38:43] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [467][000/391]   Time 0.346 (0.346)   Data 0.147 (0.147)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:53:17]
  Epoch: [467][200/391]   Time 0.173 (0.158)   Data 0.001 (0.002)   Loss 0.0036 (0.0006)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-05-03 22:53:49]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.660 Prec@5 99.760 Error@1 5.340

==>>[2018-05-03 22:54:26] [Epoch=468/500] [Need: 00:37:33] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [468][000/391]   Time 0.350 (0.350)   Data 0.157 (0.157)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:54:26]
  Epoch: [468][200/391]   Time 0.165 (0.160)   Data 0.001 (0.001)   Loss 0.0002 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:54:58]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.560 Prec@5 99.780 Error@1 5.440

==>>[2018-05-03 22:55:36] [Epoch=469/500] [Need: 00:36:22] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [469][000/391]   Time 0.312 (0.312)   Data 0.126 (0.126)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:55:37]
  Epoch: [469][200/391]   Time 0.143 (0.156)   Data 0.001 (0.001)   Loss 0.0015 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:56:08]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.750 Prec@5 99.770 Error@1 5.250

==>>[2018-05-03 22:56:46] [Epoch=470/500] [Need: 00:35:12] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [470][000/391]   Time 0.328 (0.328)   Data 0.125 (0.125)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:56:47]
  Epoch: [470][200/391]   Time 0.162 (0.157)   Data 0.001 (0.001)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 22:57:18]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.690 Prec@5 99.770 Error@1 5.310

==>>[2018-05-03 22:57:55] [Epoch=471/500] [Need: 00:34:01] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [471][000/391]   Time 0.329 (0.329)   Data 0.131 (0.131)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:57:56]
  Epoch: [471][200/391]   Time 0.154 (0.159)   Data 0.001 (0.001)   Loss 0.0020 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:58:27]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.680 Prec@5 99.790 Error@1 5.320

==>>[2018-05-03 22:59:05] [Epoch=472/500] [Need: 00:32:51] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [472][000/391]   Time 0.332 (0.332)   Data 0.134 (0.134)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 22:59:05]
  Epoch: [472][200/391]   Time 0.138 (0.157)   Data 0.001 (0.001)   Loss 0.0017 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 22:59:36]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.750 Prec@5 99.770 Error@1 5.250

==>>[2018-05-03 23:00:14] [Epoch=473/500] [Need: 00:31:41] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [473][000/391]   Time 0.349 (0.349)   Data 0.157 (0.157)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:00:15]
  Epoch: [473][200/391]   Time 0.153 (0.158)   Data 0.001 (0.001)   Loss 0.0004 (0.0006)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:00:46]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.710 Prec@5 99.780 Error@1 5.290

==>>[2018-05-03 23:01:23] [Epoch=474/500] [Need: 00:30:30] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [474][000/391]   Time 0.285 (0.285)   Data 0.121 (0.121)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:01:24]
  Epoch: [474][200/391]   Time 0.158 (0.157)   Data 0.001 (0.001)   Loss 0.0004 (0.0007)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 23:01:55]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.690 Prec@5 99.760 Error@1 5.310

==>>[2018-05-03 23:02:33] [Epoch=475/500] [Need: 00:29:20] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [475][000/391]   Time 0.331 (0.331)   Data 0.141 (0.141)   Loss 0.0001 (0.0001)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:02:34]
  Epoch: [475][200/391]   Time 0.173 (0.161)   Data 0.001 (0.001)   Loss 0.0002 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:03:06]
  **Train** Prec@1 99.992 Prec@5 100.000 Error@1 0.008
  **Test** Prec@1 94.610 Prec@5 99.760 Error@1 5.390

==>>[2018-05-03 23:03:45] [Epoch=476/500] [Need: 00:28:09] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [476][000/391]   Time 0.370 (0.370)   Data 0.159 (0.159)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:03:45]
  Epoch: [476][200/391]   Time 0.144 (0.159)   Data 0.001 (0.002)   Loss 0.0007 (0.0006)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 23:04:16]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.750 Prec@5 99.770 Error@1 5.250

==>>[2018-05-03 23:04:54] [Epoch=477/500] [Need: 00:26:59] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [477][000/391]   Time 0.352 (0.352)   Data 0.157 (0.157)   Loss 0.0009 (0.0009)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:04:55]
  Epoch: [477][200/391]   Time 0.174 (0.158)   Data 0.001 (0.002)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:05:26]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.720 Prec@5 99.760 Error@1 5.280

==>>[2018-05-03 23:06:05] [Epoch=478/500] [Need: 00:25:48] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [478][000/391]   Time 0.310 (0.310)   Data 0.128 (0.128)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:06:05]
  Epoch: [478][200/391]   Time 0.159 (0.157)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:06:36]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 94.620 Prec@5 99.770 Error@1 5.380

==>>[2018-05-03 23:07:15] [Epoch=479/500] [Need: 00:24:38] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [479][000/391]   Time 0.359 (0.359)   Data 0.146 (0.146)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:07:15]
  Epoch: [479][200/391]   Time 0.165 (0.163)   Data 0.001 (0.002)   Loss 0.0002 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:07:48]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.690 Prec@5 99.800 Error@1 5.310

==>>[2018-05-03 23:08:26] [Epoch=480/500] [Need: 00:23:28] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [480][000/391]   Time 0.329 (0.329)   Data 0.126 (0.126)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:08:27]
  Epoch: [480][200/391]   Time 0.160 (0.162)   Data 0.001 (0.001)   Loss 0.0002 (0.0007)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:08:59]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.710 Prec@5 99.770 Error@1 5.290

==>>[2018-05-03 23:09:37] [Epoch=481/500] [Need: 00:22:17] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [481][000/391]   Time 0.306 (0.306)   Data 0.133 (0.133)   Loss 0.0008 (0.0008)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:09:37]
  Epoch: [481][200/391]   Time 0.149 (0.156)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:10:08]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.650 Prec@5 99.800 Error@1 5.350

==>>[2018-05-03 23:10:45] [Epoch=482/500] [Need: 00:21:07] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [482][000/391]   Time 0.331 (0.331)   Data 0.150 (0.150)   Loss 0.0010 (0.0010)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:10:46]
  Epoch: [482][200/391]   Time 0.164 (0.157)   Data 0.001 (0.002)   Loss 0.0003 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:11:17]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.700 Prec@5 99.770 Error@1 5.300

==>>[2018-05-03 23:11:54] [Epoch=483/500] [Need: 00:19:56] [learning_rate=0.0010] [Best : Accuracy=94.77, Error=5.23]
  Epoch: [483][000/391]   Time 0.284 (0.284)   Data 0.123 (0.123)   Loss 0.0050 (0.0050)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:11:54]
  Epoch: [483][200/391]   Time 0.164 (0.154)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:12:25]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.790 Prec@5 99.780 Error@1 5.210

==>>[2018-05-03 23:13:03] [Epoch=484/500] [Need: 00:18:46] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [484][000/391]   Time 0.318 (0.318)   Data 0.132 (0.132)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:13:03]
  Epoch: [484][200/391]   Time 0.160 (0.160)   Data 0.001 (0.001)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 23:13:35]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.710 Prec@5 99.770 Error@1 5.290

==>>[2018-05-03 23:14:13] [Epoch=485/500] [Need: 00:17:35] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [485][000/391]   Time 0.306 (0.306)   Data 0.120 (0.120)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:14:13]
  Epoch: [485][200/391]   Time 0.152 (0.156)   Data 0.001 (0.001)   Loss 0.0007 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:14:44]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.690 Prec@5 99.790 Error@1 5.310

==>>[2018-05-03 23:15:23] [Epoch=486/500] [Need: 00:16:25] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [486][000/391]   Time 0.329 (0.329)   Data 0.131 (0.131)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:15:23]
  Epoch: [486][200/391]   Time 0.155 (0.155)   Data 0.001 (0.001)   Loss 0.0002 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:15:54]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.710 Prec@5 99.760 Error@1 5.290

==>>[2018-05-03 23:16:32] [Epoch=487/500] [Need: 00:15:15] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [487][000/391]   Time 0.344 (0.344)   Data 0.142 (0.142)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:16:32]
  Epoch: [487][200/391]   Time 0.155 (0.160)   Data 0.001 (0.002)   Loss 0.0002 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:17:04]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.680 Prec@5 99.790 Error@1 5.320

==>>[2018-05-03 23:17:41] [Epoch=488/500] [Need: 00:14:04] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [488][000/391]   Time 0.314 (0.314)   Data 0.150 (0.150)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:17:42]
  Epoch: [488][200/391]   Time 0.138 (0.160)   Data 0.000 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:18:14]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.660 Prec@5 99.780 Error@1 5.340

==>>[2018-05-03 23:18:51] [Epoch=489/500] [Need: 00:12:54] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [489][000/391]   Time 0.334 (0.334)   Data 0.154 (0.154)   Loss 0.0006 (0.0006)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:18:51]
  Epoch: [489][200/391]   Time 0.159 (0.158)   Data 0.001 (0.002)   Loss 0.0014 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:19:23]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.620 Prec@5 99.770 Error@1 5.380

==>>[2018-05-03 23:20:01] [Epoch=490/500] [Need: 00:11:43] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [490][000/391]   Time 0.340 (0.340)   Data 0.156 (0.156)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:20:01]
  Epoch: [490][200/391]   Time 0.136 (0.154)   Data 0.001 (0.001)   Loss 0.0003 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:20:32]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 94.600 Prec@5 99.760 Error@1 5.400

==>>[2018-05-03 23:21:10] [Epoch=491/500] [Need: 00:10:33] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [491][000/391]   Time 0.292 (0.292)   Data 0.121 (0.121)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:21:10]
  Epoch: [491][200/391]   Time 0.176 (0.156)   Data 0.001 (0.001)   Loss 0.0006 (0.0005)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 23:21:41]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 94.650 Prec@5 99.780 Error@1 5.350

==>>[2018-05-03 23:22:19] [Epoch=492/500] [Need: 00:09:23] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [492][000/391]   Time 0.315 (0.315)   Data 0.126 (0.126)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:22:19]
  Epoch: [492][200/391]   Time 0.152 (0.158)   Data 0.001 (0.001)   Loss 0.0007 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:22:50]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.680 Prec@5 99.780 Error@1 5.320

==>>[2018-05-03 23:23:28] [Epoch=493/500] [Need: 00:08:12] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [493][000/391]   Time 0.357 (0.357)   Data 0.135 (0.135)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:23:29]
  Epoch: [493][200/391]   Time 0.165 (0.157)   Data 0.001 (0.001)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:24:00]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.660 Prec@5 99.790 Error@1 5.340

==>>[2018-05-03 23:24:37] [Epoch=494/500] [Need: 00:07:02] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [494][000/391]   Time 0.289 (0.289)   Data 0.123 (0.123)   Loss 0.0002 (0.0002)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:24:38]
  Epoch: [494][200/391]   Time 0.160 (0.160)   Data 0.001 (0.001)   Loss 0.0027 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:25:09]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.640 Prec@5 99.770 Error@1 5.360

==>>[2018-05-03 23:25:48] [Epoch=495/500] [Need: 00:05:51] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [495][000/391]   Time 0.327 (0.327)   Data 0.124 (0.124)   Loss 0.0005 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:25:48]
  Epoch: [495][200/391]   Time 0.175 (0.159)   Data 0.001 (0.001)   Loss 0.0005 (0.0008)   Prec@1 100.000 (99.984)   Prec@5 100.000 (100.000)   [2018-05-03 23:26:20]
  **Train** Prec@1 99.988 Prec@5 100.000 Error@1 0.012
  **Test** Prec@1 94.700 Prec@5 99.790 Error@1 5.300

==>>[2018-05-03 23:26:58] [Epoch=496/500] [Need: 00:04:41] [learning_rate=0.0010] [Best : Accuracy=94.79, Error=5.21]
  Epoch: [496][000/391]   Time 0.290 (0.290)   Data 0.125 (0.125)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:26:59]
  Epoch: [496][200/391]   Time 0.161 (0.160)   Data 0.001 (0.001)   Loss 0.0002 (0.0005)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-05-03 23:27:30]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.810 Prec@5 99.780 Error@1 5.190

==>>[2018-05-03 23:28:08] [Epoch=497/500] [Need: 00:03:31] [learning_rate=0.0010] [Best : Accuracy=94.81, Error=5.19]
  Epoch: [497][000/391]   Time 0.325 (0.325)   Data 0.149 (0.149)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:28:08]
  Epoch: [497][200/391]   Time 0.135 (0.157)   Data 0.001 (0.002)   Loss 0.0004 (0.0005)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-05-03 23:28:40]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 94.780 Prec@5 99.780 Error@1 5.220

==>>[2018-05-03 23:29:17] [Epoch=498/500] [Need: 00:02:20] [learning_rate=0.0010] [Best : Accuracy=94.81, Error=5.19]
  Epoch: [498][000/391]   Time 0.330 (0.330)   Data 0.127 (0.127)   Loss 0.0003 (0.0003)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:29:18]
  Epoch: [498][200/391]   Time 0.149 (0.158)   Data 0.001 (0.001)   Loss 0.0003 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:29:49]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 94.800 Prec@5 99.770 Error@1 5.200

==>>[2018-05-03 23:30:26] [Epoch=499/500] [Need: 00:01:10] [learning_rate=0.0010] [Best : Accuracy=94.81, Error=5.19]
  Epoch: [499][000/391]   Time 0.335 (0.335)   Data 0.139 (0.139)   Loss 0.0004 (0.0004)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:30:27]
  Epoch: [499][200/391]   Time 0.145 (0.160)   Data 0.001 (0.002)   Loss 0.0002 (0.0005)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-05-03 23:30:58]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 94.680 Prec@5 99.780 Error@1 5.320
